---
title: "Bayesian Inference"
author: "Yongjin Park"
classoption: "aspectratio=169"
---


```{r include=FALSE}
set.seed(1447)
library(tidyverse)
library(data.table)
library(patchwork)
source("Util.R")
source("Setup.R")
fig.dir <- "Fig/"
setup.env(fig.dir, .echo=FALSE)
dir.create("Data", showWarnings=FALSE)
```


## A warm-up example: the mean and variance of log-Normal

Suppose we have a sequence of real-numbered experimental data observed: $x_{1}, x_{2}, \ldots$

* A previous graduate student told me that she calibrated the device, so that the numbers generally follow log-Normal distribution.

* However, she rushed to join some cool biotech start-up and did not inform us what were the desired mean and error parameters.

* Your PI desperately wants to know about the parameters to complete the Aim 3 in recent grant applications.

## The actual generative scheme

```{r echo=TRUE, size="large"}
set.seed(1331)
nn <- 100
xx <- rlnorm(nn, 5, sqrt(7))
```

## First, what do they look like?

:::::: {.columns}
::: {.column width=.5}


```{r fig.width=3, fig.height=3.2, only.plot="1"}
hist(xx)
```

```{r fig.width=3, fig.height=3.2, only.plot="2"}
qqnorm(xx)
```

:::
::: {.column width=.5}


```{r fig.width=3, fig.height=3.2, only.plot="1"}
hist(log(xx))
```

```{r fig.width=3, fig.height=3.2, only.plot="2"}
qqnorm(log(xx))
```

:::
::::::


## Can we infer the mean and variance of the observed data?

Now we are wondering what will be the suitable parameters...

To answer this question, we can turn to statistics.

Let's define data likelihood:

$$P(\{x_{i}\}|\mu, \gamma) = \prod_{i=1}^{n} \log\mathcal{N}\left(x_{i}\middle|\mu, \gamma^{-1}\right)$$

A **Frequentist** Question/Approach:

* What are the **most likely** $\mu$ and $\gamma$ values?

* **How confident** are we about the estimated $\hat{\mu}$ and $\hat{\gamma}$?

    - Standard error (error bars)

    - p-value of some hypothesis test (rejecting the null)

## A Bayesian way to know about the "how confident" question

A Bayesian never wants to give a point estimate, particular $\hat{\mu}$ and $\hat{\gamma}$ values.
Instead:

- **Given** data $\{x_{i}\}$, what is the probability of the parameters (posterior probability)?

$$p(\underbrace{ \mu, \gamma }_{\color{red} \textsf{unknown parameter}} | \underbrace{ \{x_{i}\} }_{\color{teal} \textsf{data}})$$


- **Eventually** we also want to predict future outcomes (posterior prediction),

$$p({\color{magenta} \underbrace{x^{\star}}_{\textsf{future}}} | {\color{teal} \underbrace{\{x_{i}\}}_{\textsf{data}}})  = \int d\mu\, d\gamma\, p( \underbrace{ {\color{magenta} x}^{\star} }_{\textsf{new observation} } \, | \underbrace{ \mu,\gamma) \, \, p(\mu, \gamma}_{\textsf{averaging over uncertainty}} \, |  \underbrace{ {\color{teal} \{x_{i}\}} }_{\textsf{training data}} ) $$

- Moreover, we want to find a better model comparing the model evidence:

$$p(\textsf{data}\,|\,\textsf{model 1}) \, \textsf{vs.} \, p(\textsf{data}\,|\,\textsf{model 2})$$


## _Side note:_ Bayesian inference often involves heavy math/computational work

In the order of "easiness" in statistical learning (generally):

* Model parameter estimation/optimization

* Classification/categorical value prediction

* Real-valued prediction

* Model averaging $\gets$ Bayesian inference

* Probability/density estimation $\gets$ Bayesian inference

* Evidence computation (aka, partition function in physics) $\gets$ Bayesian inference

## The first step in Bayesian inference is to build a generative model

In our example:

:::::: {.columns}
::: {.column width=.5}

1. Sample $\mu \sim \mathcal{N}\!\left(0, 10^2\right)$

2. Sample $\gamma \sim \operatorname{Gamma}(1, 1)$

3. Sample $X \sim \operatorname{log-}\mathcal{N}\!\left(\mu, \gamma^{-1}\right)$

:::
::: {.column width=.5}

- A prior generative scheme of $\mu$

- A prior generative scheme of $\gamma$

- A data-generating of $x$ given $\mu,\gamma$

:::
::::::


\vfill

- "What I cannot create, what I cannot understand" - Richard Feynman

- How do we recover (reverse-engineer) these unknown parameters?

## Markov Chain Monte Carlo (MCMC) simulations **infer** the distributing of unknown parameters


Show PGM and


## `stan`: A good news! All you need is to complete the model description!

:::::: {.columns}
::: {.column width=.5}

```{cc echo=TRUE, eval=FALSE}
data {
 int n;                   // sample size
 real x[n];               // n data points
}
parameters {
 real mu;                 // mean parameter
 real<lower=1e-8> lambda; // precision
}
model {
 mu ~ normal(0, 1e2);        // prior
 lambda ~ gamma(1e-2, 1e-2); // prior
 for(i in 1:n){              // data generating
   x[i] ~ lognormal(mu, sqrt(1/lambda));
 }
}
```

:::
::: {.column width=.5}

We can have a separate file with model description in `stan` language:

1. `data` block to describe expected data dimensions and types

2. `parameters` block to describe (unknown) model parameters to infer

3. `model` block to describe data-generating schemes (a probabilistic graph model)

:::
::::::


## `rstan` incorporates existing `stan` codes into `R` scripts to run Bayesian inference algorithm

:::::: {.columns}
::: {.column width=.5}


In `R`, it can be a simple string

```{r echo=TRUE}
.code <- "
data {
 int n;
 real x[n];
}
parameters {
 real mu;
 real<lower=1e-8> lambda;
}
model {
 mu ~ normal(0, 1e2);
 lambda ~ gamma(1e-2, 1e-2);
 for(i in 1:n){
   x[i] ~ lognormal(mu, sqrt(1/lambda));
 }
}
"
```

:::
::: {.column width=.5}

We can copy/paste `stan`'s model code in some `R` string.

1. `data` block to describe expected data dimensions and types

2. `parameters` block to describe (unknown) model parameters to infer

3. `model` block to describe data-generating schemes (a probabilistic graph model)

:::
::::::


## Running `stan` is as easy as calling other `R` functions

```{r run_stan_lgaussian, echo=TRUE, output=FALSE}
library(rstan)
options(mc.cores = parallel::detectCores())

if.needed("example_stan_lgaussian.rds", {       ## don't re-reun

    ## Run MCMC inference algorithm
    .fit <- stan(model_code = .code,            ## code
                 data = list(n = nn, x = xx),   ## a list of data
                 pars = c("mu", "lambda"),      ## parameters of interest
                 chains = 5,                    ## number of parallel MCMC chains
                 iter=1000)                     ## how many iterations?

    saveRDS(.fit, "example_stan_lgaussian.rds") ## save the results
})

## Extract the sampled parameters
.fit <- readRDS("example_stan_lgaussian.rds")
.mu <- rstan::extract(.fit, pars="mu", inc_warmup=TRUE, permuted=FALSE)
.lambda <- rstan::extract(.fit, pars="lambda", inc_warmup=TRUE, permuted=FALSE)
```

## As a result of MCMC...


```{r echo=TRUE, results="asis", size="large"}
dim(.mu)
```

```{r echo=TRUE, results="asis", size="large"}
dim(.lambda)
```

## As a result of four independent MCMC runs

```{r}
mu.melt <- reshape2::melt(.mu[, , 1], value.name = "mu")
lam.melt <- reshape2::melt(.lambda[, , 1], value.name = "lambda")
.mcmc.dt <- left_join(mu.melt, lam.melt) %>% as.data.table
```


```{r}
.show.until <- function(tt) {
    .gg.plot(.mcmc.dt[iterations <= tt, ], aes(`mu`, `lambda`)) +
        scale_x_continuous(expression(mu), limits=range(.mcmc.dt$mu)) +
        scale_y_continuous(expression(lambda), limits=range(.mcmc.dt$lambda)) +
        geom_line(aes(group=`chains`, colour=`chains`), size=.5) +
        geom_density_2d(color="gray40", size=.5, lty=1) +
        geom_point(aes(shape = `chains`), size=1, alpha=.8) +
        geom_point(data=.mcmc.dt[iterations==1, ], aes(colour=`chains`), size=2, pch=19) +
        geom_text_repel(data=.mcmc.dt[iterations==1, ], aes(label=chains), size=3) +
        geom_point(data = data.frame(mu=5, lambda=1/7), colour="blue", size=8, pch="x") +
        scale_color_brewer(palette = "Set1") +
        scale_shape_manual(values=20 + 1:5) +
        ggtitle("first " %&% tt %&% " iterations")
}
```

```{r fig.width=5, fig.height=3, only.plot=1}
.show.until(3)
```

```{r fig.width=5, fig.height=3, only.plot=2}
.show.until(5)
```

```{r fig.width=5, fig.height=3, only.plot=3}
.show.until(7)
```

```{r fig.width=5, fig.height=3, only.plot=4}
.show.until(10)
```

```{r fig.width=5, fig.height=3, only.plot=5}
.show.until(20)
```

```{r fig.width=5, fig.height=3, only.plot=6}
.show.until(30)
```

```{r fig.width=5, fig.height=3, only.plot=7}
.show.until(500)
```


## Posterior probability

:::::: {.columns}
::: {.column width=.5}

```{r fig.width=3, fig.height=3.2}
hist(.mu[-(1:100), , ], xlab="mu", main="")
abline(v=5, col=2, lwd=2)
abline(v=mean(.mu), col=3, lwd=2, lty=2)
```

:::
::: {.column width=.5}

```{r fig.width=3, fig.height=3.2}
hist(1/.lambda[-(1:100), , ], xlab="1/lambda", main="")
abline(v=7, col=2, lwd=2)
abline(v=mean(1/.lambda), col=3, lwd=2, lty=2)
```

:::
::::::



## Bayesian linear regression

Let's consider another toy example, Poisson regression

```{r echo = TRUE}
n <- 100      ## sample
p.tot <- 10   ## total no. of predictors
p <- 2        ## first `p` are true predictors
sigma <- 0.1  ## error StdDev

set.seed(1331)
X <- matrix(rnorm(n * p.tot), nrow=n, ncol=p.tot)
theta <- matrix(2 * rnorm(p), nrow=p, ncol=1)
y <- round(exp(X[, 1:p] %*% theta + rnorm(n) * sigma))
```

## Can you draw the corresponding probabilistic graphical model?


$$\beta_{j} \sim \mathcal{N}\!\left(0, 1\right)$$

$$\beta_{j} \sim \mathcal{N}\!\left(0, 1\right)$$


## Let's write out a Poisson regression generative model

```{cc echo=TRUE, eval=FALSE}
data {
  int n; int p;                                 // number of observations  & predictors
  matrix[n, p] X;                               // design matrix
  int y[n];                                     // response vector
}
parameters {
  vector[p] beta;                               // regression coefficients
  real alpha;                                   // intercept
  vector<lower=1e-8>[n] lambda;                 // overdispersion
  real phi;                                     // hyperparameter
}
transformed parameters {
  vector[n] eta;
  eta = X * beta + alpha;
}
model {
  alpha ~ normal(0, 1);  phi ~ gamma(1e4, 1e4); //
  for (j in 1:p) { beta[j] ~ normal(0, 1); }    // coefficients
  for (i in 1:n) {                              //
    lambda[i] ~ gamma(phi, 1);                  // random overdispersion
    y[i] ~ poisson(lambda[i] * exp(eta[i]));    // poisson rate
  }
}
```

## 

```{r echo=TRUE}
.code <- "
data {
  int n; int p;                                  // number of observations  & predictors
  matrix[n, p] X;                                // design matrix
  int y[n];                                      // response vector
}
parameters {
  vector[p] beta;                                // regression coefficients
  real alpha;                                    // intercept
  vector<lower=1e-8>[n] lambda;                  // overdispersion
  real phi;                                      // hyperparameter
}
transformed parameters {
  vector[n] eta;
  eta = X * beta + alpha;
}
model {
  alpha ~ normal(0, 1);   phi ~ gamma(1e4, 1e4); //
  for (j in 1:p) { beta[j] ~ normal(0, 1); }     //
  for (i in 1:n) {                               //
    lambda[i] ~ gamma(phi, 1);                   // random overdispersion
    y[i] ~ poisson(lambda[i] * exp(eta[i]));     // poisson rate
  }
}
"
```

## Run `stan` to sample the parameters of interest

```{r echo=TRUE, output=FALSE}
if.needed("example_stan_regression.rds", {       ## don't re-reun

    lm.fit <- stan(model_code=.code,             ## model code
                   data = list(n=nrow(X),        ## samples
                               p=ncol(X),        ## predictors
                               y=y[,1],          ## response
                               X=X),             ## design matrix
                   pars = c("beta","alpha"),     ## parameters
                   iter=1000,                    ## MCMC
                   chains=5)                     ## chains

    saveRDS(lm.fit, "example_stan_regression.rds")
})

lm.fit <- readRDS("example_stan_regression.rds") ## save the results

.beta <- extract(lm.fit, pars="beta", inc_warmup=TRUE, permuted=FALSE)
```

```{r}
.take.grad <- function(chain, coord.x = 1, coord.y = 2){

    nn <- dim(.beta)[1]

    .dx <- .beta[-nn, chain, coord.x] - .beta[-1, chain, coord.x]
    .dy <- .beta[-nn, chain, coord.y] - .beta[-1, chain, coord.y]

    data.table(x=.beta[-nn, chain, coord.x],
               y=.beta[-nn, chain, coord.y],
               xend=.beta[-1, chain, coord.x] + .05 * .dx,
               yend=.beta[-1, chain, coord.y] + .05 * .dy) %>%
        mutate(t = 1:n())
}

.show.grad.until <- function(grad.dt, tt, xlab, ylab) {
    ret <- ggplot(head(grad.dt, tt)) +
        theme_bw() +
        scale_x_continuous(xlab, limits=range(grad.dt$x)) +
        scale_y_continuous(ylab, limits=range(grad.dt$y)) +
        geom_point(aes(x,y), color="gray") +
        geom_segment(aes(x=x,y=y,xend=xend,yend=yend),
                     arrow=arrow(length = unit(.2,"lines"), type="closed"),
                     color="#009900", size=.3) +
        geom_text_repel(aes((x + xend)/2,(y + yend)/2,label=`t`), color="blue", size=3, segment.size=0) +
        ggtitle(tt %&% " MCMC iterations")

    if(tt >= 10) {
        ret <- ret + geom_density2d(aes(x,y), colour="gray60", size = .2)
    }

    ret <- ret +
        geom_point(data=data.table(x=theta[1], y=theta[2]), aes(x,y), colour="red", size=8, pch="+")

    ret <- ret +
        geom_hline(yintercept = 0, lty=2) + geom_vline(xintercept = 0, lty=2, size=.5)
    return(ret)
}
```

## Let's take a look at truly non-zero coefficients

```{r}
grad.12 <- .take.grad(1, 1, 2)
```

```{r fig.width=5, fig.height=3, only.plot=1}
.show.grad.until(grad.12, 1, expression(beta[1]), expression(beta[2]))
```

```{r fig.width=5, fig.height=3, only.plot=2}
.show.grad.until(grad.12, 5, expression(beta[1]), expression(beta[2]))
```

```{r fig.width=5, fig.height=3, only.plot=3}
.show.grad.until(grad.12, 10, expression(beta[1]), expression(beta[2]))
```

```{r fig.width=5, fig.height=3, only.plot=4}
.show.grad.until(grad.12, 20, expression(beta[1]), expression(beta[2]))
```


```{r fig.width=5, fig.height=3, only.plot=5}
.show.grad.until(grad.12, 100, expression(beta[1]), expression(beta[2]))
```

```{r fig.width=5, fig.height=3, only.plot=6}
.show.grad.until(grad.12, 300, expression(beta[1]), expression(beta[2]))
```

## Let's take a look at other "null" coefficients

```{r}
grad.34 <- .take.grad(1, 3, 4)
```

```{r fig.width=5, fig.height=3, only.plot=1}
.show.grad.until(grad.34, 1, expression(beta[3]), expression(beta[4]))
```

```{r fig.width=5, fig.height=3, only.plot=2}
.show.grad.until(grad.34, 5, expression(beta[3]), expression(beta[4]))
```

```{r fig.width=5, fig.height=3, only.plot=3}
.show.grad.until(grad.34, 10, expression(beta[3]), expression(beta[4]))
```

```{r fig.width=5, fig.height=3, only.plot=4}
.show.grad.until(grad.34, 20, expression(beta[3]), expression(beta[4]))
```

```{r fig.width=5, fig.height=3, only.plot=5}
.show.grad.until(grad.34, 30, expression(beta[3]), expression(beta[4]))
```

```{r fig.width=5, fig.height=3, only.plot=6}
.show.grad.until(grad.34, 300, expression(beta[3]), expression(beta[4]))
```

## Compare true non-zero vs. zero coefficients

```{r fig.width=6, fig.height=3}
p1 <- .show.grad.until(grad.12, 800, expression(beta[1]), expression(beta[2]))
p2 <- .show.grad.until(grad.34, 800, expression(beta[3]), expression(beta[4]))
p1|p2
```

## What have we learned?

\Large

- Bayesian Inference

- Probabilistic graphical model

- How to describe a generative model in `stan`

- How to perform parametric inference by MCMC
