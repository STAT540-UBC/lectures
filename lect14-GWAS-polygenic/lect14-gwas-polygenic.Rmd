---
title: "Polygenic prediction and sparse regression analysis"
author: "Yongjin Park, UBC Path + Stat, BC Cancer"
date: "`r format(Sys.time(), '%d %B %Y')`"
mainfont: Roboto
classoption: "aspectratio=169"
fontsize: 12pt
output:
    powerpoint_presentation:
        reference_doc: "_template.pptx"
    html_document:
        self_contained: true
    beamer_presentation:
        theme: Madrid
        keep_md: true
        keep_tex: true
        latex_engine: xelatex
        slide_level: 2
header-includes:
  - \definecolor{UBCblue}{rgb}{0.04706, 0.13725, 0.26667}
  - \setbeamertemplate{frametitle}{\color{UBCblue}\bfseries\insertframetitle\par\vskip-6pt\hrulefill}
  - \setbeamercolor{title in head/foot}{bg=white,fg=gray}
  - \setbeamercolor{section in head/foot}{bg=white,fg=gray}
  - \setbeamercolor{author in head/foot}{bg=white,fg=gray}
  - \setbeamercolor{date in head/foot}{bg=white,fg=gray}
  - \setbeamertemplate{page number in head/foot}{}
  - \setbeamertemplate{frame numbering}[none]
  - \setbeamercolor{alerted text}{bg=yellow}
  - \AtBeginSection[]{\begin{frame}\frametitle{Today's lecture}{\Large\tableofcontents[currentsection]}\end{frame}}
  - |
    \makeatletter
    \def\ps@titlepage{%
      \setbeamertemplate{footline}{}
    }
    \addtobeamertemplate{title page}{\thispagestyle{titlepage}}{}
    \makeatother
    \include{toc}
---

```{r setup, include=FALSE}
setwd("~/work/teaching/stat540_lectures/lect14-GWAS-polygenic/")
library(data.table)
library(tidyverse)
library(patchwork)
library(matrixTests)
source("Setup.R")
fig.dir <- "../Fig/pgs/"
dir.create(fig.dir, showWarnings=FALSE)
setup.env(fig.dir)
dir.create("../data", showWarnings=FALSE)
library(extrafont)
library(xkcd)
extrafont::font_import(pattern="[X/x]kcd", prompt=F)
extrafont::loadfonts()
theme_set(theme_xkcd() +
          theme(title = element_text(size=10),
                legend.background = element_blank())
          )
```

## Learning objective

\large

* Gain insights into human genetics problems.

* Understand statistical challenges in a **high-dimensional** prediction problem.

* Survey of **statistical approaches** to coping with $n{\ll}p$ settings.

\vfill

\small
$n$: sample size; $p$: dimension (predictors)

## Human genetics data -- The 1000 Genomes Project

:::::: {.columns}
::: {.column width=.5}

\centerline{\includegraphics[width=.7\linewidth]{img/1000genomes.png}}

The 1KG data using `bigsnpr` library.

```{r eval=F, echo=T, size="large"}
library(bigsnpr)
download_1000G("../data/genotype/")
```

:::
::: {.column width=.5}

\large

* International consortium

* Goal:  find common genetic variants with frequencies of at least 1%

* The project planned to sequence each sample to 4x genomic coverage.

:::
::::::

https://www.internationalgenome.org/

```{r download_1000g_data}
dir.create("../data/genotype/", recursive=TRUE, showWarnings=FALSE)
## bigsnpr
library(bigsnpr)
.bed.file <- "../data/genotype/1000G_phase3_common_norel.bed"
if.needed(.bed.file, {
    download_1000G("../data/genotype/")
})
## backup file
.bk.file <- "../data/genotype/1000G_phase3_common_norel.rds"
if.needed(.bk.file, {
    BED <- snp_readBed(.bed.file)
})
```

## How these genotypes instantiated in 1KG data

By calling `snp_readBed(.bed.file)`, we can convert the "BED"-formatted data to a "RDS" file for faster access. Later, we need to "attach" that RDS.

\vfill

```{r echo=T, tidy=T}
data <- snp_attach(.bk.file)
str(data, max.level=1, strict.width = "cut")
```

```{r include = F}
pop.info.file <- "../data/genotype/1000G_phase3_common_norel.fam2"
pop.info <- fread(pop.info.file)
```

## Definitions -- a quick review of the previous lecture

:::::: {.columns}
::: {.column width=.3}
::: {.block}
### Allele

* A different form of a gene

* A Greek word "allos," $\alpha\lambda\lambda\eta\lambda\omicron$, meaning "other"

:::

:::
::: {.column width=.3}
::: {.block}
### Variant and locus

* A specific region of the genome differs across two or more genomes

* A result of mutation

* A locus: a location where many variants lie (plural: loci).

:::

:::
::: {.column width=.3}
::: {.block}
### Ploidy

* The number of copies of chromosomes within a cell/organism

* Haploid: one copy

* Diploid: two copies

:::

:::
::::::

## More definitions

:::::: {.columns}
::: {.column width=.3}
::: {.block}
### Biallelic variant

- bi + allelic

- Two forms for a variant

- Reference (more frequently observed) vs. alternative allele

:::

:::
::: {.column width=.3}
::: {.block}

### Polymorphism

- Poly + morph

- Occurrence of different forms

:::


:::
::: {.column width=.3}
::: {.block}
### SNP

- Single Nucleotide Polymorphism

- A place in the genome where people differ by a single base pair

:::

:::
::::::

\vfill

We pronounce SNP "snip" in North America.

## Genetic variants cheat sheet

\only<1>{\centerline{\includegraphics[height=.75\textheight]{img/altshuler_daly_lander_noannot.pdf}}

\tiny Image: Altshuler, Daly, Lander (Science)}

\only<2>{\centerline{\includegraphics[height=.75\textheight]{img/altshuler_daly_lander_annot.pdf}}

\tiny Image: Altshuler, Daly, Lander (Science)}

\only<3>{\centerline{\includegraphics[height=.75\textheight]{img/altshuler_daly_lander_LD.pdf}}

\textbf{Linkage disequilibrium $\approx$ correlation block}}


# Why do we want to build a polygenic score model?

## GWAS (previous lecture), *then*, what's next?

\centerline{\includegraphics[height=.7\textheight]{img/altshuler_daly_lander_GWAS.pdf}}

## GWAS (previous lecture), *then*, what's next?

\Large

- GWAS until 2010s: heavy focuses on **mapping**

    - GWAS map: genetic variants $\to$ a phenotype

    - Stringent genome-wide p-value cutoff

    - Study design, meta analysis

- NHGRI-EBI GWAS Catalog: [`https://www.ebi.ac.uk/gwas/`](https://www.ebi.ac.uk/gwas/)

##

\huge

Let's take a look at the GWAS Catalog:

[`https://www.ebi.ac.uk/gwas/`](https://www.ebi.ac.uk/gwas/)

## GWAS (previous lecture), then, *what's next?*

\Large

- GWAS since 2010s: more emphases on **prediction**

	- Can we turn GWAS results to a prediction model?

	- Can we understand the mechanisms?

    - Machine learning, data integration, causal inference


## Polygenic score models predict disease prevalence based on genome

- Poly + genic = many genes

\vfill

::: {.block}
### A (linear) polygenic score (PGS)

$$Y_{i} = \sum_{j} X_{ij} \beta_{j}$$

for an individual $i$.

:::

\onslide<2>{Knowing $\beta$'s, we can predict:
$${\color{teal}Y^{\star}}_{i} \gets \sum_{j} {\color{magenta} X_{ij}^{\star}} \hat{\boldsymbol{\beta}}_{j}$$}


## Example: PGS for breast cancer occurrence

:::::: {.columns}
::: {.column width=.7}

\includegraphics[width=\linewidth]{img/PGS_Yang_Pharoah_NR_Cancer_2023_BRCA.pdf}

:::
::: {.column width=.3}

\normalsize

* Strong heritability (proportion of disease risk variation explained by genetics): 35% - 80%

* Prediction more accurate than life style and other environmental factors

* More powerful if combined with rare risk factors

:::
::::::

\vfill

\tiny

Yang .. Pharoah, Nature Review Genetics (2023)

## Example: PGS for coronary artery disorder $\to$ prevention

\centerline{\includegraphics[width=.8\linewidth]{img/PGS_Khera_Kathiresan_NatGen_2018_CAD.pdf}}

* Top .5% of PGS values $\to$ five fold increase of CAD

\vfill

\tiny

Khera .. Kathiresan, Nature Genetics (2018)

## Example: PGS stratifies individuals' disease onset and risk

\centerline{\includegraphics[width=.8\linewidth]{img/PGS_Mars_Ripatti_Nature_Medicine_2020_1.pdf}}

\vfill

* PRS $\approx$ PGS.

* We can partition cohorts based on PGS profiles.

\tiny

Mars .. Ripatti, Nature Medicine (2020)

# What is a polygenic score model?

## A toy example: GWAS for "Obsessive `ggplot` Disorder"^[just an illustration purpose]

```{r include=F}
#' @param X genotype matrix
#' @param W population structure
#' @param h2 heritability
#' @param pve.w population PVe
#' @param n.causal
#' @param n.traits
simulate.pgs <- function(X, W, h2, pve.w, n.causal, n.traits = 1) {
    .rnorm <- function(d1, d2) matrix(rnorm(d1*d2), d1, d2)
    causal.snp <- sample(ncol(X), n.causal)
    xx.causal <- apply(X[, causal.snp, drop=FALSE], 2, scale)
    xx.causal[is.na(xx.causal)] <- 0

    n.ind <- nrow(X)
    y.true <- apply(xx.causal %*% .rnorm(n.causal, n.traits), 2, scale)

    if(pve.w > 0){
        y.pop <- apply(W %*% .rnorm(ncol(W), n.traits), 2, scale)
    }else{
        y.pop <- 0
    }
    y.err <- apply(.rnorm(n.ind, n.traits), 2, scale)

    y.obs <- y.true * sqrt(h2) + y.pop * sqrt(pve.w)
    y.obs <- y.obs + y.err * sqrt(1 - h2 - pve.w)

    prop <- 1/(1+ exp(-y.obs))
    y.cc <- rbinom(length(prop), 1, prop)

    list(y.q = y.obs, y.cc = y.cc, pgs = y.true,
         x = X, w = W, causal = causal.snp)
}
```

```{r}
## Take first chromosome
chr1 <- which(data$map$chromosome==1)
X.tot <- data$genotypes[, chr1]
rownames(X.tot) <- data$fam$sample.ID
colnames(X.tot) <- data$map$physical.pos[chr1]

## population structure
W.tot <-
    pop.info %>%
    mutate(one = 1) %>%
    dcast(`sample.ID` ~ `Super Population`, value.var = "one", fill = 0) %>%
    (function(x) left_join(data$fam[, "sample.ID", drop = F], x)) %>%
    column_to_rownames("sample.ID") %>%
    as.matrix()
```

```{r}
set.seed(1)
.sim <- simulate.pgs(X.tot, W.tot, h2=.3, pve.w=0, n.causal=7)
.train <- sample(nrow(X.tot), 1500)
Y <- .sim$y.cc[.train]
X <- .sim$x[.train, , drop = F]
```

A genotype matrix $X$ ($X_{ij} \in \{0,1,2\}$):

```{r fig.width=6, fig.height=2.3}
.matshow(X[1:30, 1:80], .lw=.01, .lab=1.5)
```

## In GWAS, we can have case vs. control phenotype

\large

$Y_{i} = 1$ if case vs. $Y_{i} = 0$ if control

For our *OGD* GWAS:

```{r}
y <- .sim$y.cc[.train]
```

:::::: {.columns}
::: {.column width=.2}

:::
::: {.column width=.8}


```{r echo=T, size="large"}
table(y)
```

:::
::::::


- E.g., Cancer vs. non-cancer, schizophrenia vs. no mental disorder

- The fundamental question is, "Which individuals/subjects can be truly labelled control/wild type?"

## The under the hood, there are quantitative "risk" scores

```{r fig.width=5, fig.height=2}
ggplot(data.table(y=as.vector(.sim$y.q)), aes(y)) +
    geom_histogram() +
    xlab("phenotypic variation")
```

- E.g., height, BMI, parents' age of death, how man pack of cigarettes, etc.

## Side note: Almost all complex traits are polygenic

Polygenic effects of common SNPs $\to$ polymorphism in phenotypes

\vfill

> A polygenic trait is a characteristic, such as height or skin color, that is influenced by two or more genes. **Because multiple genes are involved, polygenic traits do not follow the patterns of Mendelian inheritance.** Many polygenic traits are also influenced by the environment and are called multifactorial.

\vfill

https://www.genome.gov/genetics-glossary/Polygenic-Trait


## A polygenic score to predict disease prevalence

```{r}
.dt <- data.table(y = as.vector(.sim$y.q),
                  disease = factor(.sim$y.cc, c(0, 1), c("without OGD","with OGD")))
```

```{r fig.width=5.5, fig.height=2.5, only.plot="1"}
ggplot(.dt[disease == "without OGD"], aes(y)) +
    xlab("polygenic risk score (log)") +
    geom_density(alpha = .35, fill = "#000099") +
    geom_vline(xintercept = 0, lty = 2, col = 1) +
    ggtitle("polygenic score of healthy individuals")
```

```{r fig.width=5.5, fig.height=2.5, only.plot="2"}
ggplot(.dt[disease == "with OGD"], aes(y)) +
    xlab("polygenic risk score (log)") +
    geom_density(alpha = .35, fill = "red") +
    geom_vline(xintercept = 0, lty = 2, col = 1) +
    ggtitle("polygenic score of OGD individuals")
```

```{r fig.width=5.5, fig.height=2.8, only.plot="3"}
ggplot(.dt, aes(y, fill = disease, group = disease)) +
    xlab("polygenic risk score (log)") +
    geom_density(alpha = .35) +
    geom_vline(xintercept = 0, lty = 2, col = 1) +
    theme(legend.position = c(0,1),
          legend.justification = c(0,1)) +
    scale_fill_brewer(palette="Set1", direction=-1)
```

## Polygenic score $\propto$ the odds of the case vs. control in GWAS

Log-odds ratio:

$$g(y) = \log\frac{p(\textsf{disease}|Y > y)}{p(\textsf{no disease}|Y > y)}$$

\vfill

```{r fig.width=5, fig.height=1.5, onslide.plot="2"}
count.cc <- function(.tab, .cutoff){
    .tab[y > .cutoff,
        .(.N),
        by = .(disease)] %>%
        mutate(y = .cutoff)
}

n.tot <-
    .dt[, .(ntot = .N), by = .(disease)]

n.cc <-
    lapply(seq(-1.5, 1.5, by = .25), count.cc, .tab = .dt) %>%
    do.call(what = rbind) %>%
    left_join(n.tot) %>%
    mutate(pr = `N`/`ntot`) %>%
    dcast(y ~ disease, value.var = "pr")

.aes <- aes(y, log10(`with OGD`) - log10(`without OGD`))
ggplot(n.cc, .aes) +
    geom_line() +
    geom_point(pch=21, fill = "white") +
    xlab("polygenic score cutoff")+
    scale_y_continuous("odds ratio \nOGD vs. no-OGD",
                       labels = function(x)round(10^x, 2))
```

## A PGS model to explain disease prevalence with genetics

Log-odds ratio:

$$g(y) = \log\frac{p(\textsf{disease}|Y > y)}{p(\textsf{no disease}|Y > y)}$$

\vfill

**Goal**: Estimate a function to predict this log-odds ratio.

\only<1>{$$g(y_{i}) \sim \beta_{1} X_{i1} + \beta_{2} X_{i2} \cdots + \epsilon$$}

\only<2>{$$g(y_{i}) \sim \underbrace{\beta_{1} X_{i1} + \beta_{2} X_{i2} \cdots }_{\textsf{\color{teal}\bf genetic effects}} + \epsilon$$}


## PGS estimation is a linear modelling with ${p{\gg}n}$

\only<1>{$$Y_{i} \sim X_{i1} \beta_{1} + X_{i2} \beta_{2} + \cdots$$}

\only<2>{$$\left(
\begin{array}{l}
Y_{1}\\
Y_{2}\\
\vdots\\
Y_{n}
\end{array}
\right)
\sim
\left(
\begin{array}{l}
X_{11}\\
X_{21}\\
\vdots\\
X_{n1}
\end{array}
\right)
\beta_{1} +
\left(
\begin{array}{l}
X_{12}\\
X_{22}\\
\vdots\\
X_{n2}
\end{array}
\right)
\beta_{2} +
\cdots$$}

\onslide<3-5>{$$\left(
\begin{array}{l}
Y_{1}\\
Y_{2}\\
\vdots\\
Y_{n}
\end{array}
\right)
\sim
\left(
\begin{array}{l}
X_{11}\\
X_{21}\\
\vdots\\
X_{n1}
\end{array}
\right)
\beta_{1} +
\left(
\begin{array}{l}
X_{12}\\
X_{22}\\
\vdots\\
X_{n2}
\end{array}
\right)
\beta_{2} +
\cdots +
\left(
\begin{array}{l}
X_{1p}\\
X_{2p}\\
\vdots\\
X_{np}
\end{array}
\right)
\beta_{p}
$$
}

\onslide<4-5>{
$$\mathbf{y} \sim \mathbf{x}_{1} \beta_{1} + \mathbf{x}_{2} \beta_{2} + \cdots + \mathbf{x}_{p} \beta_{p}$$

$$p{\gg}n$$
}

\only<5>{\large
$n \approx 10^{4}$
but
$p \approx 10^{6}$ for many large-scale GWAS}

## $p{\gg}n$: Why can't we just fit a model?

```{r echo = T, eval = F, size = "large"}
## lm.out <- lm(Y ~ X)
```

\large

\begin{itemize}[<+-|alert@+>]
\item $p \gg n$: need to estimate $\beta_{1},\ldots,\beta_{p}$
\item How many samples? How many unknowns?
\item Is it computationally feasible?
\end{itemize}

## $p{\gg}n$: Variant-by-variant GWAS is not a bad idea

```{r echo=T, size="large"}
.gwas <- col_t_welch(X[Y == 0, ], X[Y == 1, ]) # univar.
```

```{r onslide.plot="2", fig.width=6, fig.height=2.2}
par(pch=19, col="gray40", cex=.5)
.df <- data.table(x=colnames(X), y=-log10(.gwas$pvalue))
.df.c <- .df[.sim$causal]
ggplot(.df, aes(as.integer(x),y)) +
    ggrastr::rasterise(geom_point(stroke=0, color="gray"),dpi=300) +
    geom_point(data=.df.c, col=2, pch=21, stroke=1.5, size=2) +
    geom_hline(yintercept = -log10(0.05/ncol(X)), lty=2, col=2) +
    theme(legend.position = c(0,1)) +
    theme(legend.justification = c(0,1)) +
    xlab("genomic locations (chromosome 1)") +
    ylab("-log10 p-value")
```

## A linear model including only these "GWAS" variants

:::::: {.columns}
::: {.column width=.45}


```{r echo = T}
X.gwas <- X[, p.adjust(.gwas$pvalue) < .05]
head(X.gwas, 4)
```

:::
::: {.column width=.45}

```{r}
Y <- .sim$y.q[.train]
```

```{r echo = T}
lm.out <- lm(Y ~ X.gwas)
```

:::
::::::

:::::: {.columns}
::: {.column width=.2}

:::
::: {.column width=.8}

```{r echo=T}
anova(lm.out)
```

:::
::::::

##

:::::: {.columns}
::: {.column width=.5}

```{r fig.width=3, fig.height=2.8, onslide.plot="1-2"}
.df <- data.frame(y.hat = lm.out$fitted.values, y = Y)
ggplot(.df, aes(y.hat, y)) +
    geom_jitter(stroke=0, color="gray") +
    ggpubr::stat_regline_equation(size=4) +
    ggtitle("Train (N = " %&% nrow(X) %&% ")") +
    geom_smooth(method="lm", se=F, col=2)
```
:::
::: {.column width=.5}

```{r fig.width=3, fig.height=2.8, onslide.plot="2"}
x.test <- cbind(1, .sim$x[- .train, p.adjust(.gwas$pvalue) < .05])
y.test <- .sim$y.q[- .train]
y.hat.test <- x.test %*% as.matrix(coefficients(lm.out), ncol=1)

.df <- data.frame(y.hat = y.hat.test, y = y.test)
ggplot(.df, aes(y.hat, y)) +
    geom_jitter(stroke=0, color="gray") +
    ggpubr::stat_regline_equation(size=4) +
    ggtitle("Test (N = " %&% nrow(x.test) %&% ")") +
    geom_smooth(method="lm", se=F, col=2)
```

:::
::::::


## Do we need all of these GWAS variants?

```{r fig.width=5.5, fig.height=2.5}
.dt <-
    data.table(X.gwas, Y) %>%
    melt(id.vars = "Y")

ggplot(.dt, aes(factor(value, c(0,1,2)), Y, fill=as.factor(value))) +
    xlab("genotype") + ylab("logit phenotype") +
    facet_grid(.~variable) +
    geom_boxplot(show.legend = F) +
    scale_fill_brewer("",palette = "Paired")
```

## Just one of the GWAS variants works fine

```{r}
lm.out <- lm(Y ~ X.gwas[,1])
```

:::::: {.columns}
::: {.column width=.5}

```{r fig.width=3, fig.height=2.5}
.df <- data.frame(y.hat = lm.out$fitted.values, y = Y)
ggplot(.df, aes(y.hat, y)) +
    xlab("prediction") + ylab("training data") +
    geom_jitter(stroke=0, size=1, color="gray") +
    ggpubr::stat_regline_equation(size=4) +
    ggtitle("Train (N = " %&% nrow(X) %&% ")") +
    geom_smooth(method="lm", se=F, col=2)
```
:::
::: {.column width=.5}

```{r fig.width=3, fig.height=3}
x.test <- cbind(1, .sim$x[- .train, p.adjust(.gwas$pvalue) < .05][, 1])
y.test <- .sim$y.q[- .train]
y.hat.test <- x.test %*% as.matrix(coefficients(lm.out), ncol=1)

.df <- data.frame(y.hat = y.hat.test, y = y.test)
ggplot(.df, aes(y.hat, y)) +
    xlab("prediction") + ylab("testing data") +
    geom_jitter(stroke=0, size=1, color="gray") +
    ggpubr::stat_regline_equation(size=4) +
    ggtitle("Test (N = " %&% nrow(x.test) %&% ")") +
    geom_smooth(method="lm", se=F, col=2)
```

:::
::::::

## Can we simply add more variables by GWAS significance?

```{r}
.dt.gwas <-
    data.table(pv = .gwas$pvalue, beta = -.gwas$mean.diff) %>%
    mutate(j = 1:n()) %>%
    mutate(x = as.integer(colnames(.sim$x))) %>% 
    arrange(pv)

pred.gwas <- function(ntop){
    xx <- .sim$x[- .train, .dt.gwas[1:ntop, ]$j, drop = F]
    data.table(y.hat = as.vector(xx %*% .dt.gwas[1:ntop, ]$beta),
               y = .sim$y.cc[- .train],
               y.q = .sim$y.q[- .train])
}

plot.pgs.gwas <- function(ntop){
    .df <- pred.gwas(ntop)
    ggplot(.df, aes(y.hat, y.q)) +
        xlab("prediction") + ylab("testing data") +
        geom_jitter(stroke=0, size=1, color="gray") +
        ggpubr::stat_cor(size=4) +
        ggtitle("Top " %&% ntop %&% ifelse(ntop>1," variants"," variant")) +
        geom_smooth(method="lm", se=F, col=2)
}

cor.pgs.gwas <- function(ntop){
    .df <- pred.gwas(ntop)
    return(cor(.df$y.q, .df$y.hat, method = "spearman"))
}
```

Let's do some experiments:

\large

1. Pick top *K* variants (ordering by GWAS, $p_{(1)} < p_{(2)} < \ldots$)

2. Take GWAS effect sizes $\beta$ (the mean difference between case vs. control)

3. Predict by a linear combination of these effects:

$$\sum_{j=1}^{K} X_{i(j)} \beta_{(j)}$$

## PGS is a variable selection problem

:::::: {.columns}
::: {.column width=.5}

```{r fig.width=3, fig.height=2.5, only.plot="1"}
plot.pgs.gwas(1)
```
```{r fig.width=3, fig.height=2.5, only.plot="2"}
plot.pgs.gwas(10)
```
```{r fig.width=3, fig.height=2.5, only.plot="3"}
plot.pgs.gwas(20)
```
```{r fig.width=3, fig.height=2.5, only.plot="4"}
plot.pgs.gwas(30)
```
```{r fig.width=3, fig.height=2.5, only.plot="5"}
plot.pgs.gwas(50)
```

:::
::: {.column width=.5}

```{r fig.width=3, fig.height=2.5, onslide.plot="5-"}
nn.vec <- c(1:75)
.dt <- data.table(p = nn.vec,
                  r = sapply(nn.vec, cor.pgs.gwas))

ggplot(.dt, aes(`p`, `r`)) +
    geom_line() +
    ylab("Spearman correlation") +
    xlab("Number of variables in PGS")
```

:::
::::::

# What are the statistical challenges in PGS estimation?

## GWAS statistics only roughly tag causal variants

\centerline{\includegraphics[width=.8\linewidth]{img/direct_indirect_associations.pdf}}

\vfill

\tiny

Balding *et al.* Nature Review Genetics (2006)

## Covariance between variants obfuscates GWAS interpretation

:::::: {.columns}
::: {.column width=.3}

* Linkage disequilibrium (LD) block

* The result of recombination events throughout generations

:::
::: {.column width=.7}

\includegraphics[height=.7\textheight]{img/altshuler_daly_lander_LD.pdf}

:::
::::::


## A typical setting of PGS training

::: {.block}
### PGS training

* Input:

    1. GWAS summary statistics (p-values, univariate effect sizes)

	2. $n{\times}p$ genotype matrix $X$, where $X_{ij}\in\{0,1,2\}$

	3. $n{\times}1$ phenotype vector $\mathbf{y}$

* Output:

    - *Polygenic* effect sizes $\hat{\beta}_{1},\ldots,\hat{\beta}_{p}$

* Objective:

    - We want to predict unseen $Y^{\star}_{i}$ by $f(X^{\star}; \hat{\beta})$ as accurately as possible

	- We want $\beta_{j} = 0$ as many as possible; only some $\beta_{j}{\neq}0$.

:::


## Common strategies to deal with LD structures

:::::: {.columns}
::: {.column width=.3}

Strategy 1. Just use them all

:::
::: {.column width=.3}

Strategy 2. Pruning/clumping

:::
::: {.column width=.3}

Strategy 3. **Fine-mapping**

:::
::::::

\vfill

:::::: {.columns}
::: {.column width=.3}
\includegraphics[width=\linewidth]{img/GWAS_LD_strategy_useall.pdf}
:::
::: {.column width=.3}
\includegraphics[width=\linewidth]{img/GWAS_LD_strategy_pruning.pdf}
:::
::: {.column width=.3}
\includegraphics[width=\linewidth]{img/GWAS_LD_strategy_fm.pdf}
:::
::::::


## True causal variables explain a large fraction of variation

```{r echo = TRUE}
.lm <- lm(.sim$y.q ~ .sim$x[, .sim$causal, drop = FALSE] - 1)
```

```{r fig.width = 4, fig.height = 2.8}
.dt <- data.table(y = as.numeric(.sim$y.q), y.hat = fitted(.lm))
.aes <- aes(label =  paste(after_stat(eq.label), after_stat(adj.rr.label), sep = "~~~~"))
ggplot(.dt, aes(y.hat, y)) +
    geom_point(color = "gray45", stroke=0, size=1) +
    geom_abline(slope = 1, color = 2, lty = 2) +
    ggpubr::stat_regline_equation(.aes, size=4) +
    xlab("predicted by causal variants") +
    ylab("true polygenic score")
```



# Statistical fine-mapping to handle LD structures

## Classical statistics does not really help

\large

- Classical variable selection by univariate (one-by-one) tests will not work for a $p \gg n$ regression problem


- Especially if we have col-linearity in the design matrix $X$

- also know as Linkage Disequilibrium (LD) in GWAS/PGS


## Variable selection in high-dim. matrix ($n \ll p$)

Regression analysis = projecting the observed $\mathbf{y}$ vector on to
column space of $\{\mathbf{x}_{j}: j \in[p]\}$,
$$
\left(\begin{array}{l}
y_{1}\\
y_{2}\\
\vdots\\
y_{n}
\end{array}\right)
=
\beta_{1} \left(\begin{array}{l}
X_{11}\\
X_{21}\\
\vdots\\
X_{n1}
\end{array}\right) +
\cdots
\beta_{p} \left(\begin{array}{l}
X_{1p}\\
X_{2p}\\
\vdots\\
X_{np}
\end{array}\right).
$$
Variable selection = column selection.

\vfill

\begin{itemize}
\item <1-> Intuitive idea : choose the best combination of variables. $\to 2^{p}$ choices (even harder).
\item <2-> Alternative idea : make as many $\beta_{j}$'s nearly zero values.
\item <3-> What prior does: penalize $|\beta_{j}| > 0$ so that only the strong
  enough variables take non-zero values.
\end{itemize}

## Lasso, a linear regression with Laplace prior (L1)

Prior distribution
$$
p(\boldsymbol{\theta}) = \textsf{Laplace}(\boldsymbol{\theta}| \lambda) \propto \exp\left(-\lambda\|\boldsymbol{\theta}\|_{1}\right)
$$
where
\large
$$\|\boldsymbol{\theta}\|_{1} = \sum_{j=1}^{p} |\theta_{j}|,\,\textsf{\color{blue}L1-norm}.$$

\normalsize
Maximize
$$
\ln p(\mathbf{y}|X,\boldsymbol{\theta}) + \ln p(\boldsymbol{\theta}|\lambda)
= - \frac{1}{2\sigma^{2}} \sum_{i=1}^{n} (y_{i} - \mathbf{x}_{i} \boldsymbol{\theta})^{2}
- \lambda \|\boldsymbol{\theta}\|_{1}
$$

Minimize $L_{1}$-regularized error
$$
\sum_{i=1}^{n} (y_{i} - \mathbf{x}_{i} \boldsymbol{\theta})^{2}
+ \lambda \|\boldsymbol{\theta}\|_{1}
$$

\vfill

(Tibshirani, 1996)

## `glmnet` solves this regularized optimization problem

Goal (by variable-by-variable updates):\centerline{$\min_{\boldsymbol{\beta}} \quad
\overbrace{(\mathbf{y} - X\boldsymbol{\beta})^{\top}(\mathbf{y} - X\boldsymbol{\beta})}^{\textsf{\color{blue} RSS}} + \underbrace{\lambda \alpha \|\boldsymbol{\beta}\|_{1}}_{\textsf{\color{red} variable selection}} + \underbrace{\lambda (1 - \alpha) \|\boldsymbol{\beta}\|_{2}}_{\textsf{\color{magenta} shrinkage}}$}

\onslide<2->{
For each $\beta_{j}$,
}

\only<2>{$$
\hat{\beta}_{j}^{\textsf{glmnet}} \gets
\frac{S\left(
\sum_{i=1}^{n} X_{ij} (y_{i} - \hat{y}_{i}^{(-j)}),
\lambda\alpha
\right)}
{ \sum_{i=1}^{n} X_{ij}^{2} +\lambda (1- \alpha) }
$$
\tiny
Friedman {\it et al.}, Regularization Paths for Generalized Linear Models via Coordinate Descent (2010)
}

\only<3>{$$
\hat{\beta}_{j} \gets
\frac{\overset{\textsf{\color{red} threshold}}{S}
\left(
\sum_{i=1}^{n} X_{ij} \overbrace{(y_{i} - y_{i}^{(-j)})}^{\textsf{\color{red} residual w/o the variable } \beta_{j} },
\lambda\alpha
\right)}
{ \sum_{i=1}^{n} X_{ij}^{2} + \underbrace{\lambda (1- \alpha)}_{\textsf{\color{magenta} shrinkage}}}
$$
where $S(z, \tau)$ will set it to zero if $|z| < \tau$.
}


## Running `glmnet` for polygenic risk prediction of OGD

```{r}
Y <- .sim$y.q[.train]
X <- .sim$x[.train, , drop = F]
```

```{r echo=TRUE}
glm.cv.out <- glmnet::cv.glmnet(X, Y, nfolds=5, alpha=1)
lambda.cv <- glm.cv.out$lambda.min
glm.out <- glmnet::glmnet(x=X, y=Y, lambda = lambda.cv, alpha=1)
```

```{r}
lasso.dt <-
    data.table(beta.lasso = as.numeric(glm.out$beta)) %>%
    mutate(j = 1:n()) %>%
    left_join(.dt.gwas) %>%
    mutate(l = as.integer(j %in% .sim$causal)) %>%
    arrange(desc(abs(beta.lasso))) %>%
    mutate(lasso = 1:n()) %>%
    as.data.table
```

:::::: {.columns}
::: {.column width=.3}

```{r echo = T}
head(glm.out$beta)
```

:::
::: {.column width=.65}

```{r fig.width=3, fig.height=2, only.plot="2"}
ggplot(lasso.dt, aes(lasso, beta.lasso, fill=as.factor(l))) +
    theme(legend.position = c(1,1)) +
    theme(legend.justification = c(1,1)) +
    xlab("variables (ranked by glmnet)") + ylab("coefficients") +
    geom_point(data=head(lasso.dt, 100), pch=21) +
    geom_hline(yintercept = 0, color="black") +
    scale_x_sqrt() +
    scale_fill_manual("causal", values=c("gray", "red"))
```

:::
::::::


## Sum of Single Effect (SuSiE) regression

```{r run_susie, size="large", echo = T}
library(susieR)
susie.out <- susie(X, Y)
```

\vfill

\large
$$\mathbf{y} = \sum_{l=1}^{L} \underbrace{ \sum_{j} \mathbf{x}_{j} 
    \overset{\textsf{\color{magenta}probabilistic selection}}{\alpha_{j}^{(l)}} 
	\overset{\textsf{\color{teal} single variant effect}}{\beta_{j}^{(l)}} }_{\textsf{layer-by-layer}}  + \boldsymbol{\epsilon}$$

\normalsize
where $\sum_{j=1}^{p} \alpha_{j}^{(l)} = 1$ for each layer $l$.

\vfill

\small

Wang .. Stephens, _Journal of the Royal Statistical Society_ (2020)


## Ideas behind SuSiE

\large

1. Estimating univariate effects is easy (GWAS)

2. Many variants can show similar effects (LD)

3. Let's weight variants probabilistically

4. Regress out the probabilistically reweighted effects

5. Repeat 1-4.

## SuSiE identifies top causal variants

```{r echo = FALSE}
susie.dt <- 
    data.table(pip = susie_get_pip(susie.out)) %>%
    mutate(j = 1:n()) %>% 
    left_join(.dt.gwas) %>%
    mutate(l = as.integer(j %in% .sim$causal)) %>%
    arrange(desc(pip)) %>%
    mutate(susie = 1:n())
```

```{r fig.width=3, fig.height=2.8, echo = FALSE}
ggplot(susie.dt, aes(susie, pip, fill=as.factor(l))) +
    theme(legend.position = c(1,1)) +
    theme(legend.justification = c(1,1)) +
    xlab("variables (ranked by SuSiE)") +
    ylab("posterior probability") +
    geom_point(data=head(susie.dt, 100), pch=21) +
    geom_hline(yintercept = 0, color="black") +
    scale_x_sqrt() +
    scale_fill_manual("causal", values=c("gray", "red"))
```

## SuSiE can avoid the col-linearity (LD) problem

```{r fig.width=6, fig.height=2, echo = FALSE, only.plot="1"}
logit <- function(x) log(x) - log(1-x)
sigmoid <- function(x) 1/(1+exp(-x))

.aes <- aes(x, logit(pmin(pmax(pip,1e-2),1-1e-2)), colour=as.factor(l))

p1 <-
    ggplot(susie.dt[order(`pip`)], .aes) +
    theme(legend.position = c(1,1)) +
    theme(legend.justification = c(1,1)) +
    xlab("variables (along genomic axis)") +
    scale_y_continuous("posterior probability", labels=function(x) round(sigmoid(x),2)) +
    geom_vline(data=susie.dt[pip > .5], aes(xintercept=x), lty=2, size=.5, color=3) +
    ggrastr::rasterise(geom_point(stroke=0, size=1), dpi=300) +
    geom_hline(yintercept = 0, color="black", lty = 2, size = .2) +
    scale_colour_manual("causal", values=c("gray", "red"))
p1
```

```{r fig.width=6, fig.height=3, echo = FALSE, only.plot="2"}
p2 <-
    ggplot(susie.dt[order(`l`)], aes(x, -log10(pv), colour=as.factor(l))) +
    geom_vline(data=susie.dt[pip > .5], aes(xintercept=x), lty=2, size=.5, color=3) +
    ggrastr::rasterise(geom_point(stroke = 0, size=1), dpi=300) +
    scale_colour_manual("causal", values=c("gray", "red"), guide=FALSE) +
    xlab("variables (along genomic axis)")
p1/p2
```

```{r fig.width=6, fig.height=3.5, echo = FALSE, only.plot="3"}
susie_plot(susie.out, y = "PIP", add_bar = TRUE, add_legend = TRUE)
```

```{r fig.width=6, fig.height=3.5, echo = FALSE, only.plot="4"}
susie_plot(susie.out, y = "log10PIP", add_legend = TRUE)
```

## SuSiE+PGS accurately predict the held-out data

```{r}
x.test <- .sim$x[- .train, , drop = F]
y.test <- .sim$y.q[- .train]
```

```{r echo = T, tidy=T}
y.hat <- predict.susie(susie.out, x.test)
```

```{r fig.width = 4, fig.height = 2.5}
.dt <- data.table(y = y.test, y.hat = y.hat)
.aes <- aes(label =  paste(after_stat(eq.label), after_stat(adj.rr.label), sep = "~~~~"))
ggplot(.dt, aes(y.hat, y)) +
    geom_point(color = "gray45", stroke=0, size=1) +
    geom_abline(slope = 1, color = 2, lty = 2) +
    ggpubr::stat_regline_equation(.aes, size=4) +
    xlab("predicted by causal variants") +
    ylab("true polygenic score")
```

## A general workflow of PGS estimation

\only<1>{\centerline{\includegraphics[height=.7\textheight]{img/PGS_Choi_Protocol_1.pdf}}}

\only<2>{\centerline{\includegraphics[height=.7\textheight]{img/PGS_Choi_Protocol_2.pdf}}}

\vfill

\small

Choi .. O'Reilly, Nature Protocol, (2020)


## A proposed workflow of PGS estimation

\large

1. Take GWAS summary statistics and target population genotype $X$

2. Run SuSiE^[SuSiE can run with summary statistics only] chromosome by chromosome to select variables

3. Predict PGS per chromosome and aggregate them

<!------------->
<!-- Wrap up -->
<!------------->

## Advanced topics that we might consider next lecture

\large

- Summary-based GWAS, post-GWAS analysis

- Mendelian Randomization and other causal inference techniques


# Other topics




##

$${}$$

## Today's lecture

\Large

1. Why do we want to build a polygenic score model?

2. What is a polygenic score model?

3. How to estimate a reliable PGS model by variable selection

## PGS Catalog: You may not need to train everything from the scratch

\large

`https://www.pgscatalog.org/`

\vfill

\centerline{\includegraphics[height=.6\textheight]{img/PGS_catalog.png}}


