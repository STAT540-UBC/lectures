---
title: "Linear models and ANOVA"
title-slide-attributes:
  data-background-color: "#197aa0"
  data-background-opacity: "0.9"
  data-background-image: "https://github.com/STAT540-UBC/stat540-ubc.github.io/raw/main/images/stat540-logo-s.png"
  data-background-size: 12%
  data-background-position: 95% 90%
author: "Keegan Korthauer"
date: "26 January 2023"
date-format: long
format: 
  revealjs:
    chalkboard: true
    slide-number: c/t
    width: 1600
    height: 900
    logo: "https://github.com/STAT540-UBC/stat540-ubc.github.io/raw/main/images/stat540-logo-s.png"
    echo: true
    theme: [default, ../custom.scss]
    show-notes: false
---

## Recap: Are these genes different in NrlKO vs WT?

H~0~: the expression level of gene $g$ is the same in both genotypes

Is there **enough** evidence in the data to reject H~0~?


```{r}
#| include: false
library(tidyverse)
library(GEOquery)
library(gridExtra)
theme_set(theme_bw(base_size = 20))

eset <- getGEO("GSE4051", getGPL = FALSE)[[1]]

# recode time points
pData(eset) <- pData(eset) %>%
  mutate(sample_id = geo_accession) %>%
  mutate(dev_stage =  case_when(
    grepl("E16", title) ~ "E16",
    grepl("P2", title) ~ "P2",
    grepl("P6", title) ~ "P6",
    grepl("P10", title) ~ "P10",
    grepl("4 weeks", title) ~ "P28"
  )) %>%
  mutate(genotype = case_when(
    grepl("Nrl-ko", title) ~ "NrlKO",
    grepl("wt", title) ~ "WT"
  ))

pData(eset) <- pData(eset) %>%
  mutate(dev_stage = fct_relevel(dev_stage, "E16", "P2", "P6", "P10", "P28")) %>%
  mutate(genotype = as.factor(genotype)) %>%
  mutate(genotype = fct_relevel(genotype, "WT", "NrlKO"))

toLongerMeta <- function(expset) {
    stopifnot(class(expset) == "ExpressionSet")
    
    expressionMatrix <- lonExpressionressionMatrix <- exprs(expset) %>% 
    as.data.frame() %>%
    rownames_to_column("gene") %>%
    pivot_longer(cols = !gene, 
                 values_to = "expression",
                 names_to = "sample_id") %>%
    left_join(pData(expset) %>% select(sample_id, dev_stage, genotype),
            by = "sample_id")
  return(expressionMatrix)
}

twoGenes <- toLongerMeta(eset) %>% 
  filter(gene %in% c("1422248_at", "1450946_at")) %>%
  mutate(gene = ifelse(gene == "1422248_at", "Irs4", "Nrl")) 
```

```{r}
#| echo: false
#| fig-align: center
#| fig-width: 11
#| fig-height: 4
irsLim <- twoGenes %>%
  filter(gene == "Irs4") %>%
  ggplot(aes(y = expression, x = genotype, 
                             colour = genotype)) + 
             geom_jitter(alpha = 0.8, width = 0.2) +
             labs(title = "Irs4 gene") +
             ylim(5, 13) +
             theme(legend.position = "none")
             

nrlLim <- twoGenes %>%
  filter(gene == "Nrl") %>%
  ggplot(aes(y = expression, x = genotype, 
                             colour = genotype)) + 
             geom_jitter(alpha = 0.8, width = 0.2) +
             labs(title = "Nrl gene") +
             ylim(5, 13) +
             theme(legend.position = "none") 

grid.arrange(irsLim, nrlLim, ncol = 2)
```

## Learn about a population from a random sample

::: columns
::: column
**Population** (Unknown)
$$Y \sim F, \,\, Z \sim G$$
$$E[Y] = \mu_Y, \,\, E[Z] = \mu_Z$$
$$Var[Y] = {\sigma}_Y^2,\,\, Var[Z] = {\sigma}_Z^2$$
$$H_0: \mu_Y = \mu_Z$$

$$H_A: \mu_Y \neq \mu_Z$$

:::
::: column
**Sample** (Observed, with randomness)

$$(Y_1, Y_2, ..., Y_{n_Y}) \text{ and } (Z_1, Z_2, ..., Z_{n_Z})$$
$$\hat{\mu}_Y = \bar{Y} = \frac{\sum_{i=1}^{n_Y} Y_i}{n_Y}$$

$$\hat{\sigma}_Y^2 = S_Y^2=\frac{1}{n_Y}\sum_{i=1}^{n_Y}(Y_i-\bar{Y})^2$$

(with similar quantities for $Z:$ $\bar{Z}$ and $S^2_Z)$

$$T = \frac{\bar{Y}-\bar{Z}}{\sqrt{\hat{Var}(\bar{Y}-\bar{Z}))}}$$
:::
:::
$\bar{Y}, \bar{Z}, S^2_Y, S^2_Z$ and $T$ are examples of **statistics** computed from the sample

## Summary: Hypothesis testing

1. Formulate scientific hypothesis as a **statistical hypothesis** $(H_0 \text{ vs } H_A)$

2. Define a **test statistic** to test $H_0$ and compute its **observed value**. For example:
  - 2-sample *t*-test
  - Welch's *t*-test (unequal variance)
  - Wilcoxon rank-sum test
  - Kolmogorov-Smirnov test

3. Compute the probability of seeing a test statistic as extreme as that observed, under the **null sampling distribution** (p-value) 
4. Make a decision about the **significance** of the results, based on a pre-specified significance level ( $\alpha$ )

## We can run these tests in R 

Example: use the `t.test` function to test H~0~ using a 2-sample *t*-test with equal variance:

```{r}
filter(twoGenes, gene == "Irs4") %>%
  t.test(expression ~ genotype, data = ., var.equal = TRUE)
```


## Discussion recap

* What test should I use?
  - What test(s) might be appropriate if your sample size is just barely large enough to invoke CLT, but you also have suspected outliers?
  - If more than one test is appropriate (e.g. *t*-test, Wilcoxon, and KS), which should we report?
  - What should you do if methods that are equally appropriate and defensible give very different answers?

* What is generally more important for results interpretation: the effect size or the p-value?

::: {.notes}
What test should I use?

* How to decide which test to carry out (e.g. t-test, Wilcoxon, KS)?

* Are assumptions met for each one?

  * If so, parametric tests (e.g. t-test) tend to have slightly higher power (ability to reject H~0~ when H~0~ is false)
  
  * But if assumptions are potentially violated, non-parametric tests (Wilcoxon, KS) are a safer choice (albeit conservative)
:::

## Today's Learning Objectives

1. Compare means of different groups (2 or more) using a **linear regression model**

2. Use 'indicator' variables to represent the levels of a qualitative explanatory variable

2. Write a linear model using matrix notation and understand which matrix is built by R
  
3. Distinguish between **single** and **joint** hypothesis tests (e.g. $t$-tests vs $F$-tests)

## 3 ways to test H~0~: $\mu_1 = \mu_2$

::: {.panel-tabset}
# t-test
**2-sample t-test (with equal variance)**
```{r}
#| code-line-numbers: "2"
filter(twoGenes, gene == "Irs4") %>%
  t.test(expression ~ genotype, data = ., var.equal = TRUE)
```

# ANOVA
**(One-way) Analysis of Variance (ANOVA)**
```{r}
#| code-line-numbers: "2"
filter(twoGenes, gene == "Irs4") %>%
  aov(expression ~ genotype, data = .) %>%
  summary()
```

# linear regresion
**Linear Regression**^[Note differences in sign between t-test & linear regression: pay attention to which group is 'reference']
```{r}
#| code-line-numbers: "2"
filter(twoGenes, gene == "Irs4") %>%
  lm(expression ~ genotype, data = .) %>%
  summary()
```

:::

```{r}
#| echo: false
irs4.ttest <- filter(twoGenes, gene == "Irs4") %>%
  t.test(expression ~ genotype, data = ., var.equal = TRUE)
irs4.aov <- filter(twoGenes, gene == "Irs4") %>%
  aov(expression ~ genotype, data = .) %>%
  summary()
irs4.lm <- filter(twoGenes, gene == "Irs4") %>%
  lm(expression ~ genotype, data = .) %>%
  summary()
```

## These are not coincidences!

::: {.panel-tabset}
# t-test
**2-sample t-test (with equal variance)**

```{r}
#| echo: false
list("t statistic" = irs4.ttest$statistic, 
     "p-value" = irs4.ttest$p.value,
     "mean difference" = as.numeric(-diff(irs4.ttest$estimate)),
     "(t statistic)^2" = irs4.ttest$statistic^2)
```
# ANOVA
**(One-way) Analysis of Variance (ANOVA)**^[Note that the t-statistic squared is equal to the ANOVA F statistic]
```{r}
#| echo: false
list("F statistic" = irs4.aov[[1]]$`F value`[1], 
     "p-value" = irs4.aov[[1]]$`Pr(>F)`[1])
```

# linear regresion
**Linear Regression**
```{r}
#| echo: false
list("t statistic" = irs4.lm$coeff[2,3],
     "p-value" = irs4.lm$coeff[2,4],
     "coefficient estimate" = irs4.lm$coeff[2,1])
```

:::

. . . 

::: {.callout-tip}
# Key Question
Why are these giving us the same results?
:::

## *t*-test vs *line*ar regression: where's the *line*^[Note that the $x$-axis in these plots is not numerical, thus a line in this space does not have any mathematical meaning.]?

```{r echo=FALSE, fig.width=10, fig.height= 4, fig.align="center"}
#| echo: false
#| fig-align: center
#| fig-width: 10
#| fig-height: 4
grid.arrange(irsLim, nrlLim, ncol = 2)
```

. . .

::: {.callout-tip}
# Key Question
Why can we run a t-test with a **linear** regression model?
:::

## From *t*-test to linear regression

Let's change the notation to give a common framework to all methods

$$Y \sim G; \; E[Y] = \mu_Y$$
 <center> **↓** </center>

$$Y = \mu_Y + \varepsilon_Y; \; \varepsilon_Y \sim G; \; E[\varepsilon_Y] = 0$$ 

. . .

### Why is this equivalent?

$$E[Y] = E[\mu_Y + \varepsilon_Y] = \mu_Y + E[\varepsilon_Y] = \mu_Y$$
We are just rewriting $Y$ here


## From *t*-test to linear regression


Let's change the notation to give a common framework to all methods

$$Y \sim G; \; E[Y] = \mu_Y$$
 <center> **↓** </center>

$$Y = \mu_Y + \varepsilon_Y; \; \varepsilon_Y \sim G; \; E[\varepsilon_Y] = 0$$ 


### We can use indices to accommodate multiple groups, i.e., 

$$Y_{ij} = \mu_j + \varepsilon_{ij};\; \; \varepsilon_{ij} \sim G_j; \; \;E[\varepsilon_{ij}] = 0;$$

where $j = \textrm{\{WT, NrlKO}\}$ (or $j=\textrm{\{1, 2}\}$ ) identifies the groups; 
and $i=1, \ldots, n_j$ identifies the observations within each group

. . .

For example: $Y_{11}$ is the first observation in group 1 or WT
 

## This is called the **cell-means model**

Using data from the model

$$Y_{ij} = \mu_j + \varepsilon_{ij};\; \; \varepsilon_{ij} \sim G; \; \;E[\varepsilon_{ij}] = 0;$$

where $j$ indexes groups (e.g. WT vs NrlKO) and $i$ indexes samples within group, the goal is to test $H_0 : \mu_1 = \mu_2$

::: {.callout-note}
In the **cell-means** model parameterization, we have a parameter $E[Y_{ij}] = \mu_j$ that represents the population mean of each group (in our example: genotype) 
:::

::: {.callout-important}
We assume a common distribution $G$ for all groups (equal variance assumption)
:::

. . . 

Why the name? 'Cell' here refers to a cell of a table - e.g. make a table of means by group, and $\mu_j$ represents the population value for each cell $j$ in the table

## Recall: sample mean estimator of population mean 

* For each group $j$, the **population** mean is given by $E[Y_{ij}] = \mu_j$

* A natural *estimator* of the population mean $\mu_j$ is the **sample** mean $\hat{\mu}_j = \bar{Y_j} = \frac{\sum_{i=1}^{n_j}Y_{ij}}{n_j}$

* Recall that the `t.test` function calculates these for us in R

## But why does the `lm` function report different estimates?

::: {.panel-tabset}

# `t.test`
```{r}
# t.test
filter(twoGenes, gene == "Irs4") %>%
  t.test(expression ~ genotype, data = ., var.equal = TRUE)
```

# `lm`
```{r}
# lm
filter(twoGenes, gene == "Irs4") %>%
  lm(expression ~ genotype, data = .) %>% 
  summary()
```
:::

. . . 

* `(Intercept)` estimate from `lm` is the **sample mean** of WT group 
* `genotypeNrlKO` estimate from `lm` is **not** the sample mean of the NrlKO group... what is it?

## Parameterization: how to write the model?

- By default, the `lm` function does not use the cell-means parameterization 

- Usually, the goal is to *compare* the means, not to study each in isolation

Let's let $\theta = \mu_1$ and rewrite $\mu_j = \theta + \tau_j$, and plug into **cell-means** $(\mu_j)$ model:  $$Y_{ij} = \mu_j + \varepsilon_{ij};\; \; \varepsilon_{ij} \sim G; \; \;E[\varepsilon_{ij}] = 0;$$

<center>↓</center>

This gives us the **reference-treatment effect** $(\theta,\tau_j)$ model: $$Y_{ij} = \theta+\tau_j + \varepsilon_{ij};\; \; \tau_1=0, \; \; \varepsilon_{ij} \sim G; \; \;E[\varepsilon_{ij}] = 0;$$


## Reference-treatment effect parameterization

**Reference-treatment effect** $(\theta,\tau_j)$ model: $$Y_{ij} = \theta+\tau_j + \varepsilon_{ij};\; \; \tau_1=0, \; \; \varepsilon_{ij} \sim G; \; \;E[\varepsilon_{ij}] = 0;$$

* Note that for each group, the population mean is given by $E[Y_{ij}] = \theta+\tau_j=\mu_j,$
and $\tau_2=\mu_2-\mu_1=E[Y_{i2}] -E[Y_{i1}]$ *compares* the means

* $\tau_1$ must be set to zero, since group 1 is the *reference* group

. . .

::: {.callout-note}
In the **reference-treatment effect** model parameterization, we have the following parameters:

- $\theta$ represents the population mean of the reference group (in our example: WT) 
- $\tau_j$ represents the difference in the population mean of group $j$ compared to the reference (in our example: NrlKO - WT)
:::


## Relation between parameterizations

![](img/param_2.png){fig-align="center"}

## `lm` output

* the sample mean of the WT group (**reference**): $\hat\theta$ 
* the difference in sample mean of NrlKO and WT groups (**treatment effect**): $\hat\tau_2$

::: {.panel-tabset}
# Irs4
```{r}
#| code-fold: true
library(broom)
filter(twoGenes, gene == "Irs4")  %>%
  group_by(genotype) %>%
  summarize(meanExpr = mean(expression)) %>%
  pivot_wider(names_from = genotype, values_from = meanExpr) %>% 
  mutate(diffExp = NrlKO - WT)

filter(twoGenes, gene == "Irs4")  %>%
  lm(expression ~ genotype, data = .) %>% tidy()
```
# Nrl
```{r}
#| code-fold: true
library(broom)
filter(twoGenes, gene == "Nrl")  %>%
  group_by(genotype) %>%
  summarize(meanExpr = mean(expression)) %>%
  pivot_wider(names_from = genotype, values_from = meanExpr) %>% 
  mutate(diffExp = NrlKO - WT)

filter(twoGenes, gene == "Nrl")  %>%
  lm(expression ~ genotype, data = .) %>% tidy()
```
:::


## We still haven't answered our question... where's the line?? 

$$Y_{ij} = \theta+\tau_j + \varepsilon_{ij};\; \; \tau_1=0, \; \; \varepsilon_{ij} \sim G; \; \;E[\varepsilon_{ij}] = 0;$$

```{r}
#| echo: false
#| fig-width: 10
#| fig-height: 5
#| fig-align: "center"
grid.arrange(irsLim, nrlLim, ncol = 2)
```

## Indicator variables

Let's re-write our model using **indicator** (aka 'dummy') variables:

$$Y_{ij} = \theta+\tau_j + \varepsilon_{ij}\;\; \text{where} \; \; \tau_1=0; \; \; \varepsilon_{ij} \sim G; \; \;E[\varepsilon_{ij}] = 0;$$
<center>**↓**</center>

$$Y_{ij} = \theta+\tau x_{ij} + \varepsilon_{ij} \;\; \text{where} \; \; x_{ij}=\bigg\{\begin{array}{l} 
1\text{ if } j=2\\
0 \text{ otherwise}\\
\end{array}$$

. . . 

::: {.callout-note}
Note that $Y_{i1} = \theta + \varepsilon_{i1}$, because $x_{i1}=0$ and $Y_{i2} = \theta + \tau+ \varepsilon_{i2}$, because $x_{i2}=1$ (for all $i$)
:::

The second form is written as a *linear* ( $y=a + bx +\varepsilon$ ) regression model, with a (**indicator**) explanatory variable $x_{ij}$


  
## t-test with a linear model

::: {.callout-note}
Using indicator variables to model our categorical variable `genotype`, we can perform a  **2-sample t-test** with a linear model
:::

$$Y_{ij} = \theta+\tau x_{ij} + \varepsilon_{ij}\;\text{where}\; \; x_{ij}=\bigg\{\begin{array}{l}
1 \text{ if } j=2\\
0 \text{ if } j=1\\
\end{array}$$

- The standalone t-test is carried out on $H_0: \mu_1 = \mu_2$

- The t-test in the linear model is carried out on $H_0: \tau = 0$, where $\tau$ is the difference in population means (here NrlKO - WT)

- Recall that $\tau = \mu_2 - \mu_1$ - this is why these are equivalent tests!

## Beyond 2-group comparisons

![](img/more_2_groups.png){fig-align="center"}

## 

::: {.callout-note}
Indicator variables can be used to model one *or more* categorical variables, each with 2 *or more* levels!
:::

**2-sample *t*-test** using a linear model

$$Y_{ij} = \theta+\tau x_{ij} + \varepsilon_{ij}\;\; \text{where} \; \; x_{ij}=\bigg\{\begin{array}{l}
1 \text{ if } j=2\\
0 \text{ if } j=1\\
\end{array}$$

**1-way ANOVA with many levels**^[in general; yet *another* parameterization can be used to present ANOVA] using a linear model - e.g for 3 groups:
$$Y_{ij} = \theta+\tau_2 x_{ij2} + \tau_3 x_{ij3} +\varepsilon_{ij}\;\; \text{where} \; x_{ij2}=\bigg\{\begin{array}{l}
1\text{ if } j=2\\
0 \text{ otherwise}\\
\end{array}\; \text{ and } \; x_{ij3}=\bigg\{\begin{array}{l}
1\text{ if } j=3\\
0 \text{ otherwise}\\
\end{array}$$

::: {.callout-important}
This equivalence is why R can estimate all of them with `lm()`
:::


## Connections

::: .callout-important
* The **t-test** is a special case of **ANOVA**, but with ANOVA you can compare **more than two groups** and **more than one factor**.

* **ANOVA** is a special case of **linear regression**, but with linear regression you can include **quantitative variables** in the model. 

* **Linear regression** provides a unifying framework to model the association between a response and **many quantitative and qualitative variables**.  

* **In R** all three can be computed using the `lm()` function. 
:::


## Linear models using matrix notation


![](img/linear_form.png){fig-align="center"}

It will become handy to write our model using matrix notation


## Design matrix

Let's form a **[design matrix](http://genomicsclass.github.io/book/pages/expressing_design_formula.html)** $(X)$ for a 3-group comparison

::: columns
::: column
$$Y_{ij} = \theta+\tau_2  x_{ij2} + \tau_3 x_{ij3} +\varepsilon_{ij}$$

![](img/model_matrix_I.png)
:::
::: column
First column in $X$ for reference treatment parameterization is all 1s

Second & third columns contain $x_{ij2}$ and $x_{ij3}$:

* $x_{i12}=x_{i13}=0$ for the reference group

* $x_{i22}=1$ for the 2nd group 

* $x_{i33}=1$ for the 3rd group
:::
:::

##

![](img/model_matrix_II.png)

 <font color = "red"> $Y_{i1}= 1 \times \theta + 0 \times \tau_2 + 0 \times \tau_3 + \varepsilon_{i1} =\theta + \varepsilon_{i1}$</font>

 <font color = "blue"> $Y_{i2}= 1 \times \theta + 1 \times \tau_2 + 0 \times \tau_3 + \varepsilon_{i2}=\theta + \tau_2+\varepsilon_{i2}$</font>

 <font color = "green"> $Y_{i3}= 1 \times \theta + 0 \times \tau_2 + 1 \times \tau_3 + \varepsilon_{i3}=\theta + \tau_3+\varepsilon_{i3}$</font>

<font color = "black"> $$\; Y_{ij} = \theta +\tau_2  x_{ij2} + \tau_3  x_{ij3} + \varepsilon_{ij}$$</font>

##

![](img/rf_tx_matrix.png){fig-align="center"}

The model is still written with a reference-treatment parameterization (difference of means)

$E[Y_{i1}]=\theta$


$E[Y_{i2}]=\theta+\tau_2 \; \rightarrow \tau_2=E[Y_{i2}]-E[Y_{i1}]=\mu_2-\mu_1$


$E[Y_{i3}]=\theta+\tau_3 \; \rightarrow \tau_3=E[Y_{i3}]-E[Y_{i1}]=\mu_3-\mu_1$


## Linear^[Here we mean **linear** in the parameters $\boldsymbol{\alpha}$; $X$ can contain $x^2$, $log(x)$, etc] regression can include *quantitative* & *qualitative* covariates 


![](img/LM_vbles.png){fig-align="center"}


## How it works in practice using `lm()` in R

$$Y = X\alpha + \varepsilon$$ 
<center>**↓**</center>

```{r}
#| eval: false
lm(y ~ x, data = yourData)
```

::: columns
::: column
**`y ~ x`:**  formula

**`y`:** numeric

**`x`:** numeric and/or factor
:::
::: column
**`yourData`:** `data.frame` (or `tibble`) in which `x` and `y` are to be found 
:::
:::

By default, R uses the reference-treatment parameterization^[but [you can change that](http://genomicsclass.github.io/book/pages/expressing_design_formula.html)!] 


## `factor` class in R 

Mathematically, the design matrix $X$ in $Y=X\alpha+\varepsilon$ needs to be a numeric matrix

::: {.callout-important}
- If your data contains categorical variables (e.g., `genotype`), you need to set them as **factors**

  * especially important if your categorical variables are encoded numerically!!
  
  * `lm` will automatically treat character variables as factors)

- R creates appropriate indicator variables (numeric) for factors!
:::

```{r}
str(twoGenes$genotype)
```

## Under the hood, R creates a numeric $X$


```{r}
# create design matrix
mm <- model.matrix( ~ genotype, data = twoGenes) 

# show first 3 and last 3 rows of model.matrix
head(mm, 3) 
tail(mm, 3)

# show first 3 and last 3 values of genotype
twoGenes %>% 
  slice(c(1:3, (n()-3):n())) %>%
  pull(genotype)
```

## Beyond 2-group comparisons in our case study

::: {.callout-note}
# Biological question
Is the expression of gene X the same at all developmental stages?
:::

. . .

$$H_0 : \mu_{E16} = \mu_{P2} = \mu_{P6} = \mu_{P10} = \mu_{P28}$$

. . .

Let's look at another two genes for some variety

```{r, include=FALSE}
#| echo: false
twoGenes <- toLongerMeta(eset) %>% 
  filter(gene %in% c("1440645_at", "1443184_at")) %>%
  mutate(gene = ifelse(gene == "1440645_at", "BB114814", "Cdc14a")) 
```

```{r}
#| echo: false
#| fig-width: 12.5
#| fig-height: 4
#| fig-align: "center"
geneA <- twoGenes %>% filter(gene == "BB114814") %>%
  ggplot(aes(x = dev_stage, y = expression)) + 
             geom_jitter(width = 0.2, alpha = 0.5) +
             labs(title = "BB114814") +
             theme(legend.position = "none") +
             ylim(5, 10) +
             xlab("") +
             stat_summary(aes(group=1), fun=mean, geom="line", colour="red")

geneB <- twoGenes %>% filter(gene == "Cdc14a") %>%
  ggplot(aes(x = dev_stage, y = expression)) + 
             geom_jitter(width = 0.2, alpha = 0.5) +
             labs(title = "Cdc14a") +
             theme(legend.position = "none") +
             ylim(5, 10) +
             xlab("") +
             stat_summary(aes(group=1), fun=mean, geom="line", colour="red")

grid.arrange(geneB, geneA, nrow = 1)
```

## The sample means: $\hat\mu_{E16}, \; \hat\mu_{P2}, \; \hat\mu_{P6}, \; \hat\mu_{P10}, \; \hat\mu_{P28}$

::: columns
::: column
``` {r, message=FALSE}
#| message: false
twoGenes %>% 
  group_by(gene, dev_stage) %>%
  summarize(meanExpr = mean(expression)) %>%
  pivot_wider(values_from = meanExpr, names_from = gene)
```
:::
::: column
```{r}
#| echo: false
#| fig-width: 6
#| fig-height: 7.5
#| fig-align: "center"
grid.arrange(geneB, geneA, nrow = 2)
```
:::
:::


## BB114814 gene with notable time effect

``` {r}
twoGenes %>% filter(gene == "BB114814") %>%
  group_by(dev_stage) %>%
  summarize(cellMeans = mean(expression)) %>%
  mutate(timeEffect = cellMeans - cellMeans[1])
```

"Effect" here is relative to reference/baseline (E16)


## BB114814 gene with notable time effect

::: columns
::: column
``` {r, echo = FALSE}
(BB.means <- twoGenes %>% filter(gene == "BB114814") %>%
  group_by(dev_stage) %>%
  summarize(cellMeans = mean(expression)) %>%
  mutate(timeEffect = cellMeans - cellMeans[1]))
```
:::
::: column
```{r, echo=FALSE, fig.height= 6, fig.align="center", fig.width=8, warning=FALSE}
twoGenes %>% filter(gene == "BB114814") %>%
  ggplot(aes(x = dev_stage, y = expression)) + 
             geom_jitter(width = 0, alpha = 0.15, size = 2) +
             labs(title = "BB114814") +
             theme(legend.position = "none") +
             ylim(5, 10) +
             xlab("") +
             stat_summary(aes(group=1), fun=mean, geom="line", colour="red") +
  geom_text(aes(x="P28", y = BB.means$cellMeans[1]), 
            label = expression(paste(hat(mu)["E16"], "=", hat(theta))), 
            size = 7, nudge_x = -0.1, colour = "blue") +
  geom_text(aes(x="P28", y = BB.means$cellMeans[5]), 
            label = expression(paste(hat(mu)["P28"])), 
            size = 7, nudge_x = -0.2, colour = "blue") +
  geom_segment(aes(x="P28", y=BB.means$cellMeans[1], 
                   xend="P28", yend=BB.means$cellMeans[5]), 
               arrow=arrow(ends="both", length=unit(0.1, "inches"))) +
  geom_text(aes(x="P28", y = (BB.means$cellMeans[1]+BB.means$cellMeans[5])/2), 
            label = expression(paste(hat(tau)["P28"])), 
            size = 7, nudge_x = 0.2, colour = "blue") 
  
```
::: 
:::

. . .

::: {.callout-tip}
# Check your understanding
Can you guess the size of the $X$ matrix needed to test for any time differences? How many indicator variables do we need?
:::

## Gene BB114814 with notable time effect

We need `__` indicator variables to estimate and test `__` time differences (between `__` time points):

::: {.notes}
We need 4 indicator variables to estimate and test 4 time differences (between 5 time points):
- $x_{P2}$: P2 vs E16
- $x_{P6}$: P6 vs E16
- $x_{P10}$: P10 vs E16
- $x_{P28}$: P28 vs E16
:::

. . .

Mathematically:

$$Y_{ij}=\theta+\tau_{P2} x_{ijP2}+\tau_{P6} x_{ijP6}+\tau_{P10} x_{ijP10}+\tau_{P28} x_{ijP28}+\varepsilon_{ij}$$

**Notation**: $x_{ijk}$: 

- $i$ indexes for the observation/sample within group
- $j$ indexes the group (here: level of `dev_stage`)
- $k$ is the name of the indicator variable

## Under the hood, R creates a numeric $X$

```{r}
str(twoGenes)
model.matrix( ~ dev_stage, data = twoGenes)
```

## Hypothesis tests in `lm` output

```{r}
#| echo: false
BB.means
```

```{r}
twoGenes %>% filter(gene == "BB114814") %>%
  lm(expression ~ dev_stage, data = .) %>% tidy()
```


::: columns
::: column
$H_0: \theta=0$ or $H_0: \mu_{E16}=0$

**Estimate**: $\hat\theta=\hat\mu_{E16}=\bar{Y}_{\cdot E16}$
:::
::: column
> we are not usually interested in testing this hypothesis: baseline mean = 0

:::
:::

---

## Hypothesis tests in `lm` output


```{r}
#| echo: false
BB.means
```

```{r}
twoGenes %>% filter(gene == "BB114814") %>%
  lm(expression ~ dev_stage, data = .) %>% tidy()
```


::: columns
::: column
$H_0: \tau_{P2}=0$ or $H_0: \mu_{P2}=\mu_{E16}$

**Estimate**: $\hat{\tau}_{P2}=\hat{\mu}_{P2}-\hat{\mu}_{E16}=\bar{Y}_{\cdot P2}-\bar{Y}_{\cdot E16}$
:::
::: column
> we *are* usually interested in testing this hypothesis: change from E16 to 2 days old = 0

:::
:::

## Hypothesis tests in `lm` output

```{r}
#| echo: false
BB.means
```

```{r}
twoGenes %>% filter(gene == "BB114814") %>%
  lm(expression ~ dev_stage, data = .) %>% 
  tidy()
```


::: columns
::: column
$H_0: \tau_{P28}=0$ or $H_0: \mu_{P28}=\mu_{E16}$

**Estimate**: $\hat\tau_{P28}=\hat\mu_{P28}-\hat\mu_{E16}=\bar{Y}_{\cdot P28}-\bar{Y}_{\cdot E16}$
:::
::: column
> we *are* usually interested in testing this hypothesis: change from E16 to 4 weeks old = 0

:::
:::

## Notice the standard error estimates

```{r}
#| code-fold: true
twoGenes %>% filter(gene == "BB114814") %>%
  lm(expression ~ dev_stage, data = .) %>%
  summary() %>% .$coef
```
. . . 

All data points are used to estimate the variance of the error term for the indicator variables


## Two types of null hypotheses: single vs joint
$$Y = X \alpha + \varepsilon$$
$$\alpha = (\theta, \tau_{P2}, \tau_{P6}, \tau_{P10}, \tau_{P28})$$
      
::: columns
::: column
$H_0: \tau_j = 0$
vs
$H_0: \tau_j \neq 0$

**for each *j* individually**
          
          
For example: Is gene *A* differentially expressed 2 days after birth (compared to embryonic day 16)?

$$H_0: \tau_{P2}=0$$

::: {.callout-note}
This single hypothesis can be tested with a **t-test**
:::

:::
::: column
$H_0: \tau_j = 0$
        vs
$H_0: \tau_j \neq 0$

**for all *j* at the same time**
        
For example: Is gene *A* significantly affected by time? In other words, is gene *A* differentially expressed at *any* time point?
        
$$H_0: \tau_{P2}=\tau_{P6}=\tau_{P10}=\tau_{P28}=0$$

::: {.callout-tip}
# Key Question
How do we test this joint hypothesis?
:::

:::
:::

## *F*-test and overall significance of one or more coefficients

- the *t*-test in linear regression allows us to test single hypotheses:
      $$H_0 : \tau_j = 0$$
      $$H_A : \tau_j \neq 0$$
- but we often like to test multiple hypotheses *simultaneously*: 
      $$H_0 : \tau_{P2} = \tau_{P6} = \tau_{P10} = \tau_{P28}=0\textrm{ [AND statement]}$$
      $$H_A : \tau_j \neq 0 \textrm{ for some j [OR statement]}$$
- the **_F_-test** allows us to test such compound tests

   * more on this type of test next week

## Single and joint tests in `lm` output

Can you locate the results of  each type of test in the `lm` output?

$H_0: \tau_j = 0$ vs $H_0: \tau_j \neq 0$ for each $j$ **individually**

$H_0: \tau_j = 0$ vs $H_0: \tau_j \neq 0$ for all $j$ **together**

```{r}
twoGenes %>% filter(gene == "BB114814") %>%
  lm(expression ~ dev_stage, data = .) %>%
  summary()
```

## To conclude

1. We can compare group means  (2 or more) using a linear model

2. We can use different parameterizations (**cell means** and **reference-treatment effect**) to write statistical models

3. We can write a **linear model** using matrix notation: $Y = X \alpha + \varepsilon$

4. Linear models can include **quantitative & qualitative covariates** 

5. We use different tests to distinguish between **single** and **joint** hypotheses (e.g. $t$-tests vs $F$-tests)
  