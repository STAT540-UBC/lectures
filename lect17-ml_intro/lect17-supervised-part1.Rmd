---
title: "Supervised Learning I: Introduction"
author: |
    | Yongjin Park
    | University of British Columbia
    | (with Keegan Korthauer)
date: "`r format(Sys.time(), '%d %B, %Y')`"
classoption: "aspectratio=169"
output:
    powerpoint_presentation:
        reference_doc: "_template.pptx"
    html_document:
        self_contained: true
    beamer_presentation:
        colortheme: "orchid"
        keep_tex: true
        latex_engine: xelatex
        slide_level: 2
header-includes:
  - \AtBeginSection[]{\begin{frame}\frametitle{Today's lecture}\tableofcontents[currentsection]\end{frame}}
  - |
    \makeatletter
    \def\ps@titlepage{%
      \setbeamertemplate{footline}{}
    }
    \addtobeamertemplate{title page}{\thispagestyle{titlepage}}{}
    \makeatother
    \include{toc}
---

```{r setup, include=FALSE}
library(tidyverse)
library(data.table)
library(patchwork)
source("Util.R")
source("Setup.R")
fig.dir <- "Fig/"
setup.env(fig.dir)
dir.create("Data", showWarnings=FALSE)
theme_set(theme_classic())
```

# General discussions on supervised learning

## What is Supervised Learning?

:::::: {.columns}
::: {.column width=.45}
::: {.block}

### Supervised learning (today and next lecture)

* Input: data $\mathcal{X} = \{\mathbf{x}_{i},\ldots\}$

* Input 2: label $\mathcal{Y} = \{y_{i},\ldots\}$

* Goal: learn $f: \mathcal{X} \to \mathcal{Y}$

:::

X: gene expression samples; Y: disease labels

:::
::: {.column width=.45}

::: {.block}

### Unsupervised learning (previous)

* Input: data $\mathcal{X} = \{\mathbf{x}_{i},\ldots\}$

* Goal 1: learn $f: \mathcal{Z} \to \mathcal{X}$

* Goal 2: hidden/latent states, $\mathcal{Z} = \{\mathbf{z}_{i},\ldots\}$

:::

X: gene expression matrices

:::
::::::

## Machine learning vs. Classical statistics

:::::: {.columns}
::: {.column width=.45}

::: {.block}

### Machine Learning

* *"Statistical learning with the help of algorithm"*

* Large number of variables (less scrutiny in variable selection)

* Higher model complexity, non-linearity

* Focusing on **classification/prediction** (that's it)

* Scalability, computation

:::

:::
::: {.column width=.45}

::: {.block}

### Classical Statistics

* A handful of variables with clear idea

* A (generalized) linear model

* Data generating process (Bayesian)

* More focuses on **scrutiny** (type-I-error, FDR, etc) and **variable selection**

:::

:::
::::::

*Note:* We take a bit more statistical stance.

## Four Steps for Supervised Learning

1. Gather some training data (this is a part of research!):

\begin{eqnarray*}
	\mathcal{X} &\equiv& \{\mathbf{x}_{i}: \, i \in [n]\} \quad \textsf{predictors/covariates/features} \\
	\mathcal{Y} &\equiv& \{\mathbf{y}_{i}: \, i \in [n]\} \quad \textsf{outcome/response}
\end{eqnarray*}

2. Write down a model (classifier) $f: \mathcal{X} \to \mathcal{Y}$

    - What is the loss function?

3. Fit the model to the training data

4. Use the model

    - We can focus on the interpretation of model parameters

    - We can focus on prediction on *new* data points

## Discussion

\vfill

\Huge

Why do we do supervised learning?

\vfill

\normalsize

*Don't we already know the labels?*

# Classification with a decision rule

## Both regression and classification is supervised

:::::: {.columns}
::: {.column width=.45}

::: {.block}

### Multivariate Linear Regression

* $X$: $n \times p$ design matrix

* $\mathbf{y}$: $n \times 1$ outcome vector

* $\beta_{k}$: a regression coefficient for a covariate $k$

* $\boldsymbol{\epsilon}$: a vector of residual errors

* A regression model:

$$Y_{i} = \sum_{k} X_{ik} \beta_{k} + \epsilon_{i}$$

(or $\mathbf{y} = X \boldsymbol{\beta} + \boldsymbol{\epsilon}$)

:::

:::
::: {.column width=.45}

:::
::::::


## Both regression and classification is supervised

:::::: {.columns}
::: {.column width=.45}

::: {.block}

### Multivariate Linear Regression

* $X$: $n \times p$ design matrix

* $\mathbf{y}$: $n \times 1$ outcome vector

* $\beta_{k}$: a regression coefficient for a covariate $k$

* $\boldsymbol{\epsilon}$: a vector of residual errors

* A regression model:

$$\mathbf{y} = X \boldsymbol{\beta} + \boldsymbol{\epsilon}$$

:::

:::
::: {.column width=.45}

::: {.block}

### (A linear) Classifier

* $X$: $n \times p$ design matrix

* $\mathbf{y}$: $n \times 1$ **class label** vector, e.g., $\{0,1\}^{n}$

* $\beta_{k}$: a coefficient for a covariate $k$

* A prediction model:

$$Y_{i} \gets f\left( \sum_{k} X_{ik} \beta_{k} \right)$$

:::

:::
::::::

## A primer question: Can we simply use regression for classification?

```{r}
set.seed(17)
.rnorm <- function(a,b) matrix(rnorm(a*b), nrow=a, ncol=b)
n <- 100
x <- .rnorm(100, 1)
eps <- .rnorm(100, 1)
y <- (sign(scale(x + eps)) + 1)/2
```

```{r fig.width=3, fig.height=2.5, only.plot = "1"}
ggplot(as.data.frame(cbind(x,y)), aes(x,y)) +
    geom_point()
```

```{r fig.width=3, fig.height=3, only.plot = "2"}
ggplot(as.data.frame(cbind(x,y)), aes(x,y)) +
    geom_point() +
    geom_smooth(method="lm", se=FALSE, colour="red") +
    ggpubr::stat_cor()
```

## We don't need to cover all the "areas" of $Y$

```{r echo=TRUE, results="asis", size="Large"}
.glm <- glm(y ~ x, family="binomial")
```

Unlike $X_{i} \in \mathbb{R}$, we have

$$Y_{i} \in \{0, 1\}$$

## Classification = drawing a decision boundary

```{r}
.df <- data.frame(x = as.numeric(x),
                  y.hat = fitted(.glm),
                  y = as.numeric(y))
```

```{r fig.width=3, fig.height=2.5, only.plot="1"}
ggplot(.df, aes(x, y)) +
    geom_point()
```

```{r fig.width=3, fig.height=2.5, only.plot="2"}
ggplot(.df, aes(x, y)) +
    geom_point() +
    geom_line(aes(x, y.hat), colour="red") +
    geom_hline(yintercept = .5, colour="blue", lty = 2) +
    geom_vline(xintercept = 0, colour="blue", lty = 2)
```

## Step-like functions

\large
We will use a step-like function to approximate classification rules

```{r fig.width=6, fig.height=2}
.funs <- c("step", "sigmoid", "hinge", "softplus")

.df <- tibble(x = seq(-3, 3, .05)) %>%
    mutate(step = if_else(x > 0, 1, 0)) %>%
    mutate(sigmoid = 1/(1+ exp(-x))) %>%
    mutate(hinge = pmax(x/2, 0)) %>%
    mutate(softplus = log(1 + exp(x/2-1))) %>%
    gather(key="function", value="y", -x) %>%
    mutate(`function` = factor(`function`, .funs))

.gg.plot(.df, aes(x,y)) +
    geom_hline(yintercept = .5, color="green", lty = 2) +
    geom_vline(xintercept = 0, color="green", lty = 2) +
    geom_line(size=1) +
    facet_grid(.~`function`) +
    scale_y_continuous(limits = c(0, 1.5))
```

$$f(x)=\left\{
\begin{array}{ll}
1 & x > 0\\
0 & x \le 0
\end{array}
\right.,\,
f(x)=\frac{e^{x}}{1 + e^{x}},\,
f(x)=\max\{0, x \xi\},\,
f(x)=\log(1 + \exp(x\alpha - \beta))$$


## Classification = drawing a decision boundary - 2D

```{r}
x <- .rnorm(100, 2)
eps <- .rnorm(100, 1)
coeff <- .rnorm(2,1)
y <- (sign(scale(x %*% coeff + eps)) + 1)/2
```

```{r fig.width=3.2, fig.height=2.5}
.df <- data.frame(x = x, y)

.gg.plot(.df, aes(x.1, x.2, colour=as.factor(`y`))) +
    geom_point() +
    scale_colour_manual("y",values=c("red","blue"))
```

## Classification = drawing a decision boundary - 2D

```{r echo=TRUE, results="asis", size="Large"}
.glm <- glm(y ~ x, family = "binomial")
y.hat <-  fitted(.glm)
```

## Classification = drawing a decision boundary - 2D

```{r fig.width=3.2, fig.height=2.5, only.plot="1"}
.df <- data.frame(x = x, y, y.hat)

p1 <-
    .gg.plot(.df, aes(x.1, x.2, fill=as.numeric(`y.hat`))) +
    geom_point(pch=21) +
    scale_fill_distiller("predicted", palette = "YlGnBu")
print(p1)
```

```{r fig.width=6, fig.height=2.5, only.plot="2"}
p2 <-
    .gg.plot(.df, aes(x.1, x.2, fill=as.factor(`y`))) +
    geom_point(pch=21) +
    scale_fill_manual("observed", values=c("red","blue"))
p1 | p2
```


## Understanding loss functions (regression vs. classification)

:::::: {.columns}
::: {.column width=.45}

::: {.block}
### Regression loss

$$L_{n} = n^{-1}\sum_{i=1}^{n}(\underset{\textsf{\color{red} observed}}{y_{i}} - \underset{\textsf{\color{blue} predicted}}{\mathbf{x}_{i} \boldsymbol{\beta}})^{2}$$

Average distance between the observed and predicted.

:::

:::
::: {.column width=.45}

::: {.block}
### Classification loss

Given a classifier spits out class label, e.g., $\{0,1\}$,

$$L_{n} = n^{-1}\sum_{i=1}^{n}I\left\{\underset{\textsf{\color{red} observed}}{y_{i}} \neq \underset{\textsf{\color{blue} predicted}}{f\left(\mathbf{x}_{i} \boldsymbol{\beta}\right)} \right\}$$

Average frequency of a wrong answer by the classifier.

:::

:::
::::::

* **Algorithm:** Minimize the loss function with respect to free parameters $\boldsymbol{\beta}$.

## Logistic regression: a probabilistic version of classification loss

\large

If a classifier $f(\mathbf{x}_{i} \boldsymbol{\beta}) \in (0,1)$ provides a probability of $Y_{i}=1$, $\hat{p}(Y_{i}|\mathbf{x}_{i},\beta)$:

\onslide<1->{
Bernoulli likelihood:
$$\prod_{i=1}^{n} f^{Y_{i}} (1-f)^{1-Y_{i}}$$
}

\only<2>{
where for each data point $i$
$$\textsf{likelihood}_{i} = \left\{
\begin{array}{ll}
f & \textsf{if } Y_{i}=1 \\
(1-f) & \textsf{if } Y_{i}=0 \\
\end{array}\right.$$

We will use the sigmoid function
$$f(z) = \frac{e^{z}}{1 + e^{z}}$$
}

\only<3>{
We can use negative log-likelihood as a "softer" version classification loss
$$L = -\log \sum_{i=1}^{n} \left[ f(\mathbf{x}_{i} \boldsymbol{\beta})^{Y_{i}} \left[1 - f(\mathbf{x}_{i} \boldsymbol{\beta})\right]^{1-Y_{i}} \right]$$
}

\only<4>{
We can use negative log-likelihood as a "softer" version classification loss
$$L = \sum_{i=1}^{n} Y_{i} \log \overset{\textsf{\color{red} what is this?}}{\frac{1-f(\mathbf{x}_{i}\beta)}{f(\mathbf{x}_{i}\beta)}} \underset{\color{red} \textsf{What if } f \to 1 \textsf{?}}{- \log(1- f(\mathbf{x}_{i}\beta))}$$
}

## Understanding loss functions (soft vs. hard classification)

:::::: {.columns}
::: {.column width=.45}

::: {.block}
### Soft classification loss

Given a classifier $f$ outputs out class label probability $\in (0,1)$

\begin{eqnarray*}
L_{n} &=& \sum_{i=1}^{n} Y_{i} \log \overbrace{\frac{1-f(\mathbf{x}_{i}\beta)}{f(\mathbf{x}_{i}\beta)}}^{\textsf{\color{red} mistake odds ratio}} \\
 && \underbrace{- \log( 1- f(\mathbf{x}_{i}\beta))}_{\color{red}\textsf{will try to predict } f=0}
\end{eqnarray*}

Total "mistake" log-odds ratio

:::

:::
::: {.column width=.45}

::: {.block}
### Classification loss

Given a classifier spits out class label, e.g., $\{0,1\}$,

$$L_{n} = \frac{1}{n}\sum_{i=1}^{n}I\left\{\underset{\textsf{\color{red} observed}}{y_{i}} \neq \underset{\textsf{\color{blue} predicted}}{f\left(\mathbf{x}_{i} \boldsymbol{\beta}\right)} \right\}$$

Average frequency of a wrong answer by the classifier.

:::

:::
::::::

## Logistic loss function

\large

Minimize this with respect to $\beta$:

$$L_{n} =
\sum_{i=1}^{n} \left[
- Y_{i} \overbrace{\sum_{k=1}^{p} X_{ik} \beta_{k}}^{\textsf{\color{red} log-odds}}
\underbrace{+ \log\left(1 + e^{\sum_{k=1}^{p} X_{ik}\beta_{k}}\right)}_{\textsf{\color{blue} "price" for the positive}}
\right]$$

$\iff$

maximize the likelihood

$$\mathcal{L}_{n} =
\sum_{i=1}^{n} \left[
Y_{i} {\color{red} \sum_{k=1}^{p} X_{ik} \beta_{k}}
- \log\left(1 + e^{\color{red} \sum_{k=1}^{p} X_{ik}\beta_{k}}\right)
\right]$$


## A classification rule

**Classifier**

\huge

$$\hat{Y}_{i}
= \left\{\begin{array}{l l}
1 & \sum_{k=1}^{p} X_{ik} \beta_{k} > 0 \\
0 & \sum_{k=1}^{p} X_{ik} \beta_{k} \le 0 \\
\end{array}\right.$$



## Binary classification in action


```{r}
library(torch)

yy <- torch_tensor(y)
xx <- torch_tensor(x)

llik.fun <- function(.beta){
    .eta <- torch_mm(xx, .beta)
    .out <- yy * .eta - nnf_softplus(.eta)
    torch_sum(.out, dim=2)
}

torch_manual_seed(37)
beta <- torch_randn(ncol(xx), ncol(yy), requires_grad = TRUE)
opt <- optim_adam(list(beta), lr=.1)

beta.mat <- matrix(NA, length(beta), 100)

for(iter in 1:100){
    opt$zero_grad()
    llik <- llik.fun(beta)
    loss <- torch_mean(-llik)
    loss$backward()
    . <- opt$step()
    beta.mat[,iter] <- as.numeric(beta)
}
```

```{r}
.df <- data.frame(x = x, y = y)

show.decision <- function(.beta){
    .df.1 <- .df %>%
        mutate(eta = x %*% as.matrix(.beta, ncol=1))
    .aes <- aes(x.1, x.2,
                fill=pmax(pmin(`eta`, 2), -2),
                shape = as.factor(y))

    .sigmoid <- function(z) round(1/(1 + exp(-z)), 2)

    .accuracy <- .df.1 %>%
        summarize(p1 = sum(y == 1 & eta > 0) / sum(y==1) * 100,
                  p0 = sum(y == 0 & eta < 0) / sum(y==0) * 100)

    ggplot(data=.df.1) +
        geom_point(.aes, size=3) +
        geom_text(aes(label="correct\nfor y=1\n" %&% p1 %&% "%"), x=2, y=2, data = .accuracy) +
        geom_text(aes(label="correct\nfor y=0\n" %&% p0 %&% "%"), x=-2, y=-2, data = .accuracy) +
        geom_abline(slope = -.beta[1]/.beta[2], color="red", size=1) +
        geom_segment(x=0,y=0,xend=.beta[1],yend=.beta[2],
                     color="red",
                     arrow = arrow(length=unit(.1,"inches"),
                                   type="closed")) +
        scale_shape_manual("y", values=c(21, 22)) +
        scale_fill_distiller("f", palette = "YlGnBu",
                             labels = .sigmoid)
}
```

```{r fig.width=4, fig.height=3, only.plot="1"}
show.decision(beta.mat[,1]) + ggtitle("iter = 1")
```

```{r fig.width=4, fig.height=3, only.plot="2"}
show.decision(beta.mat[,5]) + ggtitle("iter = 5")
```

```{r fig.width=4, fig.height=3, only.plot="3"}
show.decision(beta.mat[,6]) + ggtitle("iter = 6")
```

```{r fig.width=4, fig.height=3, only.plot="4"}
show.decision(beta.mat[,7]) + ggtitle("iter = 7")
```

```{r fig.width=4, fig.height=3, only.plot="5"}
show.decision(beta.mat[,8]) + ggtitle("iter = 8")
```

```{r fig.width=4, fig.height=3, only.plot="6"}
show.decision(beta.mat[,9]) + ggtitle("iter = 9")
```

```{r fig.width=4, fig.height=3, only.plot="7"}
show.decision(beta.mat[,10]) + ggtitle("iter = 10")
```

```{r fig.width=4, fig.height=3, only.plot="8"}
show.decision(beta.mat[,20]) + ggtitle("iter = 20")
```


```{r fig.width=4, fig.height=3, only.plot="9"}
show.decision(beta.mat[,30]) + ggtitle("iter = 30")
```

```{r fig.width=4, fig.height=3, only.plot="10"}
show.decision(beta.mat[,50]) + ggtitle("iter = 50")
```

```{r fig.width=4, fig.height=3, only.plot="11"}
show.decision(beta.mat[,100]) + ggtitle("iter = 100")
```

# Generalized linear model

## Revisit logistic regression as a "linear" model

(blurring the distinction between regression and classification)

\onslide<1->{\large

A {\bf linear} combination of features:

$$\sum_{k=1}^{p} X_{ik} \beta_{k}$$

}

\onslide<2->{\large

Letting $\eta_{i} = \sum_{k=1}^{p} X_{ik} \beta_{k}$, we define log-likelihood as:

$$\mathcal{L} = \sum_{i=1}^{n} \log p(Y_{i} | \eta_{i}) = \sum_{i=1}^{n} \sigma(\eta_{i}) + (1- Y_{i}) \sigma(-\eta_{i})$$
where $\sigma(z) = 1/(1+\exp(-z))$.
}

## Many interesting data types can be modelled as a GLM

:::::: {.columns}
::: {.column width=.45}

\onslide<1->{

\large

A linear combination of features

$$\eta_{i} = \sum_{k=1}^{p} X_{ik} \beta_{k}$$

can be morphed to many different types of $Y$:

$$Y_{i} \sim p(Y_{i} | \eta_{i})$$

}

:::
::: {.column width=.45}

\onslide<2->{

\begin{block}{Poisson}

$$p(Y_{i} | \lambda_{i}) = \frac{\lambda_{i}^{Y_{i}} e^{-\lambda_{i}}}{Y_{i}!}$$ where $\lambda_{i} = \exp(\eta_{i})$

\end{block}

}

\onslide<3>{

\begin{block}{Gamma}

$$p(Y_{i} | \mu_{i}, \phi) =
\frac{\left( \phi\mu_{i} \right)^{-1/\phi}}{\Gamma(1/\phi)}
Y_{i}^{1/\phi - 1}
e^{- Y_{i} / \mu_{i}\phi }$$

where
$\mu_{i} = \exp(\eta_{i})$
and
$\phi$ is an overdispersion parameter

\end{block}

}

:::
::::::

## Negative Binomial GLM: directly modelling RNA-seq count

\large

Poisson and Gamma distributions are the building blocks of NB.

\begin{eqnarray*}
p(Y_{i}|\mu_{i},\phi)
&=& \int \overbrace{p(Y_{i}|\lambda_{i})}^{\textsf{\color{red} Poisson}} \underbrace{p(\lambda_{i}|\mu_{i},\phi)}_{\textsf{\color{blue} Gamma}} d\lambda_{i} \\
\onslide<2->{
&=&
\underbrace{
\frac{\Gamma(Y_{i} + 1/\phi)}{\Gamma(Y_{i}+1)\Gamma(1/\phi)}
}_{\textsf{\color{magenta} negative binomial}}
\underbrace{\left(
\frac{\mu_{i}}{1/\phi + \mu_{i}}
\right)^{Y_{i}}
}_{\textsf{\color{magenta} success rate}}
\underbrace{
\left(
\frac{1/\phi}{1/\phi + \mu_{i}}
\right)^{1/\phi}
}_{\textsf{\color{magenta} failure rate}}
}
\end{eqnarray*}

where we model
$$\mu_{i} = \exp\left(\sum_{k=1}^{p} X_{ik} \beta_{k}\right).$$

## Negative Binomial GLM: directly modelling RNA-seq count

\large

* $Y$: number of successfully "observed" reads in RNA-seq (~targeting)
* $r$: number of permitted "dropped" reads until $Y$ observed (~budget)
* $\rho$: success rate

\begin{eqnarray*}
\onslide<1->{
p(Y_{i}|\mu_{i},\phi)
&=&
\underbrace{{Y_{i} + r - 1 \choose Y_{i}}}_{\textsf{\color{magenta} negative binomial}}
\underbrace{\rho_{i}^{Y_{i}}}_{\textsf{\color{magenta} success rate}}
\underbrace{(1-\rho_{i})^{r}}_{\textsf{\color{magenta} drop rate}} \\
}
\onslide<2->{
&=& \textsf{NB}(Y_{i}|r = \phi^{-1}, \rho_{i} = \mu_{i}/(\phi^{-1} + \mu_{i})) \\
}
\onslide<3->{
\textsf{or}
&=& \textsf{NB}(Y_{i}| \textsf{mean} = \mu_{i}, \textsf{overdispersion} = \phi)
}
\end{eqnarray*}

\onslide<4>{
We can check:

mean: $\mathbb{E}\!\left[Y_{i}|r,\rho\right] = \rho r / (1 - \rho) = \mu_{i}$

variance: $\mathbb{V}\!\left[Y_{i}|r,\rho\right] = \rho r / (1 - \rho)^{2} = \mu_{i} + \mu_{i}^{2} \phi$ (overdispersed mean-variance)
}

## Four Steps for Supervised Learning

\Large

1. Gather some training data (this is a part of research!):

2. Write down a model (classifier) $f: \mathcal{X} \to \mathcal{Y}$

3. Fit the model to the training data

4. Use the model

## Step 1. Cell type deconvolution data

:::::: {.columns}
::: {.column width=.45}

::: {.block}

### Data set

- Human pancreatic islet gene expression data: [GSE50244](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE50244)

- Single cell RNA-seq data in the same tissue: [E-MTAB-5061](https://www.ebi.ac.uk/arrayexpress/experiments/E-MTAB-5061/)

- A processed data set is available here: https://github.com/xuranw/MuSiC/tree/master/vignettes/data

:::

```{r download_data_music}
readRDSFromWeb <- function(ref) {
    .file <- "Data/" %&% basename(ref)
    if.needed(.file,
    {
        .rds <- readRDS(gzcon(url(ref)))
        saveRDS(.rds, .file)
    })
    readRDS(.file)
}

nb.demo.file <- "Data/nb_demo_file.RDS"
if.needed(nb.demo.file,
{
    ## Download GSE50244 dataset from Github
    GSE50244.bulk.eset <- readRDSFromWeb("https://xuranw.github.io/MuSiC/data/GSE50244bulkeset.rds")
    ## Download EMTAB single cell dataset from Github
    EMTAB.eset <- readRDSFromWeb("https://xuranw.github.io/MuSiC/data/EMTABesethealthy.rds")

    Y <- Biobase::exprs(GSE50244.bulk.eset)

    y.pheno <- Biobase::pData(GSE50244.bulk.eset)

    xx <- Biobase::exprs(EMTAB.eset)

    .celltypes <- c("alpha","beta","gamma","delta","epsilon",
                    "ductal","acinar","endothelial","PSC")

    .pheno <-
        Biobase::pData(EMTAB.eset) %>%
        as.data.table %>%
        mutate(Var2 = colnames(xx)) %>%
        filter(cellType %in% .celltypes)

    x.melt <-
        reshape2::melt(xx) %>%
        as.data.table %>%
        left_join(.pheno, by = "Var2") %>%
        filter(`Var1` %in% rownames(Y)) %>%
        na.omit %>%
        as.data.table

    X <- x.melt[, .(x = mean(value)), by = .(Var1, cellType)] %>%
        filter(x >= 10) %>%
        dcast(Var1 ~ cellType, value.var = "x", fill = 0)

    .common <- na.omit(intersect(rownames(Y), as.character(X$Var1)))

    Y <- Y[rownames(Y) %in% .common, , drop = FALSE]

    X <- X %>% filter(Var1 %in% .common)
    Y <- Y[match(as.character(X$Var1), rownames(Y)), , drop = FALSE]

    X <- X %>% select(-Var1) %>% as.matrix
    saveRDS(list(Y,X, y.pheno), nb.demo.file)
})
demo.data <- readRDS(nb.demo.file)
Y <- demo.data[[1]]
X <- demo.data[[2]]
pheno <- demo.data[[3]]
```

:::
::: {.column width=.45}

::: {.block}
### Problem definition

* $\mathbf{y}$: bulk gene expression (gene $\times$ sample)

* $X$: cell-type-specific single-cell expression (gene $\times$ cell type)

**Goal**

1. Fit a model regressing the bulk profile $\mathbf{y}_{i}$ of a sample $i$ on the single-cell-type-specific matrix $X$.

2. What are the estimated cell type fractions in the bulk sample?

:::

:::
::::::

## Step 1. (show the data)

```{r}
## take most variable genes
genes <-
    apply(X, 2, function(x) head(order(x), 50)) %>%
    as.numeric %>%
    unique

Y.sub <- Y[genes, , drop = FALSE]
X.sub <- X[genes, , drop = FALSE]
##x.tot <- apply(X.sub, 2, sum)
##X.sub <- sweep(X.sub, 2, x.tot, `/`)
```

```{r fig.width=5, fig.height=3}
p1 <-
    .matshow(scale(log(1 + head(Y.sub, 100))), .size=0, .lab=0, .scale=TRUE) +
    ggtitle("scale(log(1 + Y))")

p2 <-
    .matshow(scale(log(1 + head(X.sub, 100))), .size=0, .lab=0, .scale=TRUE) +
    ggtitle("scale(log(1 + X))")

p0 <- ggplot() + theme_void()

wrap_plots(p1, p0, p2, widths=c(4, .5, 1.5))
```


## Step 2. NB GLM for the cell type deconvolution problem

**Q:** Can we estimate cell type fractions in tissue-level *bulk* data?


$$\overset{\textsf{\color{magenta} bulk}}{\mathbf{y}_{i}}
\sim
\textsf{NB}\left(\textsf{mean} = \overset{\textsf{\color{gray} scale factor}}{s_{i}} \underset{\textsf{\color{blue} cell-type-sorted}}{\sum_{t} X_{gt}}
\overset{\textsf{\color{red} GOAL}}{\theta_{ti}},
\textsf{overdispersion} = \phi \right)$$

* We use the same data set used in the vignette of [MuSic](https://github.com/xuranw/MuSiC/blob/master/vignettes/vignette.Rmd) package (Wang _et al._, Nature Comm., 2019)


## Step 3. Fit the model to the data

```{r}
library(torch)
yy <- torch_tensor(Y.sub)
xx <- torch_tensor(X.sub)

logit.theta <- torch_zeros(ncol(xx), ncol(yy), requires_grad = TRUE)
log.scale <- torch_zeros(1, ncol(yy), requires_grad = TRUE)
log.phi <- torch_zeros(nrow(yy), 1, requires_grad = TRUE)

nb.llik <- function(logit.theta, log.scale, log.phi){

    theta <- torch_exp(nnf_log_softmax(logit.theta, dim=1))

    log.mu <- torch_log(torch_mm(xx, theta) * torch_exp(log.scale))

    od <- torch_exp(-log.phi) + 1e-8
    .term1 <- torch_lgamma(yy + od) - torch_lgamma(yy + 1) - torch_lgamma(od)

    ## log(mu) - log(mu + 1/phi)
    ## log(mu / (mu + 1/phi))
    ## log(1 / (1 + 1/mu phi))
    ## - log(1 + 1/mu phi)
    ## - log(1 + exp(- log.mu - log.phi))
    .term2 <- - nnf_softplus(-log.mu - log.phi) * yy

    ## log(1/phi) - log(1/phi + mu)
    ## log(1 / (1 + mu * phi))
    ## - log(1 + mu * phi)
    ## - log(1 + exp(log.mu + log.phi))
    .term3 <- - nnf_softplus(log.mu + log.phi) * od

    llik <- .term1 + .term2 + .term3
    torch_sum(llik, dim=1)
}
```

```{r}
nb.deconv.file <- fig.dir %&% "/NB_deconvolution.RDS"
if.needed(nb.deconv.file,
{
    opt <- optim_adam(list(logit.theta, log.scale, log.phi), lr = 1e-2)

    loss.vec <- c()

    for(iter in 1:10000){
        opt$zero_grad()
        llik <- nb.llik(logit.theta, log.scale, log.phi)
        loss <- torch_mean(-llik)
        loss$backward()
        loss.vec <- c(loss.vec, loss$sum()$item())
        . <- opt$step()
        cat(loss$mean()$item(), "\r", file=stderr())
        flush(stderr())
    }

    theta <-
        torch_exp(nnf_log_softmax(logit.theta, dim=1)) %>%
        as.matrix()
    rownames(theta) <- colnames(X.sub)
    colnames(theta) <- colnames(Y.sub)
    saveRDS(list(theta, loss.vec), nb.deconv.file)
})
.out <- readRDS(nb.deconv.file)
theta <- .out[[1]]
loss.vec <- .out[[2]]
```

```{r fig.width=4, fig.height=3.5}
plot(loss.vec, pch=19, cex=.5, type = "b", ylab = "loss", log="x", xlab = "iteration")
```


## Step 4. Show the cell type fraction estimates

```{r fig.width=6, fig.height=2.5}
ind.order <-
    order(theta["acinar", ]) %>%
    (function(x) colnames(theta)[x])

.df <-
    reshape2::melt(theta) %>%
    as_tibble() %>%
    rename(row = Var2, col = Var1, weight = value) %>%
    col.order(.ro = ind.order, ret.tab = TRUE)

.gg.plot(.df, aes(`row`, `weight`, fill = `col`)) +
    ggtitle("theta") +
    geom_bar(stat="identity") +
    scale_fill_brewer("cell type",palette = "Paired") +
    ylab("cell type fraction") +
    xlab("bulk samples") +
    theme(axis.text.x = element_blank())
```

# Generative Modelling approach

## Discriminative vs. generative approach

* Y: class label/outcome, X: covariates/predictor variables

:::::: {.columns}
::: {.column width=.45}

::: {.block}
### Discriminative learning

- Directly learn $p(Y|X)$

:::

:::
::: {.column width=.45}

::: {.block}
### Generative learning

- First learn $p(X|Y)$

- Apply **Bayes** rule: $p(Y|X) \propto p(X|Y) p(Y)$

:::

:::
::::::

* If we have domain-specific knowledge, $p(X|Y)$, a generative-modelling approach can be more powerful.

* If our focus is solely on classification, a discriminative learning approach usually outperforms with small to moderate sample size.

## Naive Bayes Classifier: How do we estimate $P(X|Y)$?

\Large

If $\mathbf{x}_{i} = (X_{i1}, \ldots, X_{ip})$, how do we estimate $p(\mathbf{x}_{i}|Y_{i}=1)$ or $p(\mathbf{x}_{i}|Y_{i} = 0)$?

$$
p(X_{i1}, \ldots, X_{ip}|Y_{i}=y_{i}) \underset{\textsf{\color{red} fully-factored}}{\approx} \prod_{k=1} p(X_{ik}|Y_{i})
$$

\large

* Is this accurate?

* What have we missed?

##

Let's take an example from the pancreatic islet data (`GSE50244`).

```{r fig.width=5.8, fig.height=3, only.plot="1"}
pheno <- demo.data[[3]]

.df <- pheno %>%
    as_tibble() %>%
    gather(key = "var", value="val", age, bmi, hba1c) %>%
    mutate(y = factor(`gender`, c("Female","Male"), c("y=Female","y=Male"))) %>%
    mutate(var = "x: " %&% `var`) %>%
    arrange(y)

ggplot(.df, aes(x=val, fill=`y`)) +
    facet_wrap(~ `y` + var, scales = "free", nrow = 2) +
    geom_density(aes(y = ..count..)) +
    geom_histogram(bins=50, fill="gray30", alpha = .7) +
    scale_fill_manual(values=c("#fb9a99","#a6cee3"), guide="none") +
    theme(strip.background = element_rect(size=0, fill="gray90"),
          strip.text = element_text(size=8)) +
    ylab("~ p(X|Y)") + xlab("X")
```


```{r}
xx <- Y
colnames(xx) <- pheno$gender
.xx.dt <- reshape2::melt(xx) %>% as.data.table

## This is cheating... just for illustration
## In practice, we need to select features only within training set
.fun <- function(.temp){
    .x <- .temp[`Var2` == "Female"]$value
    .y <- .temp[`Var2` == "Male"]$value
    .test <- wilcox.test(.x, .y)
    list(statistic = .test$statistic, p.value = .test$p.value)
}

top.genes <-
    .xx.dt[, .fun(.SD), by = .(`Var1`)] %>%
    arrange(p.value) %>%
    head(3) %>%
    select(`Var1`)
```

```{r fig.width=5.8, fig.height=3, only.plot="2"}
.df <-
    left_join(top.genes, .xx.dt) %>%
    mutate(y = factor(`Var2`, c("Female","Male"), c("y=Female","y=Male"))) %>%
    mutate(var = "x: " %&% `Var1`) %>%
    arrange(y)

ggplot(.df, aes(x=`value`, fill=`y`)) +
    facet_wrap(~ `y` + var, scales = "free_y") +
    geom_density(aes(y = ..count..)) +
    scale_x_continuous(trans = "log", labels = function(x) round(x)) +
    scale_fill_manual(values=c("#fb9a99","#a6cee3"), guide="none") +
    theme(strip.background = element_rect(size=0, fill="gray90"),
          strip.text = element_text(size=8)) +
    ylab("~ p(X|Y)") + xlab("X")
```

## Not every gene can predict well


```{r}
.svd <- svd(log(1 + Y), nu=5, nv=5)
.df <- data.frame(V = .svd$v,
                  gender = pheno$gender)
```

```{r fig.width=3, fig.height=2.5}
ggplot(.df, aes(V.1, V.2, fill = `gender`)) +
    geom_point(pch=21) +
    theme(legend.position = "bottom") +
    ylab("PC2") + xlab("PC1")
```

## These genes are very informative

:::::: {.columns}
::: {.column width=.45}

```{r}
.df <-
    data.frame(t(Y[unlist(top.genes), ])) %>%
    mutate(gender = pheno$gender)
```

```{r fig.width=2.5, fig.height=2.5, onslide.plot="1-"}
ggplot(.df, aes(`PRKY`, `XIST`, fill = `gender`)) +
    theme(legend.position = "bottom") +
    geom_point(pch=21)
```

:::
::: {.column width=.45}


```{r fig.width=2.5, fig.height=2.5, onslide.plot="2"}
ggplot(.df, aes(`XIST`,`TTTY15`, fill = `gender`)) +
    theme(legend.position = "bottom") +
    geom_point(pch=21)
```

:::
::::::

## Naive Bayes Classifier

\Large

\begin{eqnarray*}
p(Y_{i} = 1|\mathbf{x}_{i}) 
&\approx&
	\frac{ \overbrace{p(Y_{i} = 1)}^{\textsf{\color{red} prior}} \overbrace{\prod_{k} \hat{p}(X_{ik} | \theta_{k1})}^{\textsf{\color{red} fully-factored}} }
	{ \sum_{c=0}^{1} p(Y_{i} = c) \prod_{k} \hat{p}(X_{ik} | \theta_{kc}) } \\
\onslide<2>{
&\approx&
	\frac{ p(Y_{i} = 1) \prod_{k} \overset{\textsf{\color{blue} Gaussian approximation}}{\mathcal{N}\!\left(X_{ik}|\mu_{k1}, \sigma_{k1}^{2} \right)} }
	{ \sum_{c=0}^{1} p(Y_{i} = c) \prod_{k} \underset{\textsf{\color{blue} Gaussian approximation}}{\mathcal{N}\!\left(X_{ik} | \mu_{kc}, \sigma_{kc}^{2} \right)}} \\
}
\end{eqnarray*}

\normalsize

* Prior can be $1/2$ if the class labels are balanced (or $1/K$ for $K$ classes).

## Can we improve Naive Bayes Classifier?

```{r out.width=".5\\linewidth", results="asis"}
knitr::include_graphics("Vis/supervised_NB.pdf")
```

$$p(Y_{i} = 1|\mathbf{x}_{i})  \approx
	\frac{ \overbrace{p(Y_{i} = 1)}^{\textsf{\color{red} prior}} \overbrace{\prod_{k} \hat{p}(X_{ik} | \theta_{k1})}^{\textsf{\color{red} fully-factored}} }
	{ \sum_{c=0}^{1} p(Y_{i} = c) \prod_{k} \hat{p}(X_{ik} | \theta_{kc}) }$$

## Tree-augmented Naive Bayes Classifier can better approximate $P(X|Y)$

```{r out.width=".5\\linewidth", results="asis"}
knitr::include_graphics("Vis/supervised_TANB.pdf")
```

$$p(Y_{i} = 1|\mathbf{x}_{i})  \approx
	\frac{ \overbrace{p(Y_{i} = 1)}^{\textsf{\color{red} prior}} \overbrace{\prod_{k} \hat{p}(X_{ik} | X_{i \textsf{Pa}(k)}, \theta_{k1})}^{\textsf{\color{red} tree-structured}} }
	{ \sum_{c=0}^{1} p(Y_{i} = c) \prod_{k} \hat{p}(X_{ik} | X_{i \textsf{Pa}(k)}, \theta_{kc}) }$$

* Can we use prior knowledge, such as Gene Ontology?

## Discussions

\Large

* Can we include all the genes in a NB classifier?

* Can we first try out DEG analysis to select informative features?

* How do we deal with high-dimensional classification problems?

# Other Methods

## 

\Large

1. K-nearest neighbour classifier (will discuss in the single-cell lecture)

2. Running average estimator

    - Local piece-wise regression (`loess`)

    - Useful for low-dimensional settings

3. Ensemble models (previous + next lectures)

    - A mixture of (logistic) regression

    - Boosting/bagging (combining the power of many weak classifiers)

4. Deep learning (will discuss in the single-cell lecture)
