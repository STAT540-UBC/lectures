---
title: "Supervised Learning II: high-dimensional model selection"
author: |
    | Yongjin Park
    | University of British Columbia
date: "`r format(Sys.time(), '%d %B, %Y')`"
classoption: "aspectratio=169"
output:
    powerpoint_presentation:
        reference_doc: "_template.pptx"
    html_document:
        self_contained: true
    beamer_presentation:
        colortheme: "orchid"
        keep_tex: true
        latex_engine: xelatex
        slide_level: 2
header-includes:
  - \usepackage{cancel}
  - \AtBeginSection[]{\begin{frame}\frametitle{Today's lecture}\tableofcontents[currentsection]\end{frame}}
  - |
    \makeatletter
    \def\ps@titlepage{%
      \setbeamertemplate{footline}{}
    }
    \addtobeamertemplate{title page}{\thispagestyle{titlepage}}{}
    \makeatother
    \include{toc}
---

```{r setup, include=FALSE}
library(tidyverse)
library(data.table)
library(patchwork)
library(bigsnpr)
source("Util.R")
source("Setup.R")
fig.dir <- "Fig/supervised/"
setup.env(fig.dir)
dir.create("Data", showWarnings=FALSE)
theme_set(theme_classic())
```

## Learning Objectives

\large

* Model selection, Bias-Variance tradeoff, a Bayesian View

* How do we handle $p \gg n$ situation in practice?

* Multiple Frequenist & Bayesian approaches


## A working example: predicting gene expressions from genetic information

::: {.block}

\Large

Q. Can we predict gene expressions based on genetic information?

\centering

DNA $\overset{\textsf{\color{red} here?}}{\to}$ mRNA $\to$ protein

:::


## If we could predict gene expression...

::: {.block}

\Large

$$\textsf{DNA} \overset{\textsf{\color{red} here?}}{\to} \textsf{mRNA} \to \textsf{protein}$$

:::

::: {.block}

\normalsize

We can guess potential mechanisms of genetic disorders:

\Large

$$\textsf{DNA change} \to \overset{\textsf{\color{red} black-box}}{(\cdots)} \to \textsf{disease}$$

\normalsize

because we can do transcriptome-wide association studies (TWAS):

\Large

$$\Delta\textsf{DNA} \to \textsf{mRNA}(\Delta \textsf{DNA}) \overset{\textsf{\color{red} test this}}{\to} \textsf{disease}$$

:::


\tiny Gamazon *et al.* Nature Genetics (2015)

\tiny Gusev *et al.* Nature Genetics (2016)

## If we could predict gene expression by genetic information...

:::::: {.columns}
::: {.column width=.45}

```{r out.height=".7\\textheight", results="asis", onslide.plot="1-"}
knitr::include_graphics("Vis/TWAS.pdf")
```

\tiny Gusev *et al.* Nature Genetics (2016)

:::
::: {.column width=.45}

```{r out.height=".7\\textheight", results="asis", onslide.plot="2"}
knitr::include_graphics("Vis/TWAS2.pdf")
```

\tiny Gamazon *et al.* Nature Genetics (2015)

:::
::::::

## Today's problem: gene expression prediction

\large

* We will focus on supervised learning (regression) of gene expression

* We will revisit the problem to discuss biological aspects in the GWAS lectures

## Why regression?

```{r out.width=".8\\textwidth", results="asis", onslide.plot="1"}
knitr::include_graphics("Vis/peer.pdf")
```

* Handle multiple types of biological and technical factors

* Including all the variables often improve statistical powers

* What if there are too many variables?


\vfill

\flushleft{\tiny Stegle {\it et al.} PLoS Genetics (2010)}


## Modeling gene expression as a function of genetic variants

$$
\mathbf{y} = \left(\begin{array}{l}
y_{1}\\
y_{2}\\
\vdots\\
y_{n}
\end{array}\right),\quad
X = \left(\begin{array}{l l l}
X_{11} & \cdots & X_{1p} \\
X_{21} & \cdots & X_{2p} \\
 & \cdots & \\
X_{n1} & \cdots & X_{np} \\
\end{array}\right),\quad
\boldsymbol{\theta} = \left(\begin{array}{l}
\theta_{1}\\
\theta_{2}\\
\vdots\\
\theta_{p}
\end{array}\right)
$$

\vfill

**Multivariate linear regression model:**
$$
\mathbf{y} = X \boldsymbol{\theta} + \epsilon,\, \boldsymbol{\epsilon} \sim \mathcal{N}\!\left(\mathbf{0}, \sigma^{2}I\right).
$$

\vfill

**Example**

- $\mathbf{y}$ : a gene expression measured by RNA-seq / microarray.

- ${(X_{ij})}$ : genetic variants at locus $j$ measured on individual
  $i$.  $X$ can be anything of interest, such as other genes and
  phenotypes.

- We can fit the model gene by gene (independence) or all the genes
  jointly (dependency between genes)

## Two major interests in regression analysis

$$
\mathbf{y} = \left(\begin{array}{l}
y_{1}\\
y_{2}\\
\vdots\\
y_{n}
\end{array}\right),\quad
X = \left(\begin{array}{l l l}
X_{11} & \cdots & X_{1p} \\
X_{21} & \cdots & X_{2p} \\
 & \cdots & \\
X_{n1} & \cdots & X_{np} \\
\end{array}\right),\quad
\boldsymbol{\theta} = \left(\begin{array}{l}
\theta_{1}\\
\theta_{2}\\
\vdots\\
\theta_{p}
\end{array}\right)
$$

\vfill

_Multivariate linear regression model:_
$$
\mathbf{y} = X \boldsymbol{\theta} + \epsilon,\, \boldsymbol{\epsilon} \sim \mathcal{N}\!\left(\mathbf{0}, \sigma^{2}I\right).
$$

\vfill

\onslide<2->{
1. Estimation of unknown parameters (\textbf{posterior probability}):
$$
p(\boldsymbol{\theta}|X,\mathbf{y}) \propto p(\mathbf{y}|X,\boldsymbol{\theta}) p(\boldsymbol{\theta})
$$
}

\onslide<3->{
2. Prediction of future phenotype (\textbf{posterior prediction}):
$$
p(\mathbf{y}^{\textrm{new}}|X^{\textrm{new}},X,\mathbf{y}) = \int p(\mathbf{y}^{\textrm{new}}|X^{\textrm{new}},\boldsymbol{\theta}) p(\mathbf{y}|X,\boldsymbol{\theta}) p(\boldsymbol{\theta}) d\boldsymbol{\theta}
$$
}

## Of many important questions, we will try to tackle this one...

\Huge

$$p \gg n$$

\vfill

\normalsize

* $n$: sample size

* $p$: number of parameters

# Bias-variance tradeoff

## A class of models $\hat{f} \in \mathcal{F}$

$$
\left(\begin{array}{l}
y_{1}\\
y_{2}\\
\vdots\\
y_{n}
\end{array}\right)
=
\theta_{1} \left(\begin{array}{l}
X_{11}\\
X_{21}\\
\vdots\\
X_{n1}
\end{array}\right) +
\cdots
\theta_{p} \left(\begin{array}{l}
X_{1p}\\
X_{2p}\\
\vdots\\
X_{np}
\end{array}\right).
$$

- Different number of variables will define a class of potential models

- E.g., $\mathcal{F}_{1}$: a class of models with one variable

- $\mathcal{F}_{2}$: a class of models with two variables

- (...)

- $\mathcal{F}_{q}$: a class of models with $q$ variables

## How do we know if one model is better than the other?

\large

* Training vs. (unseen) testing data

* Our hope: training $\approx$ testing

## What is a good classifier?

```{r out.height=".6\\textheight", results="asis", only.plot="1"}
knitr::include_graphics("Vis/supervised_learning_curves.pdf")
```

```{r out.height=".6\\textheight", results="asis", only.plot="2"}
knitr::include_graphics("Vis/supervised_learning_curves_2.pdf")
```

```{r out.height=".7\\textheight", results="asis", only.plot="3"}
knitr::include_graphics("Vis/supervised_learning_curves_3.pdf")
```


## The ultimate goal: generalization error minimization

\Large

k-fold CV error $\to$ leave-one-out CV error $\to$ generalization error

\vfill

\normalsize

* No matter what may come, we will still predict as good as this...

* We will use k-fold cross validation error to estimate generalization error

## Bias-variance tradeoff in generalization error

\begin{eqnarray*}
\mathbb{E}\!\left[\overset{\textsf{\color{red} true unknown model}}{f(X)} - \overset{\textsf{\color{blue} our attempt}}{\hat{f}(X)}\right]^2
&=&
\mathbb{E}\!\left[f(X) \overbrace{- \mathbb{E}\!\left[\hat{f}\right] + \mathbb{E}\!\left[\hat{f}\right]}^{\textsf{\color{pink} average model within the class}} - \hat{f}(X)\right]^2 \\
\onslide<2->{
\textsf{\color{gray} (expand the square)}
&=&
\mathbb{E}\!\left[ (f(X) - \mathbb{E}\!\left[\hat{f}\right])^2 + (\mathbb{E}\!\left[\hat{f}\right] - \hat{f}(X))^2 \right. \\
& & \left. + 2 (f(X) - \mathbb{E}\!\left[\hat{f}\right]) (\mathbb{E}\!\left[\hat{f}\right] - \hat{f}(X))\right] \\
}
\onslide<3->{
\textsf{\color{gray} (rearrange the terms)}
&=& \mathbb{E}\!\left[ f(X) - \mathbb{E}\!\left[\hat{f}\right] \right]^2 + 
\mathbb{E}\!\left[ \mathbb{E}\!\left[\hat{f}\right] - \hat{f}(X)) \right]^2 \\
& &
+ 2 \mathbb{E}\!\left[ f(X) - \mathbb{E}\!\left[\hat{f}\right] \right]
\cancelto{0}{\mathbb{E}\!\left[\mathbb{E}\!\left[\hat{f}\right] - \hat{f}(X)\right]}\\
}
\onslide<4>{
&=& \underbrace{\mathbb{E}\!\left[ f(X) - \mathbb{E}\!\left[\hat{f}\right] \right]^2}_{\textsf{\color{magenta} bias}^2} + 
\underbrace{\mathbb{E}\!\left[ \mathbb{E}\!\left[\hat{f}\right] - \hat{f}(X)) \right]^2}_{\textsf{\color{magenta} variance}}
}
\end{eqnarray*}

\onslide<5>{
\tiny
Remark: We didn't factor out irreducible errors.
}

## Bias-variance tradeoff in generalization error

$$\mathbb{E}\!\left[\overset{\textsf{\color{red} true unknown model}}{f(X)} - \overset{\textsf{\color{blue} our attempt}}{\hat{f}(X)}\right]^2 =
\underbrace{\mathbb{E}\!\left[ f(X) - \mathbb{E}\!\left[\hat{f}\right] \right]^2}_{\textsf{\color{magenta} bias}^2} + 
\underbrace{\mathbb{E}\!\left[ \mathbb{E}\!\left[\hat{f}\right] - \hat{f}(X)) \right]^2}_{\textsf{\color{magenta} variance}}$$



## k-fold cross validation in regression modelling (M. Stone 1974)

```{r out.height=".8\\textheight", results="asis"}
knitr::include_graphics("Vis/Stone_CV.pdf")
```

# High-dimensional multivariate regression

## In multivariate regression modelling

\Large

Model selection $\approx$ variable selection


## Challenges in our $p\gg n$ regression problem

:::::: {.columns}
::: {.column width=.45}

::: {.block}

### Degeneracy

High degree of freedom, many, many unknown, but very title information

:::

```{r out.width=".9\\linewidth", results="asis", onslide.plot="1-"}
knitr::include_graphics("Vis/intractability1.pdf")
```

:::
::: {.column width=.45}


::: {.block}

### Col-linearity

Variables are somewhat similar to each other

:::


```{r out.width=".7\\linewidth", results="asis", onslide.plot="2"}
knitr::include_graphics("Vis/intractability2.pdf")
```

:::
::::::

```{r}
dir.create("Data/genotype/", recursive=TRUE, showWarnings=FALSE)
.bed.file <- "Data/genotype/1000G_phase3_common_norel.bed"
if(!file.exists(.bed.file)){
    download_1000G("Data/genotype/")
}
.bk.file <- "Data/genotype/1000G_phase3_common_norel.rds"
if(!file.exists(.bk.file)){
    BED <- snp_readBed(.bed.file)
}
data <- snp_attach(.bk.file)
```

```{r}
#' @param X
#' @param h2
#' @param n.causal
#' @param n.traits
simulate.pgs <- function(X, h2, n.causal, n.traits = 1) {
    .rnorm <- function(d1, d2) matrix(rnorm(d1*d2), d1, d2)
    causal.snp <- sample(ncol(X), n.causal)
    xx.causal <- scale(X[, causal.snp, drop=FALSE])
    xx.causal[is.na(xx.causal)] <- 0

    n.ind <- nrow(X)
    y.true <- scale(xx.causal %*% .rnorm(n.causal, n.traits))
    y.err <- scale(.rnorm(n.ind, n.traits))
    y.obs <- y.true * sqrt(h2) + y.err * sqrt(1 - h2)

    list(y = y.obs, causal = causal.snp)
}
```

```{r}
set.seed(1)
ntot <- nrow(data$genotypes)
.train.ind <- sample(ntot, 1000)
.test.ind <- setdiff(1:ntot, .train.ind)
X <- data$genotypes[.train.ind, 1:2000]
sim <- simulate.pgs(X, h2=0.2, n.causal=20)
X <- scale(X)
y <-  sim$y

## sort individuals for visualization
o <- order(y)
y <- y[o, , drop = FALSE]
X <- X[o, , drop = FALSE]
```

## A working example - data

:::::: {.columns}
::: {.column width=.45}

```{r fig.width=2.5, fig.height=1.5}
ggplot(data.table(y, i = 1:nrow(y)), aes(i, y)) +
    geom_point(size=.5) +
    xlab("samples")
```

```{r echo = TRUE, results = "asis"}
dim(y)
```

:::
::: {.column width=.45}


```{r fig.width=3, fig.height=1.5}
p1 <-
    .matshow(X[1:20, 1:50], .size=0, .lab=0) +
    ggtitle(nrow(X) %&% " x " %&% ncol(X))

p0 <- ggplot() + theme_void() + geom_text(aes(x=0,y=0,label="..."), size=5)

wrap_plots(p1, p0, p0, p0, nrow = 2, widths=c(3,1), heights=c(4,1))
```

```{r echo = TRUE, results = "asis"}
dim(X)
```

:::
::::::

There are `r length(sim$causal)` true non-zero variables.

## True causal variables explain a large fraction of variation

```{r echo = TRUE}
.lm <- lm(y ~ X[, sim$causal, drop = FALSE] - 1)
```

```{r fig.width = 4, fig.height = 2.8}
.dt <- data.table(y = as.numeric(y), y.hat = fitted(.lm))
.gg.plot(.dt, aes(y.hat, y)) +
    geom_point(color = "gray40", size = .3) +
    geom_abline(slope = 1, color = "red", lty = 2)
```

## Variant-by-variant correlations

```{r fig.width=5.5, fig.height=2}
.df <- reshape2::melt(X[, sim$causal, drop = FALSE], value.name="x") %>%
    left_join(data.frame(Var1 = 1:nrow(y), y = as.numeric(y)))

.gg.plot(.df, aes(as.factor(x), y)) +
    xlab("x") +
    facet_wrap(~Var2, scales = "free", nrow = 2) +
    geom_boxplot(outlier.size=0, outlier.stroke=0) +
    theme(strip.text = element_blank()) +
    theme(axis.text.x = element_blank())+
    theme(strip.background = element_blank())
```

## How do we know "causal" variables from `r num.int(ncol(X))` variables?

* Let's try out one by one and rank them by univariate

\Large

`cor.test(x,y)`

\normalsize

```{r}
univar.dt <-
    apply(X, 2, function(x) cor.test(x, sim$y)$p.value) %>%
    (function(p) data.table(pval = p)) %>%
    mutate(i = 1:n()) %>%
    mutate(l = as.integer(i %in% sim$causal)) %>%
    arrange(pval) %>%
    mutate(r = 1:n()) %>%
    as.data.table
```

```{r}
plt.pvalue.upto <- function(pp){
    .aes <- aes(as.integer(r), -log10(pval), fill=as.factor(l))
    .dt <- univar.dt[1:pp,] %>% arrange(l)
    .gg.plot(.dt, .aes) +
        xlab("variables ranked by p-value") +
        theme(legend.position = c(1,1)) +
        theme(legend.justification = c(1,1)) +
        geom_point(pch=21) +
        scale_fill_manual("causal", values =c("gray", "red")) +
        scale_y_continuous("p-value", labels=function(x)num.sci(10^(-x)))
}
```


```{r fig.width=2, fig.height=2, only.plot="2"}
plt.pvalue.upto(10)
```

```{r fig.width=3, fig.height=2, only.plot="3"}
plt.pvalue.upto(50)
```

```{r fig.width=4, fig.height=2, only.plot="4"}
plt.pvalue.upto(150)
```

```{r fig.width=5, fig.height=2, only.plot="5"}
plt.pvalue.upto(250)
```

```{r fig.width=5.5, fig.height=2, only.plot="6"}
plt.pvalue.upto(500)
```


##

\large

- Classical variable selection by univariate (one-by-one) tests will not work for a $p \gg n$ regression problem


- Especially if we have col-linearity in the design matrix $X$


## Can we get helped by multivariate regression?

```{r echo = TRUE, size="large"}
lm.out <- lm(y ~ X - 1)
```

If you look at the coefficients:

```{r results="asis"}
coefficients(summary(lm.out)) %>%
    head %>%
    knitr::kable()
```

Anything strange? Hint: $\hat{\theta} = (X^{\top}X)^{-1} X^{\top}\mathbf{y}$.

## OLS overfits to the data

```{r fig.width = 5, fig.height = 3}
.ols.dt <-
    coefficients(summary(lm.out)) %>%
    as.data.frame() %>%
    as.data.table() %>%
    mutate(i = 1:n()) %>%
    mutate(l = factor(as.integer(i %in% sim$causal), c(0, 1))) %>%
    arrange(desc(abs(`Estimate`))) %>%
    mutate(r = 1:n()) %>%
    arrange(l)

.aes <- aes(r, `Estimate`, fill = l)

.gg.plot(.ols.dt, .aes) +
    geom_point(pch=21, stroke=0) +
    theme(legend.position = c(1,1)) +
    theme(legend.justification = c(1,1)) +
    xlab("variables (ranked by OLS)") + ylab("coefficients") +
    geom_hline(yintercept = 0, color="black") +
    scale_fill_manual("causal", values=c("gray", "red"))
```

## Can we get helped by multivariate regression?

```{r out.width=".7\\linewidth", results="asis"}
knitr::include_graphics("Vis/intractability1.pdf")
```

OLS (a.k.a. MLE/MSE) is degenerate if $p \gg n$

## Variable selection in high-dimensional genotype matrix ($n \ll p$)

Regression analysis = projecting the observed $\mathbf{y}$ vector on to
column space of $\{\mathbf{x}_{j}: j \in[p]\}$,
$$
\left(\begin{array}{l}
y_{1}\\
y_{2}\\
\vdots\\
y_{n}
\end{array}\right)
=
\theta_{1} \left(\begin{array}{l}
X_{11}\\
X_{21}\\
\vdots\\
X_{n1}
\end{array}\right) +
\cdots
\theta_{p} \left(\begin{array}{l}
X_{1p}\\
X_{2p}\\
\vdots\\
X_{np}
\end{array}\right).
$$
Variable selection = column selection.

\vfill

\begin{itemize}
\item <1-> Intuitive idea : choose the best combination of variables. $\to 2^{p}$ choices (even harder).
\item <2-> Alternative idea : make as many $\theta_{j}$'s nearly zero values.
\item <3-> What prior does: penalize $|\theta_{j}| > 0$ so that only the strong
  enough variables take non-zero values.
\end{itemize}

## Reconciling two related concepts -- MLE and MSE

Equivalence of maximum-likelihood estimation and mean square error
minimization (isotropic Gaussian error distribution).

\vfill

__[MLE]__ Find ${\boldsymbol{\theta}}$ maximizing
$$
\ln p(\mathbf{y}|X,\boldsymbol{\theta})
= - \frac{1}{2\sigma^{2}} \sum_{i=1}^{n} (y_{i} - \mathbf{x}_{i} \boldsymbol{\theta})^{2} + \textrm{const.}
$$
_without prior contribution of parameter, and $\sigma$ is known._

\vfill

__[MSE]__ Find $\boldsymbol{\theta}$ minimizing
$$
\sum_{i=1}^{n} (y_{i} - \mathbf{x}_{i} \boldsymbol{\theta})^{2}.
$$

\vfill

## MLE, MSE, an optimization problem

Minimization of the convex loss function:
$$
L(\boldsymbol{\theta}) = (\mathbf{y} - X\boldsymbol{\theta})^{\top}(\mathbf{y} - X\boldsymbol{\theta})
$$

\onslide<2->{We can optimize setting the derivative with respect to $\boldsymbol{\theta}$ to zero:
$$
\nabla_{\boldsymbol{\theta}} L = X^{\top} (\mathbf{y} - X\boldsymbol{\theta}) = 0
$$
Rearranging the equation
$$
\mathbf{y}^{\top} X = X^{\top} X \boldsymbol{\theta} \implies
\hat{\boldsymbol{\theta}}_{MLE} = (X^{\top} X)^{-1} X^{\top} \boldsymbol{y}.
$$
}

\vfill

* \onslide<3->{Approximately, $p(\boldsymbol{\theta} | \mathbf{y}, X) \approx \mathcal{N}\!\left(\boldsymbol{\theta} \middle| \hat{\boldsymbol{\theta}}_{MLE}, \sigma^{2} (X^{\top} X)^{-1} \right)$.}

* \onslide<4->{How hard is $(X^{\top} X)^{-1}$ (i.e., inverse of ${p\times p}$ matrix)?}

* \onslide<5->{What if $n \ll p$?  What if we want to include $p(\boldsymbol{\theta})$?}

## Bayesian/regularization idea to add the missing probability component

\large

We've been discussing the conditional likelihood

$$p(\mathbf{y}|X,\boldsymbol{\theta})$$

without a prior probability of regression coefficients,

$${\color{blue} p(\boldsymbol{\theta})}$$

**What will be a suitable prior distribution of $\boldsymbol{\theta}$?**

## Recall: Reconciling two related concepts -- MLE and MSE

Equivalence of maximum-likelihood estimation and mean square error
minimization (isotropic Gaussian error distribution).

\vfill

__[MLE]__ Find ${\boldsymbol{\theta}}$ maximizing
$$
\ln p(\mathbf{y}|X,\boldsymbol{\theta})
= - \frac{1}{2\sigma^{2}} \sum_{i=1}^{n} (y_{i} - \mathbf{x}_{i} \boldsymbol{\theta})^{2} + \textrm{const.}
$$
_without prior contribution of parameter, and $\sigma$ is known._

\vfill

__[MSE]__ Find $\boldsymbol{\theta}$ minimizing
$$
\sum_{i=1}^{n} (y_{i} - \mathbf{x}_{i} \boldsymbol{\theta})^{2}.
$$

## Ridge regression, a linear regression with Gaussian prior (L2)

Prior distribution
$$
p(\boldsymbol{\theta}) = \mathcal{N}\!\left(\boldsymbol{\theta}|\mathbf{0}, \lambda^{-1} I\right) \propto \exp\left(-\frac{\lambda}{2}\|\boldsymbol{\theta}\|^{2}\right)
$$
where
\large
$$\|\boldsymbol{\theta}\|^{2} = \sum_{j=1}^{p} \theta_{j}^{2},\,\textsf{\color{blue}L2-norm}.$$

\normalsize
Maximize
$$
\ln p(\mathbf{y}|X,\boldsymbol{\theta}) + \ln p(\boldsymbol{\theta}|\lambda)
= - \frac{1}{2\sigma^{2}} \sum_{i=1}^{n} (y_{i} - \mathbf{x}_{i} \boldsymbol{\theta})^{2}
- \frac{\lambda}{2} \|\boldsymbol{\theta}\|^{2}
$$

Minimize $L_{2}$-regularized error
$$
\sum_{i=1}^{n} (y_{i} - \mathbf{x}_{i} \boldsymbol{\theta})^{2}
+ \frac{\lambda}{2} \|\boldsymbol{\theta}\|^{2}
$$

## Lasso regression, a linear regression with Laplace prior (L1)

Prior distribution
$$
p(\boldsymbol{\theta}) = \textsf{Laplace}(\boldsymbol{\theta}| \lambda) \propto \exp\left(-\lambda\|\boldsymbol{\theta}\|_{1}\right)
$$
where
\large
$$\|\boldsymbol{\theta}\|_{1} = \sum_{j=1}^{p} |\theta_{j}|,\,\textsf{\color{blue}L1-norm}.$$

\normalsize
Maximize
$$
\ln p(\mathbf{y}|X,\boldsymbol{\theta}) + \ln p(\boldsymbol{\theta}|\lambda)
= - \frac{1}{2\sigma^{2}} \sum_{i=1}^{n} (y_{i} - \mathbf{x}_{i} \boldsymbol{\theta})^{2}
- \lambda \|\boldsymbol{\theta}\|_{1}
$$

Minimize $L_{1}$-regularized error
$$
\sum_{i=1}^{n} (y_{i} - \mathbf{x}_{i} \boldsymbol{\theta})^{2}
+ \lambda \|\boldsymbol{\theta}\|_{1}
$$

(Tibshirani, 1996)


## Geometric intuition of regularization.

Consider a simple regression model: $y_{i} = \theta_{1} X_{i1} + \theta_{2} X_{i2}$.

\vfill

\only<1-2>{\includegraphics[width=.35\textwidth]{Vis/Geom_L2.pdf}}
\only<2>{\includegraphics[width=.35\textwidth]{Vis/Geom_L1.pdf}}
\only<3>{\centerline{\includegraphics[width=.75\textwidth]{Vis/L1L2.pdf}}}

\vfill

\only<3>{
\begin{itemize}
\item Both regularization priors shrink the coefficients toward zero.
\item But only L1 can effectively "select" variables; although we want $L_{0}$.
\end{itemize}

\flushleft{\tiny Hastie, Tibshirani, Friedman, {\it The Elements of Statistical Learning.}}
}


## Posterior inference of the regularized regression models

Our goal is to estimate (1) posterior distribution
$$
p(\boldsymbol{\theta}|\mathbf{y}, X) = \frac{p(\mathbf{y}|X,\boldsymbol{\theta})p(\boldsymbol{\theta})}{p(\boldsymbol{y}|X)}.
$$

Then (2) using $p(\boldsymbol{\theta}|\mathbf{y},X)$, predict
$p(\mathbf{y}^{\star}|\mathbf{y},X)$ by averaging over all possible
$\boldsymbol{\theta}$ sampled from the estimated posterior
distribution.

\vfill

* Usually posterior prediction (2) is can be easily simulated with
  accurate estimation of posterior distribution (1).

* Posterior inference can be done analytically or not, depending on
  the choice of $p(\boldsymbol{\theta})$\footnote{We term prior $p(\boldsymbol{\theta})$ a {\it conjugate prior} if its posterior $p(\boldsymbol{\theta}|\mathbf{y},X)$ is of the same type of distribution.}.

## We can find an analytical solution in L2-regularized regression

$$
\ln p(\mathbf{\boldsymbol{\theta}}|\mathbf{y},X) =
-\frac{1}{2\sigma^{2}} (\mathbf{y} - X\boldsymbol{\theta})^{\top}(\mathbf{y} - X\boldsymbol{\theta}) - \frac{\lambda}{2} \boldsymbol{\theta}^{\top}\boldsymbol{\theta} + \textrm{const.}
$$

\onslide<2->{
By taking derivative with respect to $\boldsymbol{\theta}$ and setting it the zero vector:
$$
\nabla_{\boldsymbol{\theta}} = -\frac{1}{\sigma^{2}} X^{\top}(\mathbf{y} - X\boldsymbol{\theta})
-\lambda \boldsymbol{\theta} = 0
$$
}

\vfill

\onslide<3->{
Rearranging the equation:
$$
X^{\top}\mathbf{y} = (X^{\top}X + \lambda\sigma^{2} I) \boldsymbol{\theta}\,
\implies\,
\hat{\boldsymbol{\theta}} = (X^{\top}X + \lambda\sigma^{2} I)^{-1} X^{\top}\mathbf{y}
$$
}

\onslide<4>{
\textit{Remark}: For $n \ll p$, the inverse $(X^{\top}X)^{-1}$ may not exists, but $(X^{\top}X + \lambda\sigma^{2}I)^{-1}$ can exist with a proper $\lambda$.
}


## We can solve L1-regularized regression numerically

:::::: {.columns}
::: {.column width=.45}

```{r out.width=".9\\textwidth", results="asis", onslide.plot="1-"}
knitr::include_graphics("Vis/LARS.pdf")
```

:::
::: {.column width=.45}

Algorithms from statistics:

* Efron *et al.* Least Angle Regression (2002)

* Hans *et al.*, Shotgun search (2007)

* Friedman *et al.*, `glmnet` (2010)

From ML:

* Figueiredo *et al.* PAMI (2003)

* Seeger *et al.* JMLR (2008)

:::
::::::

## In practice, the greedy algorithm of `glmnet` works so well

Goal:

$$
\min_{\boldsymbol{\theta}} \quad
\overbrace{(\mathbf{y} - X\boldsymbol{\theta})^{\top}(\mathbf{y} - X\boldsymbol{\theta})}^{\textsf{\color{blue} RSS}} + \underbrace{\lambda \alpha \|\boldsymbol{\theta}\|_{1}}_{\textsf{\color{red} variable selection}} + \underbrace{\lambda (1 - \alpha) \|\boldsymbol{\theta}\|_{2}}_{\textsf{\color{magenta} shrinkage}}
$$

\onslide<2->{
The variable-by-variable update equation makes sense:

For each $\theta_{j}$,
}

\only<2>{$$
\hat{\theta}_{j}^{\textsf{glmnet}} \gets
\frac{S\left(
\sum_{i=1}^{n} X_{ij} (y_{i} - \hat{y}_{i}^{(-j)}),
\lambda\alpha
\right)}
{ \sum_{i=1}^{n} X_{ij}^{2} +\lambda (1- \alpha) }
\quad\textsf{vs.}\quad
\theta_{j}^{\textsf{MLE}} \gets
\frac{
\sum_{i=1}^{n} X_{ij} \left(
y_{i} - \sum_{k\neq j} X_{ik}\hat{\theta}_{k}
\right)}
{\sum_{i=1}^{n} X_{ij}^{2}}
$$
\tiny
Friedman {\it et al.}, Regularization Paths for Generalized Linear Models via Coordinate Descent (2010)
}

\only<3>{$$
\hat{\theta}_{j} \gets
\frac{\overset{\textsf{\color{red} threshold}}{S}
\left(
\sum_{i=1}^{n} X_{ij} \overbrace{(y_{i} - y_{i}^{(-j)})}^{\textsf{\color{red} residual w/o the variable } \theta_{j} },
\lambda\alpha
\right)}
{ \sum_{i=1}^{n} X_{ij}^{2} + \underbrace{\lambda (1- \alpha)}_{\textsf{\color{magenta} shrinkage}}}
$$
where $S(z, \tau)$ will set it to zero if $|z| < \tau$.
}

## Cross-validation: How do we tune hyper-parameters (e.g., $\lambda$)?

1. Divide the total training data $\mathcal{D}^{\textsf{train}} = \{(X,y)\}$ into two parts:

    * (1) cross-validation training $\{(X,y)\}$ and
	* (2) CV testing data $\{(X^\star,y^\star)\}$

2. For each different $(\lambda, \alpha)$ combination,

    - Train coefficients $\theta$ using CV training $\{(X,y)\} \subset \mathcal{D}^{\textsf{train}}$

	- Test how well $\sum_{j} X^{\star}_{ij} \hat{\theta}_{j}$ predicts $y^\star$?

3. Choose the optimal $(\lambda^\star, \alpha^\star)$


## How do we tune hyper-parameters (e.g., $\lambda$)?

Well, in `R`, we simply run

:::::: {.columns}
::: {.column width=.35}

```{r echo=TRUE, size="large"}
glm.cv.out <-
 glmnet::cv.glmnet(X,
          y,
          nfolds=5,
          alpha=1)
```

:::
::: {.column width=.55}

```{r fig.width=3, fig.height=2.5, only.plot="3"}
plot(glm.cv.out)
```

:::
::::::


## Revisit our working example with L1-regularization (`glmnet`)

```{r fig.width=5, fig.height=3.5}
.out <- glmnet::glmnet(X,y,alpha=1)
plot(.out)
```

## At the optimal $\lambda$ found by `cv.glmnet`

```{r}
lambda.cv <- glm.cv.out$lambda.min
.out <- glmnet::glmnet(x=X, y=y, lambda = lambda.cv)

lasso.dt <-
    data.table(beta = as.numeric(.out$beta)) %>%
    mutate(i = 1:n()) %>%
    mutate(l = as.integer(i %in% sim$causal)) %>%
    left_join(univar.dt) %>%
    arrange(desc(abs(beta))) %>%
    mutate(lasso = 1:n()) %>%
    as.data.table
```

```{r fig.width=5, fig.height=3}
.gg.plot(lasso.dt, aes(lasso, beta, fill=as.factor(l))) +
    theme(legend.position = c(1,1)) +
    theme(legend.justification = c(1,1)) +
    xlab("variables (ranked by glmnet)") + ylab("coefficients") +
    geom_point(data=head(lasso.dt, 100), pch=21) +
    geom_hline(yintercept = 0, color="black") +
    scale_fill_manual("causal", values=c("gray", "red"))
```

## Bias-variance tradeoff explains why a regularized regression works in practice

\large

$$
\min_{\boldsymbol{\theta}} \quad
\overbrace{(\mathbf{y} - X\boldsymbol{\theta})^{\top}(\mathbf{y} - X\boldsymbol{\theta})}^{\approx \textsf{\color{blue} bias}} + \underbrace{\lambda \alpha \|\boldsymbol{\theta}\|_{1}}_{\textsf{\color{red} variable selection}} + \underbrace{\lambda (1 - \alpha) \|\boldsymbol{\theta}\|_{2}}_{\textsf{\color{magenta} shrinkage}}
$$

* The second and the third terms control the model variance


## Can we try out different prior (regularization)?

\centerline{\includegraphics[width=.75\textwidth]{Vis/L1L2.pdf}}

## Bayesian spike-and-slab prior to achieve L0 norm

\centerline{\includegraphics[width=.75\textwidth]{Vis/SS.pdf}}

\vfill

Hern\'andez-Lobato {\it et al.} (2015)

## Bayesian spike-and-slab prior to select variables (literally)

With **indicator** variables, ${z_{1},\ldots,z_{p} \in \{0, 1\}}$,
$$\left(\begin{array}{l}
y_{1}\\
y_{2}\\
\vdots\\
y_{n}
\end{array}\right)
=
z_{1} \beta_{1} \left(\begin{array}{l}
X_{11}\\
X_{21}\\
\vdots\\
X_{n1}
\end{array}\right) +
\cdots
z_{1} \beta_{p} \left(\begin{array}{l}
X_{1p}\\
X_{2p}\\
\vdots\\
X_{np}
\end{array}\right),$$
$$\mathbf{y} = X \boldsymbol{\theta} + \boldsymbol{\epsilon},\quad
\theta_{j} | z_{j} = 1 \sim \mathcal{N}\!\left(\beta_{j}, \sigma_{j}^{2}\right),\, \forall j.$$

\vfill

\flushleft{\tiny Mitchell\& Beauchamp (1988); Ishwaran\& Rao (2005); (...); Carbonetto\& Stephens (2012)}

## Bayesian inference with sparse Bayesian prior

```{r}
if(!require(fqtl)) remotes::install_github("ypark/fqtl")
spike.slab.result <- fig.dir %&% "/spike_slab.RDS"
if.needed(spike.slab.result,
{
    ss.out <- fit.fqtl(y, X, tol = 1e-8, vbiter = 3000,
                       options = list(out.residual=FALSE))
    saveRDS(ss.out, spike.slab.result)
})
ss.out <- readRDS(spike.slab.result)
ss.dt <- setDT(lapply(ss.out$mean, as.numeric)) %>%
    mutate(i = 1:n())
```

```{r fig.width=5, fig.height=3, only.plot="1"}
.dt <- lasso.dt %>%
    left_join(ss.dt) %>%
    arrange(desc(lodds)) %>%
    mutate(ss = 1:n())

.gg.plot(.dt, aes(ss, theta, fill=as.factor(l))) +
    theme(legend.position = c(1,1)) +
    theme(legend.justification = c(1,1)) +
    xlab("variables (ranked by Bayesian posterior)") + ylab("coefficients") +
    geom_point(data = head(.dt, 100), pch=21, stroke = 0.1) +
    geom_hline(yintercept = 0, color="black") +
    scale_fill_manual("causal", values=c("gray", "red"))
```

```{r fig.width = 5, fig.height = 3, only.plot="2"}
.gg.plot(.dt, aes(x=`beta`, y=`theta`, fill=as.factor(l))) +
    geom_hline(yintercept = 0, color="black") +
    theme(legend.position = c(0,1)) +
    theme(legend.justification = c(0,1)) +
    xlab("lasso (glmnet)") + ylab("spike-and-slab") +
    geom_point(data=.dt[abs(`beta`) > 0 | `lodds` > -2, ], pch=21) +
    scale_fill_manual("causal", values=c("gray", "red"))
```

## A Bayesian approach successfully handles high-dimensional regression problems

```{r out.width="\\textwidth", results="asis"}
knitr::include_graphics("Vis/gwas_cm.pdf")
```

\vfill

\tiny
Carbonetto & Stephens (2012)

# Knock-off filter to control False Discovery Rate

##

\Large

Many variables to test for their non-zero-ness $\to$

\Huge

multiple hypothesis testing!

## False discovery rate in high-dimensional variable selection

\Large

$$FDR(\tau) =
\frac{\sum_{j=1}^{p} I\{ |\hat{\theta}_{j}| > \tau \wedge \theta_{j} = 0 \}}
{\max\{1, \sum_{j=1}^{p} I\{ |\hat{\theta}_{j}| > \tau \} \}}$$

\normalsize

where

* $\hat\theta_{j}$: estimation using data

* $\theta_{j}$: true random variable


## Can we simply attempt to control FDR as in DEG analysis?

\Large

1. Perform variable-by-variable association tests

2. Combine p-values

3. Run multiple hypothesis correction (e.g., Bonferroni, Benjamini-Hochberg)


## Can we simply attempt to control FDR as in DEG analysis?

```{r}
.dt <-
    univar.dt %>% mutate(qv = p.adjust(pval, "BH")) %>%
    arrange(l)

.fdr <- max(min(.dt$qv), .05)
.cutoff <- max(.dt[qv <= .fdr, .(pval)])
.fwer <- 0.05 / nrow(univar.dt)
```

```{r fig.width=5, fig.height=2.5, only.plot="1"}
.gg.plot(.dt, aes(i, -log10(pval))) +
    xlab("variables") +
    geom_point(pch=21, stroke=.1, fill="gray") +
    scale_y_continuous("p-value", labels=function(x) 10^(-x))
```

```{r fig.width=5, fig.height=2.5, only.plot="2"}
.gg.plot(.dt, aes(i, -log10(pval))) +
    xlab("variables") +
    geom_point(aes(fill = as.factor(l)), pch=21, stroke=.1) +
    scale_y_continuous("p-value", labels=function(x) 10^(-x)) +
    scale_fill_manual("causal", values = c("gray", "red")) +
    geom_hline(yintercept = -log10(.cutoff), colour="red", lty=2) +
    geom_label(data = data.table(),
               aes(0, -log10(.cutoff),
                   label="Benjamini Hochberg FDR <= " %&% round(100*.fdr) %&% "%"),
               size=4, alpha=.7, colour="red", vjust = 0, hjust = 0) +
    geom_hline(yintercept = -log10(.fwer), colour="blue", lty=2) +
    geom_label(data = data.table(),
               aes(nrow(univar.dt), -log10(.fwer),
                   label="Bonferroni FWER < 5%"),
               size=4, alpha=.7, colour="blue", vjust = 1, hjust = 1)
```

Variant-by-variant tests fail to control false discovery rate. Why?

## How about using multivariate OLS results?

```{r fig.width=5, fig.height=2.5}
.dt <- .ols.dt %>%
    mutate(qv = p.adjust(`Pr(>|t|)`, "BH")) %>%
    arrange(l, desc(`Pr(>|t|)`)) %>%
    as.data.table

.fdr <- max(min(.dt$qv), .05)

.cutoff <- max(.dt[qv <= .fdr, .(`Pr(>|t|)`)])

.fwer <- 0.05 / nrow(univar.dt)

.gg.plot(.dt, aes(i, -log10(`Pr(>|t|)`))) +
    xlab("variables") +
    geom_point(aes(fill = as.factor(l)), pch=21, stroke=.1) +
    scale_y_continuous("p-value", labels=function(x) 10^(-x)) +
    scale_fill_manual("causal", values = c("gray", "red")) +
    geom_hline(yintercept = -log10(.cutoff), colour="red", lty=2) +
    geom_label(data = data.table(),
               aes(0, -log10(.cutoff),
                   label="Benjamini Hochberg FDR <= " %&% round(100*.fdr) %&% "%"),
               size=4, alpha=.7, colour="red", vjust = 1, hjust = 0) +
    geom_hline(yintercept = -log10(.fwer), colour="blue", lty=2) +
    geom_label(data = data.table(),
               aes(nrow(univar.dt), -log10(.fwer),
                   label="Bonferroni FWER < 5%"),
               size=4, alpha=.7, colour="blue", vjust = 0, hjust = 1)
```

* What happened to the other 1000 variables?

## Bayesian posterior inclusion probability can help

```{r}
sigmoid <- function(x) {
    1/(1 + exp(-x))
}
logit <- function(x) {
    log(x) - log(1 - x)
}
```

```{r fig.width=5, fig.height=2.5}
.dt <- lasso.dt %>%
    left_join(ss.dt) %>%
    arrange(desc(lodds)) %>%
    mutate(ss = 1:n()) %>%
    arrange(l)

.gg.plot(.dt, aes(i, lodds)) +
    geom_hline(yintercept = logit(.9), color="red", lty = 2) +
    xlab("variables") +
    geom_point(aes(fill = as.factor(l)), pch=21, stroke = .1) +
    scale_fill_manual("causal", values = c("gray", "red")) +
    scale_y_continuous("Posterior Inclusion Probability",
                       labels = function(x) round(sigmoid(x), 2))
```

- Okay, but what is FDR? Can we consider ($1-$ PIP) as FDR?

## How we estimate the False Discovery Rate for non-zero regression coefficient?

\Large

- What should be the null distribution of regression coefficient?

- Is it t-distributed (the default option of `lm`)?


## First attempt: Construct "null" regression data by sample permutation?

```{r echo = TRUE, size = "large"}
set.seed(17)
X.perm <- apply(X, 2, sample)
```

```{r}
spike.slab.result <- fig.dir %&% "/spike_slab_perm.RDS"
if.needed(spike.slab.result,
{
    ss.out <- fit.fqtl(y, cbind(X, X.perm),
                       tol = 1e-8, vbiter = 3000,
                       options = list(out.residual=FALSE))
    saveRDS(ss.out, spike.slab.result)
})
ss.out <- readRDS(spike.slab.result)
```

\Large

- What are we missing?

- It is not clear whether we can control Type-I error.

$$\mathbf{y} \sim [X, \underbrace{\tilde{X}}_{\textsf{\color{red} permuted}}]$$

## Can we learn FDR cutoff from the permuted coefficients?

```{r}
.dt <- setDT(lapply(ss.out$mean, as.numeric)) %>%
    mutate(i = 1:n()) %>%
    mutate(l = as.integer(i %in% sim$causal)) %>%
    mutate(permuted = if_else(i > ncol(X), "permuted", "observed")) %>%
    mutate(i = i %% ncol(X) + 1) %>%
    arrange(l)

.fdr <- .05
.cutoff <- quantile(.dt[permuted == "permuted"]$lodds, 1 - .fdr)
```

```{r fig.width=6, fig.height=3, only.plot="1"}
ggplot(.dt, aes(i, lodds)) +
    facet_grid(. ~ permuted) +
    geom_point(aes(fill=as.factor(l)), pch=21, stroke = .1) +
    xlab("variables") +
    scale_fill_manual("causal", values = c("gray", "red")) +
    scale_y_continuous("Posterior Inclusion Probability",
                       labels = function(x) round(sigmoid(x), 2))
```

```{r fig.width=6, fig.height=3, only.plot="2"}
ggplot(.dt, aes(i, lodds)) +
    facet_grid(. ~ permuted) +
    geom_point(aes(fill=as.factor(l)), pch=21, stroke = .1) +
    geom_hline(yintercept = .cutoff, lty = 2, color="magenta") +
    geom_label(data = data.table(i=2000, lodds=.cutoff),
               aes(label="FDR <= " %&% round(100*.fdr) %&% "%?"),
               size=4, alpha=.7, colour="magenta", vjust = 0, hjust = 1) +
    xlab("variables") +
    scale_fill_manual("causal", values = c("gray", "red")) +
    scale_y_continuous("Posterior Inclusion Probability",
                       labels = function(x) round(sigmoid(x), 2))
```

## Can we calibrate FDR using the permuted data?

\large

Not really...

* Let $\rho_{j}$ be estimated posterior probability $p(\theta_{j} \neq 0|\textsf{data})$

* empirical False Discovery Rate =

$$\frac{\sum_{j} I\{\rho_{j} > \tau \wedge \theta_{j} = 0 \}}{\sum_{j} I\{ \rho_{j} > \tau \}}$$

```{r}
efdr <- .dt[permuted == "observed",
            sum(lodds > .cutoff & l == 0) / pmax(sum(lodds > .cutoff), 1)]
```

In this example, we have `r round(efdr*100)` %

* What have we missed?

## What went wrong?

\Large

1. We need to apply different threshold levels for different varaibles

2. We didn't consider correlation (col-linearity) structures between variables

3. Naive permutation steps break the covariance structure in $X$

## KO Idea 1: Preserve dependency structure between variables

:::::: {.columns}
::: {.column width=.45}

\onslide<1->{Dependency structure in observed data}

```{r out.height=".6\\textheight", results="asis", onslide.plot="1-"}
knitr::include_graphics("Vis/Knockoff1.pdf")
```

:::
::: {.column width=.45}

\only<2>{\large What permutation did...}

```{r out.height=".58\\textheight", results="asis", only.plot="2"}
knitr::include_graphics("Vis/Knockoff2.pdf")
```

\only<3>{\large What we want}

```{r out.height=".58\\textheight", results="asis", only.plot="3"}
knitr::include_graphics("Vis/Knockoff3.pdf")
```

:::
::::::


## Second attempt: Construct "null" regression data by permutation preserving inter-variable dependency

```{r echo = TRUE, size = "large"}
set.seed(17)
X.perm <- X[sample(nrow(X)), ]
```

```{r}
spike.slab.result <- fig.dir %&% "/spike_slab_perm_2.RDS"
if.needed(spike.slab.result,
{
    ss.out <- fit.fqtl(y, cbind(X, X.perm),
                       tol = 1e-8, vbiter = 3000,
                       options = list(out.residual=FALSE))
    saveRDS(ss.out, spike.slab.result)
})
ss.out <- readRDS(spike.slab.result)
```

\Large

- What are we missing?

- It is not clear whether we can control Type-I error.

$$\mathbf{y} \sim [X, \underbrace{\tilde{X}}_{\textsf{\color{red} permuted}}]$$

## Can we learn FDR cutoff from the permuted coefficients?

```{r}
.dt <- setDT(lapply(ss.out$mean, as.numeric)) %>%
    mutate(i = 1:n()) %>%
    mutate(l = as.integer(i %in% sim$causal)) %>%
    mutate(permuted = if_else(i > ncol(X), "permuted", "observed")) %>%
    mutate(i = i %% ncol(X) + 1) %>%
    arrange(l)

.fdr <- .05
.cutoff <- quantile(.dt[permuted == "permuted"]$lodds, 1 - .fdr)
```

```{r fig.width=6, fig.height=3, only.plot="1"}
ggplot(.dt, aes(i, lodds)) +
    facet_grid(. ~ permuted) +
    geom_point(aes(fill=as.factor(l)), pch=21, stroke = .1) +
    xlab("variables") +
    scale_fill_manual("causal", values = c("gray", "red")) +
    scale_y_continuous("Posterior Inclusion Probability",
                       labels = function(x) round(sigmoid(x), 2))
```

```{r fig.width=6, fig.height=3, only.plot="2"}
ggplot(.dt, aes(i, lodds)) +
    facet_grid(. ~ permuted) +
    geom_point(aes(fill=as.factor(l)), pch=21, stroke = .1) +
    geom_hline(yintercept = .cutoff, lty = 2, color="magenta") +
    geom_label(data = data.table(i=2000, lodds=.cutoff),
               aes(label="FDR <= " %&% round(100*.fdr) %&% "%?"),
               size=4, alpha=.7, colour="magenta", vjust = 0, hjust = 1) +
    xlab("variables") +
    scale_fill_manual("causal", values = c("gray", "red")) +
    scale_y_continuous("Posterior Inclusion Probability",
                       labels = function(x) round(sigmoid(x), 2))
```

## Can we calibrate FDR using the permuted data?

\large

Not really...

* Let $\rho_{j}$ be estimated posterior probability $p(\theta_{j} \neq 0|\textsf{data})$

* empirical False Discovery Rate =

$$\frac{\sum_{j} I\{\rho_{j} > \tau \wedge \theta_{j} = 0 \}}{\sum_{j} I\{ \rho_{j} > \tau \}}$$

```{r}
efdr <- .dt[permuted == "observed",
            sum(lodds > .cutoff & l == 0) / pmax(sum(lodds > .cutoff), 1)]
```

In this example, we have `r round(efdr*100)` %

* What are we still missing?


## Is our comparison scheme fair? We comparing one variable against all the null variables

```{r out.width=".8\\textwidth", results="asis"}
knitr::include_graphics("Vis/Knockoff4.pdf")
```

## KO Idea 2: Matched comparison: $X_{j}$ vs. $\tilde{X}_{j}$

```{r out.width=".8\\textwidth", results="asis"}
knitr::include_graphics("Vis/Knockoff5.pdf")
```


## Construct a knockoff matrix preserving both correlation structures

::: {.block}
### Knock-off filter

Given $X = (X_{1},\ldots, X_{p})$, a new family of random variables,
$\tilde{X} = (\tilde{X}_{1},\ldots, \tilde{X}_{p})$ are considered a valid "knockoff" filter if

1. $\tilde{X}$ is independent of $Y$ given $X$

2. distribution of $(X, \tilde{X})$ remain invariant to any swapping between the original and knockoff variables.

:::

E.g.,

\onslide<2->{
\centering
	$(X_{1},{\color{red} X_{2}},X_{3}, \tilde{X}_{1}, {\color{red} \tilde{X}_{2}},\tilde{X}_{3}) \overset{d}{=}
      (X_{1},{\color{red} \tilde{X}_{2}}, X_{3}, \tilde{X}_{1}, {\color{red} X_{2}},\tilde{X}_{3})$
}
\onslide<3>{
\centering
	$({\color{red} X_{1}, X_{2}}, X_{3}, {\color{red}\tilde{X}_{1}, \tilde{X}_{2}}, \tilde{X}_{3}) \overset{d}{=}
      ({\color{red} \tilde{X}_{1}, \tilde{X}_{2}}, X_{3}, {\color{red} X_{1}, X_{2}}, \tilde{X}_{3})$
\centerline{(...)}
}

\vfill

\tiny
Candes, Fan, Janson, and Lv, *Panning for Gold: Model-X Knockoffs for High-dimensional Controlled Variable Selection*, (2018)

## Knockoff filter (null design matrix) preserves swap exchangeability

```{r out.width=".8\\textwidth", results="asis"}
knitr::include_graphics("Vis/Knockoff6.pdf")
```

## 

\Large

What would happen if we replace some variable $X_{k}$ with its knock-off copy $\tilde{X}_{k}$ whilst dependence with all the other variables remain unchanged?


## A reasonable approximation for knockoff construction

\Large
\begin{enumerate}
\item<1-> Fit $X \sim W Z$ matrix factorization
\item<2-> Predict $\hat{X} \gets \hat{U} \hat{Z}$
\item<3-> Take residuals $\epsilon = X - \hat{X}$
\item<4-> Add permuted residuals $\tilde{\epsilon}$, i.e., $\tilde{X} = \hat{X} + \tilde{\epsilon}$
\end{enumerate}

\vfill

\tiny
Zhu *et al.* DeepLINK: Deep learning inference using knockoffs with applications to genomics (2021)


## Knockoff statistics

\large

$$\mathbf{y} \sim [X, \underset{\textsf{\color{red} knockoff}}{\tilde{X}}]$$

For each variable $j$ (Lasso):

$$W_{j} = |\hat{\theta}_{j}| - |\tilde{\theta}_{j}|$$

For each variable $j$ (Bayesian PIP):

$$W_{j} = \hat{\rho}_{j} - \tilde{\rho}_{j}$$


## Knockoff statistics: What is FDR here?

```{r}
.svd <- rsvd::rsvd(X, k=30)
.lm <- lm(X ~ .svd$u)
X.ko <- scale(.lm$fitted + apply(.lm$residuals, 2, sample))

spike.slab.result <- fig.dir %&% "/spike_slab_ko.RDS"
if.needed(spike.slab.result,
{
    ss.out <- fit.fqtl(y, cbind(X, X.ko),
                       tol = 1e-8, vbiter = 3000,
                       options = list(out.residual=FALSE))
    saveRDS(ss.out, spike.slab.result)
})
ss.out <- readRDS(spike.slab.result)
```

```{r}
.dt <- setDT(lapply(ss.out$mean, as.numeric))

obs.dt <- .dt[1:ncol(X)] %>% mutate(i = 1:n())
ko.dt <- .dt[seq(ncol(X)+1, ncol(X) + ncol(X.ko))] %>%
    mutate(i = 1:n())

spike.slab.ko.dt <- left_join(obs.dt, ko.dt,
                                by = "i",
                                suffix = c("",".ko")) %>%
    mutate(W = lodds - lodds.ko) %>%
    mutate(l = as.integer(i %in% sim$causal)) %>%
    arrange(l)

ko.cutoff <- knockoff::knockoff.threshold(spike.slab.ko.dt$W, 0.2)
```

```{r fig.width=5, fig.height=2.5, only.plot="1"}
ggplot(spike.slab.ko.dt, aes(i, W)) +
    xlab("variables") +
    geom_point(aes(fill=as.factor(l)), pch=21, stroke = .1) +
    scale_fill_manual("causal", values = c("gray", "red"))
```

```{r fig.width=5, fig.height=2.5, only.plot="2"}
ggplot(spike.slab.ko.dt, aes(i, W)) +
    xlab("variables") +
    geom_point(aes(fill=as.factor(l)), pch=21, stroke = .1) +
    scale_fill_manual("causal", values = c("gray", "red")) +
    geom_hline(yintercept = ko.cutoff, colour = "red", lty = 2) +
    geom_hline(yintercept = -ko.cutoff, colour = "red", lty = 2)
```

# Discussion

## Other methods that we haven't had a chance to discuss

* Ensemble learning

    - Boosting, Model-averaging

* Bayesian non-parametric models

    - We select models by *not* selecting a model
	
	- Gaussian process

* Deep neural network model
