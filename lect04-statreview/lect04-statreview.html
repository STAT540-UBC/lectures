<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Review of Probability and Statistics</title>
    <meta charset="utf-8" />
    <meta name="author" content="  Keegan Korthauer " />
    <script src="libs/header-attrs-2.11/header-attrs.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# <big><big>Review of Probability and Statistics</big></big>
### <br><font size=6> Keegan Korthauer </font>
### <font size=6> 19 January 2022 </font> <br><br><font size=4> with slide contributions from Sara Mostafavi </font>

---






&lt;style type="text/css"&gt;
pre {
  white-space: pre-wrap;
}
.remark-code {
  background: #f8f8f8;
}
.remark-inline-code {
  background: "white";
}
.remark-code {
  font-size: 22px;
}
.huge .remark-code { /*Change made here*/
  font-size: 200% !important;
}
.tiny .remark-code { /*Change made here*/
  font-size: 60% !important;
}
.smaller .remark-code { /*Change made here*/
  font-size: 90% !important;
}
.smaller2 .remark-code { /*Change made here*/
  font-size: 80% !important;
}
.smaller3 .remark-code { /*Change made here*/
  font-size: 70% !important;
}
&lt;/style&gt;


&lt;style&gt;
div.blue { background-color:#e8f2f6; border-radius: 5px; padding: 20px;}
&lt;/style&gt;

#  Preview of next 6 lectures

* **Lecture 4: Review of Probability and Statistics**

* Lecture 5: Statistical Inference - two group comparisons 

* Lecture 6: Statistical Inference - linear regression and ANOVA

* Lecture 7: Statistical Inference - multiple linear regression 

* Lecture 8: Statistical Inference - continuous regression + limma

* Lecture 9: Statistical Inference - multiple testing

---

#  Outline for today

* Intro: philosophy, goals, and central concepts

* Review: Random Variables, Probability Distributions, Sampling Distribution, Estimation, Inference, CLT, Hypothesis Testing

&lt;div class = "blue"&gt;
Learning objectives: 
  &lt;ol&gt;1. be familiar with the terminology&lt;/ol&gt;
  &lt;ol&gt;2. have a clear understanding of the concepts&lt;/ol&gt;
&lt;/div&gt;
---

class: middle
# &lt;center&gt; What is Statistics?

  
---

#  Statistics 

* The field of statistics concerns the science of **collecting, analyzing/modeling, interpreting** data and **communicating uncertainty** about the results
  - Data science and machine learning have facilitated application to 'big data'
  
* Statistical and computational methods should not be used as generic "recipes" to follow `\(\rightarrow\)` non-robust science

* We aim for:
  - rigorous understanding to perform routine statistical analysis
  
  - solid foundation to follow up on specific topics
---


#  Statistical Inference


A framework for generating conclusions about a population from a sample of noisy data

&lt;img src="img/statInference.png" width="650" style="display: block; margin: auto;" /&gt;

* Language of **probability** enables us to discuss *uncertainty* and make *predictions*
* **Statistical inference** enables us to make *conclusions* about the data
* We need both to learn from data

---

#  Review: terminology &amp; basic concepts

* Random variables and their distributions

* Models, parameters, and their estimators

* Central Limit Theorem (CLT)

* Hypothesis Testing

---

#  Variables

&lt;div class = "blue"&gt;
&lt;b&gt;Variable:&lt;/b&gt; an element, feature, or factor that is liable to vary or change
&lt;/div&gt;

* In statistical terminology, a **variable** is an unknown quantity that we'd like to study

* Most research questions can be formulated as 
&gt;What's the relationship between two or more variables?  
---

#  Random variables

&lt;div class = "blue"&gt;
&lt;b&gt;Random Variable (RV):&lt;/b&gt; A variable whose value results from the measurement of a quantity that is subject to variation (e.g. the &lt;i&gt;outcome&lt;/i&gt; an experiment)
&lt;/div&gt;

  - Examples: a coin flip, a dice throw, the expression level of gene X
  
  - An RV has a *probability distribution*
  
---
  
#  Distributions of Random Variables (RVs)

&lt;div class = "blue"&gt;
&lt;b&gt;Probability:&lt;/b&gt; A number assigned to an outcome/event that describes the extent to which it is likely to occur
&lt;/div&gt;

  - Must satisfy certain rules (e.g. be between 0 and 1)
  
  - Represents the (long-term) *frequency* of an event
  
--

&lt;div class = "blue"&gt;
&lt;b&gt;Probability distribution:&lt;/b&gt; A mathematical function that maps outcomes/events to probabilities
&lt;/div&gt;  
  
---

#  Example experiment: Two coin tosses



.pull-left[

* **Experiment:** Toss two coins

* **Sample space:** set of all possible outcomes &lt;small&gt; `\({\normalsize S=\{TT, HT, TH, HH\}}\)` &lt;/small&gt;

* **Random Variable of interest:** number of heads



]
.pull-right[

|       | Outcome  | Number of Heads |
| :---: |:---------:| :-----------:| 
| TT    | &lt;img src="img/tails.png" width="60" /&gt;&lt;img src="img/tails.png" width="60" /&gt; | 0 |
| HT    | &lt;img src="img/heads.png" width="60" /&gt;&lt;img src="img/tails.png" width="60" /&gt; | 1 |
| TH    | &lt;img src="img/tails.png" width="60" /&gt;&lt;img src="img/heads.png" width="60" /&gt; | 1 |
| HH    | &lt;img src="img/heads.png" width="60" /&gt;&lt;img src="img/heads.png" width="60" /&gt; | 2 |

]

---

#  Assigning probability to outcomes


.pull-left[

* Let:

  * `\(\omega=\)` an outcome 

  * `\(X(\omega)=\)` number of heads in `\(\omega\)` (RV)
  
* Each possible outcome is associated with a probability

* **Event:** A set of outcomes that satisfy some condition

* Each realization of the RV corresponds to an **event** (e.g. `\(X(\omega)=1\)` corresponds to the outcomes `\(TH\)` and `\(HT\)` )

]

.pull-right[

|       | `\(\omega\)`  | `\(X(\omega)\)` | Probability |
| :---: |:---------:| :-----------:| :---------: |
| TT    | &lt;img src="img/tails.png" width="60" /&gt;&lt;img src="img/tails.png" width="60" /&gt; | 0 | 0.25 |
| HT    | &lt;img src="img/heads.png" width="60" /&gt;&lt;img src="img/tails.png" width="60" /&gt; | 1 | 0.25 |
| TH    | &lt;img src="img/tails.png" width="60" /&gt;&lt;img src="img/heads.png" width="60" /&gt; | 1 | 0.25 |
| HH    | &lt;img src="img/heads.png" width="60" /&gt;&lt;img src="img/heads.png" width="60" /&gt; | 2 | 0.25 |
]

---

#  Assigning probability to events

The probability distribution of the Random Variable `\(X\)` tells us how likely each event (number of heads) is to occur in the experiment


| Event | `\(x\)` | `\(P(X=x)\)` |
| :-----------:| :-----------:| :---------: |
| &lt;img src="img/tails.png" width="60" /&gt;&lt;img src="img/tails.png" width="60" /&gt; | 0 | 0.25 |
| &lt;img src="img/heads.png" width="60" /&gt;&lt;img src="img/tails.png" width="60" /&gt; &lt;big&gt;, &lt;img src="img/tails.png" width="60" /&gt;&lt;img src="img/heads.png" width="60" /&gt; | 1 | 0.50 |
| &lt;img src="img/heads.png" width="60" /&gt;&lt;img src="img/heads.png" width="60" /&gt; | 2 | 0.25 |


Note on notation: `\(P(X=x)\)` can also be written as `\(P_X(x)\)`

---

#  Two types of random variables

* A **discrete** RV has a countable number of possible values
  
  - e.g. throwing dice, genotype measured on a SNP chip

* A **continuous** RV takes on values in an interval of numbers

  - e.g. blood glucose level, height of individuals

---

# Discrete or Continuous? 

* Select a **clap** reaction if you think the example is **discrete** &lt;img src="img/zoom_clap.png" width="60" /&gt;

* Select a **thumbs up** reaction if you think the example is **continuous** &lt;img src="img/zoom_thumb.png" width="60" /&gt;

---

#  Standard Gaussian (Normal) distribution 


.pull-left[
* probability density function (pdf):
`$$f(x|\mu,\sigma^2) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$$`

* **Parameters**: quantities that summarize a population
  * Mean `\(=\mu\)`
  * Standard Deviation `\(=\sigma\)`

* For convenience, we write `\(N(\mu, \sigma^2)\)`

* When `\(\mu=0\)` and `\(\sigma=1\)`, this is the *Standard* Normal distribution `\(N(0,1)\)`
]

.pull-right[
&lt;img src="img/normal.png" width="425" style="display: block; margin: auto;" /&gt;
]

---

#  Gaussian (Normal) distribution 

&lt;img src="img/normal2.png" width="610" style="display: block; margin: auto;" /&gt;

`$$\text{pdf: }f(x|\mu,\sigma^2) = \phi(x) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$$`

---

#  Density `\(\rightarrow\)` probability requires integration



&lt;img src="lect04-statreview_files/figure-html/unnamed-chunk-7-1.png" width="576" style="display: block; margin: auto;" /&gt;

---

#  Empirical Rule for Normal Distributions


&lt;img src="img/empirical_rule.png" width="450" style="display: block; margin: auto;" /&gt;

&lt;small&gt;&lt;small&gt;&lt;a href="https://commons.wikimedia.org/wiki/File:Empirical_rule_histogram.svg" style="color:grey;"&gt;Image source: By Melikamp - Own work, CC BY-SA 4.0&lt;/a&gt;

---

#  Statistical Inference

* The **parameter space** is the set of all possible values of a parameter

* One major goal: to "figure out" (i.e. estimate) the **parameter values**
  - i.e. *"fit the model to the data"*
  
* The model is a representation that (we hope) approximates the data and (more importantly) the population that the data were sampled from 

* We can then use this model for:
  - hypothesis testing
  - prediction 
  - simulation
 
---

#  Statistical Inference

&lt;img src="img/statInference.png" width="700" style="display: block; margin: auto;" /&gt;

---

#  IID

* A requirement (assumption) in many settings is that the data are **IID:** **I**ndependent and **I**dentically **D**istributed

--

* **Identically Distributed**: a set of observations (events) are from the same population 

  - i.e. they have the same underlying probability distribution
  - e.g. a t-test assumes that under the null, all observations come from the same normal distribution

--

* **Independent**: Events `\(A\)` and `\(B\)` are independent if and only if `\(P(A,B) = P(A)P(B)\)` 

  - i.e. the joint probability is the product of the individual event probabilities
  - The above statement is for two events, but the same definition applies for any number of events

---

#  Violations of independence

* Experimental design is in part about trying to avoid unwanted dependence

* Example of design with violation of independence assumption: 

&gt; Height measurements of individuals sampled from *related* females in a particular family are **not** independent 

---

#  Recall: parameters of the normal distribution 
.pull-left[
&lt;img src="img/normal2.png" width="650" style="display: block; margin: auto;" /&gt;

]

.pull-right[
`$$f(x|\mu,\sigma^2) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$$`
* Mean `\(=\mu\)`

* Standard Deviation `\(=\sigma\)`

* For convenience, we write `\(N(\mu, \sigma^2)\)`

* Population parameters are unknown

]

---

#  Parameter estimation

&lt;big&gt;

* **Estimator**: A function (or rule) used to estimate a parameter of interest

* **Estimate**: A particular realization (value) of an estimator

---

#  Estimators for normally distributed data

* If we are given a sample of `\(n\)` observations from a normally distributed population, how do we estimate the parameter values `\(\mu\)` and `\(\sigma\)`?

* Recall `\(\mu\)` is the mean and `\(\sigma\)` the standard deviation of the distribution

--

`$$\hat{\mu} = \bar{x} = \frac{x_1 + x_2 + ... + x_n}{n} =  \frac{1}{n} \sum_{i=1}^n x_i$$`

`$$\hat{\sigma} = s = \sqrt{\frac{\sum_{i=1}^n(x_i - \bar{x})^2}{n-1}}$$`


---

#  Estimators vs Parameters 

|   | Estimators | Parameters |  
| ---- | :---------: | :----------: |
| Summarize | Sample | Population (ground truth) |
| Value | Computed from data | Unknown&lt;sup&gt;`*`&lt;/sup&gt; |
| Notation | `\(\hat{\theta}\)` | `\(\theta\)` |

&lt;sup&gt;`*`&lt;/sup&gt;almost always

---

#  Normal **Mean**: Estimator vs Parameter

|   | Estimator | Parameter |  
| ---- | :---------: | :----------: |
| Summarizes |  Sample/data | Population (ground truth) |
| Value | `\(\bar{x}=\frac{1}{n} \sum_{i=1}^n x_i\)` | Unknown&lt;sup&gt;`*`&lt;/sup&gt; |
| Notation | `\(\hat{\mu}\)` | `\(\mu\)` |

&lt;sup&gt;`*`&lt;/sup&gt;almost always

---

#  Normal **Standard Deviation**: Estimator vs Parameter

|   | Estimator | Parameter |  
| ---- | :---------: | :----------: |
| Summarizes |  Sample/data | Population (ground truth) |
| Value | `\(s=\sqrt{\frac{\sum_{i=1}^n(x_i - \bar{x})^2}{n-1}}\)` | Unknown&lt;sup&gt;`*`&lt;/sup&gt; |
| Notation | `\(\hat{\sigma}\)` | `\(\sigma\)` |

&lt;sup&gt;`*`&lt;/sup&gt;almost always

---

# Estimator for normally distributed data

* Let's say we collected a **sample** from a population we assume to be normal 

* We estimate the mean `\(\large \hat{\mu}=\bar{x}\)`

* How good is the estimate?

* The answer depends on:

--

  - sample size
  
  - variability of the population 

---

# Sampling distribution

* **Statistic**: any quantity computed from values in a sample 

* Any function (or statistic) of a sample (data) is a random variable

* Thus, any statistic (because it is random) has its own probability distribution function `\(\rightarrow\)` specifically, we call this the **sampling distribution**

* Example: the sampling distribution of the mean

---

# Sampling distribution of the mean

The sample mean `\(\large \bar{x}\)` is a RV, so it has a probability or sampling distribution

&lt;img src="img/samplingdist.png" width="750" style="display: block; margin: auto;" /&gt;

&lt;small&gt;&lt;small&gt;&lt;a href="http://www.incertitudes.fr/book.pdf" style="color:grey;"&gt;Image source: incertitudes.fr/book.pdf&lt;/a&gt;

---

# Central Limit Theorem (CLT)

By the *Central Limit Theorem (CLT)*, we know that the sampling distribution of the mean (of `\(n\)` observations) is Normal:
* with mean `\(\mu_{\bar{X}} = \mu\)` and standard deviation `\(\sigma_{\bar{X}} = \frac{\sigma}{\sqrt{n}}\)`

&lt;img src="img/clt.png" width="725" style="display: block; margin: auto;" /&gt;

&lt;small&gt;&lt;small&gt;&lt;a href="http://www.incertitudes.fr/book.pdf" style="color:grey;"&gt;Image source: incertitudes.fr/book.pdf&lt;/a&gt;

---

# &lt;center&gt; ⚠️ Standard deviation vs standard error ⚠️

&lt;big&gt;

* The sampling distribution of the mean of `\(n\)` observations (by CLT): `$$\bar{X} \sim N(\mu, \frac{\sigma^2}{n})$$`

--

* The *standard error* of the mean is `\(\frac{\sigma}{\sqrt{n}}\)`

--

* The *standard deviation* of `\(X\)` is `\(\sigma\)`

---

class: middle

## Estimation of parameters of the sampling distribution of the mean

Just as we estimated `\(\mu\)` and `\(\sigma\)` before, we can estimate `\(\mu_{\bar{X}}\)` and `\(\sigma_{\bar{X}}\)`

  - `\(\hat{\mu}_{\bar{X}} = \hat{\mu} = \bar{x}\)`
  
  - `\(\hat{\sigma}_{\bar{X}} = \frac{\hat{\sigma}}{\sqrt{n}} = \frac{s}{\sqrt{n}}\)`

---

# Standard error of the mean

`$$\large\hat{\sigma}_{\bar{X}} = \frac{\hat{\sigma}}{\sqrt{n}} = \frac{s}{\sqrt{n}}$$`
* The standard error (SE) of the mean reflects uncertainty about our estimate of the population mean `\(\large\hat{\mu}\)`

* For the distributional assumptions to hold, the CLT assumes a 'large enough' sample: 

  - when the sample size is ~30 or more, the normal distribution is a good approximation for the sampling distribution of the mean
  
  - for smaller samples, the SE `\(\large\frac{s}{\sqrt{n}}\)` is an underestimate 
  
---

# CLT applies to any population... 

.pull-left[
**...regardless of distribution**

Let `\(\normalsize X_1, X_2, ..., X_n\)` be a random sample from a population with a non-normal distribution. If the sample size `\(\normalsize n\)` is sufficiently large, then the sampling distribution of the mean will be approximately normal: `\(\normalsize \bar{X} \sim N(\mu, \frac{\sigma^2}{n})\)`
]

.pull-right[
&lt;img src="img/clt2.png" width="575" style="display: block; margin: auto;" /&gt;
&lt;small&gt;&lt;small&gt;&lt;a href="https://saylordotorg.github.io/text_introductory-statistics/s10-02-the-sampling-distribution-of-t.html" style="color:grey;"&gt;Image source: saylordotorg.github.io/text_introductory-statistics&lt;/a&gt;
]


---

# Illustration (n = 3)
&lt;small&gt;

.pull-left[
&lt;img src="lect04-statreview_files/figure-html/unnamed-chunk-14-1.png" width="432" style="display: block; margin: auto;" /&gt;
]

.pull-right[
&lt;img src="lect04-statreview_files/figure-html/unnamed-chunk-15-1.png" width="432" style="display: block; margin: auto;" /&gt;
]

On right: dashed pink line is `\(N(\mu, \sigma^2/n)\)`
---

# Illustration (n = 10)
&lt;small&gt;

.pull-left[
&lt;img src="lect04-statreview_files/figure-html/unnamed-chunk-16-1.png" width="432" style="display: block; margin: auto;" /&gt;
]

.pull-right[
&lt;img src="lect04-statreview_files/figure-html/unnamed-chunk-17-1.png" width="432" style="display: block; margin: auto;" /&gt;
]

On right: dashed pink line is `\(N(\mu, \sigma^2/n)\)`

---

# Illustration (n = 30)
&lt;small&gt;

.pull-left[
&lt;img src="lect04-statreview_files/figure-html/unnamed-chunk-18-1.png" width="432" style="display: block; margin: auto;" /&gt;
]

.pull-right[
&lt;img src="lect04-statreview_files/figure-html/unnamed-chunk-19-1.png" width="432" style="display: block; margin: auto;" /&gt;
]


On right: dashed pink line is `\(N(\mu, \sigma^2/n)\)`

---

# Illustration (n = 100)
&lt;small&gt;

.pull-left[
&lt;img src="lect04-statreview_files/figure-html/unnamed-chunk-20-1.png" width="432" style="display: block; margin: auto;" /&gt;
]

.pull-right[
&lt;img src="lect04-statreview_files/figure-html/unnamed-chunk-21-1.png" width="432" style="display: block; margin: auto;" /&gt;
]


On right: dashed pink line is `\(N(\mu, \sigma^2/n)\)`

---

# Hypothesis Testing


* **Hypothesis:** A *testable (falsifiable)* idea for explaining a phenomenon

* **Statistical hypothesis:** A hypothesis that is testable on the basis of observing a process that is modeled via a set of random variables

* **Hypothesis Testing:** A formal procedure for determining whether to *accept* or *reject* a statistical hypothesis 

* Requires comparing two hypotheses:
  
  - `\(H_0\)`: null hypothesis
  
  - `\(H_A\)` or `\(H_1\)`: alternative hypothesis

---

# Hypothesis Testing: Motivating Example


* The expression level of gene `\(\normalsize g\)` is measured in `\(\normalsize n\)` patients with disease (e.g. cancer), and `\(\normalsize m\)` healthy (control) individuals:
  - `\(\normalsize z_1, z_2, ..., z_n\)` and `\(\normalsize y_1, y_2, ..., y_m\)`

--

* Is gene `\(\normalsize g\)` differentially expressed in cancer vs healthy samples?
  - `\(\normalsize H_0: \mu_Z = \mu_Y\)`
  - `\(\normalsize H_A: \mu_Z \neq \mu_Y\)`

* In this setting, hypothesis testing allows us to determine whether observed differences between groups in our data are *significant*


---

# Steps in Hypothesis Testing

&lt;big&gt; 

1. Formulate your hypothesis as a statistical hypothesis

2. Define a test statistic `\(t\)` (RV) that corresponds to the question. You typically know the expected distribution of the test statistic *under the null*

3. Compute the p-value associated with the observed test statistic under the null distribution `\(\normalsize p(t | H_0)\)`

---

# Motivating example (cancer vs healthy gene expression)

&lt;img src="lect04-statreview_files/figure-html/unnamed-chunk-22-1.png" width="324" /&gt;

---


# Motivating example (cancer vs healthy gene expression)

&lt;img src="lect04-statreview_files/figure-html/unnamed-chunk-23-1.png" width="648" /&gt;

---

# Motivating example (cancer vs healthy gene expression)

&lt;img src="lect04-statreview_files/figure-html/unnamed-chunk-24-1.png" width="972" /&gt;

--

* Is there a **significant** difference between the two means?

--

* All three samples drawn from **iid** Normal distributions with equal variance and `\(\mu_Z-\mu_Y=1\)`

---

# Is there a **significant** difference between the two means?

&lt;img src="lect04-statreview_files/figure-html/unnamed-chunk-25-1.png" width="972" /&gt;

--

Mean difference needs to be put into context of the *spread (standard deviation)* and *sample size*. This shouldn't be surprising when we recall the formula for the **sampling distribution of the mean:** `\(\normalsize \bar{X} \sim N(\mu, \frac{\sigma^2}{n})\)`

---

# 2 sample t-statistic

* **2-sample t-statistic:** measures difference in means, adjusted for spread/standard deviation:

`$$\normalsize t=\frac{\bar{z}-\bar{y}}{SE_{\bar{z}-\bar{y}}}$$`
    e.g. for `\(z_1, z_2, ..., z_n\)` expression measurements in healthy samples and `\(y_1, y_2, ..., y_m\)` cancer samples

* From the theory, we know the distribution of our test statistic, if we are willing to make some assumptions



---

# 2 sample t-test

* If we assume:

  - `\(\bar{Z}\)` and `\(\bar{Y}\)` are normally distributed
  
  - `\(Z\)` and `\(Y\)` have equal variance

* Then the standard error estimate for the difference in means is:

`$$SE_{\bar{z}-\bar{y}} = s_p \sqrt{\frac{1}{n} + \frac{1}{m}} \text{ , where  } s_p^2 = \frac{s^2_z + s^2_y}{(n-1) + (m-1)}$$`
  
* And our t-statistic follows a t distribution with m+n-2 degrees of freedom `$$t \sim t_{n+m-2}$$`

* (Alternative formulations for unequal variance setting)

---

# t distribution

.pull-left[
&lt;img src="img/tdist.png" width="600" style="display: block; margin: auto;" /&gt;
]

.pull-right[
*  statistic value tells us how extreme our observed data is relative to the null 

* obtain **p-value** by computing area to the left and/or right of the t statistic (one-sided vs two-sided)

]

---

# Summary

* Random variables are variables that have a probability distribution 

* Any statistic of sampled data is a RV, and hence has an associated probability distribution

* The CLT gives us the sampling distribution of the mean 

* Hypothesis testing gives us a framework to assess a statistical hypothesis under the null
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
