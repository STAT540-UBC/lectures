---
title: "Statistical Methods for single cell data analysis (focusing on technology)"
author: "Yongjin Park, UBC Path + Stat, BC Cancer"
date: "`r format(Sys.time(), '%d %B %Y')`"
mainfont: Roboto
classoption: "aspectratio=169"
fontsize: 12pt
output:
    powerpoint_presentation:
        reference_doc: "_template.pptx"
    html_document:
        self_contained: true
    beamer_presentation:
        theme: Madrid
        keep_md: true
        keep_tex: true
        latex_engine: xelatex
        slide_level: 2
header-includes:
  - \usepackage{cancel}
  - \definecolor{UBCblue}{rgb}{0.04706, 0.13725, 0.26667}
  - \usecolortheme[named=UBCblue]{structure}
  - \setbeamertemplate{frametitle}{\color{UBCblue}\bfseries\insertframetitle\par\vskip-6pt\hrulefill}
  - \setbeamercolor{title in head/foot}{bg=white,fg=gray}
  - \setbeamercolor{section in head/foot}{bg=white,fg=gray}
  - \setbeamercolor{author in head/foot}{bg=white,fg=gray}
  - \setbeamercolor{date in head/foot}{bg=white,fg=gray}
  - \setbeamertemplate{page number in head/foot}{}
  - \setbeamertemplate{frame numbering}[none]
  - \setbeamercolor{alerted text}{bg=yellow}
  - \AtBeginSection[]{\begin{frame}\frametitle{Today's lecture}{\Large\tableofcontents[currentsection]}\end{frame}}
  - |
    \makeatletter
    \def\ps@titlepage{%
      \setbeamertemplate{footline}{}
    }
    \addtobeamertemplate{title page}{\thispagestyle{titlepage}}{}
    \makeatother
    \include{toc}
---


```{r setup, include=FALSE}
setwd("~/work/teaching/stat540_lectures/lect16-scRNA-technology/")
library(data.table)
library(tidyverse)
library(patchwork)
library(matrixTests)
library(mmutilR)
source("Setup.R")
fig.dir <- "../Fig/scRNA/"
dir.create(fig.dir, showWarnings=FALSE)
setup.env(fig.dir)
dir.create("../data", showWarnings=FALSE)
library(extrafont)
library(xkcd)
extrafont::font_import(pattern="[X/x]kcd", prompt=F)
extrafont::loadfonts()
theme_set(theme_xkcd() +
          theme(title = element_text(size=10),
                legend.background = element_blank())
          )
```

## Next Lectures -- single cell genomics

* Lecture 16: technology and data normalization

* Lecture 17: unsupervised learning -1

* Lecture 18: unsupervised learning -2




# Single-cell sequencing technology

## Droplet-based single-cell sequencing technology

\vfill
\centerline{\includegraphics[width=\textwidth]{img/dropseq_technology/dropseq1.pdf}}
\vfill
\flushright
\tiny
Macosko \textit{et al.}, \emph{Cell} (2015)

## Drop-seq idea 1: Capture one cell with a microbead in a droplet

\vfill
\centerline{\includegraphics[height=.7\textheight]{img/dropseq_technology/dropseq2.pdf}}
\vfill
\flushright
\tiny
Macosko \textit{et al.}, \emph{Cell} (2015)

## Drop-seq idea 2: Massively-parallel sequencing followed by cell-specific barcoding

\vfill
\centerline{\includegraphics[width=\textwidth]{img/dropseq_technology/dropseq3.pdf}}
\vfill
\flushright
\tiny
Macosko \textit{et al.}, \emph{Cell} (2015)


## Drop-seq idea 3: How do we keep track of mRNA short reads' membership to a certain droplet?

\vfill
\centerline{\includegraphics[width=.9\textwidth]{img/dropseq_technology/dropseq4.pdf}}
\vfill
\flushright
\tiny
Macosko \textit{et al.}, \emph{Cell} (2015)

## How do we construct millions of unique barcodes? Use DNA as a hashing function!

\vfill

\only<1>{
\includegraphics[height=.5\textheight]{img/dropseq_technology/dropseq5.pdf}
}

\only<2>{
\includegraphics[height=.5\textheight]{img/dropseq_technology/dropseq6.pdf}
}

\only<3>{
\includegraphics[height=.55\textheight]{img/dropseq_technology/dropseq7.pdf}
}

\vfill

\flushright
\tiny
Macosko \textit{et al.}, \emph{Cell} (2015)

## The lengths of barcode sequences determine data dimensionality

\vfill

:::::: {.columns}
::: {.column width=.75}

\centerline{\includegraphics[width=\linewidth]{img/dropseq_technology/dropseq8.pdf}}

:::
::: {.column width=.2}

\begin{block}{}
Technically, we can build up to a `r num.int(4^8)` $\times$ `r num.int(4^12)`, gene $\times$ cell expression matrix in one single-cell RNA-seq experiment.
\end{block}

:::
::::::

\vfill
\tiny
Macosko \textit{et al.}, \emph{Cell} (2015)


## Droplet-based single-cell sequencing pipeline

\vfill
\centerline{\includegraphics[width=.8\textwidth]{img/dropseq_technology/dropseq9.pdf}}
\vfill
\flushright
\tiny
Macosko \textit{et al.}, \emph{Cell} (2015)


## Overview of single-cell data analysis

\centerline{\includegraphics[width=\textwidth]{img/scRNA_overview/Karchenko_1.pdf}}

\vfill

\flushright
\tiny
Karchenko, \emph{Nature Methods} (2021)

## Overview of single-cell data analysis cont'd

\centerline{\includegraphics[width=\textwidth]{img/scRNA_overview/Karchenko_2.pdf}}

\vfill

\flushright
\tiny
Karchenko, \emph{Nature Methods} (2021)


# Basic quality control

## Example: human pancreatic cells (Muraro _et al._ 2016)

:::::: {.columns}
::: {.column width=.4}

\centerline{\includegraphics[width=.9\linewidth]{img/Muraro_GSE85241.png}}

:::
::: {.column width=.6}


```{r read_raw_data_gse85241}
raw.data <- fileset.list("../data/GSE85241/GSE85241")
.info <- rcpp_mmutil_info(raw.data$mtx)
cell.index <- rcpp_mmutil_read_index(raw.data$idx)
X <- read.sparse(raw.data$mtx)
.rows <- fread(raw.data$row, header=F)
.rows[, c("gene","chr") := tstrsplit(`V1`, split="[_]+", keep=1:2)]
rownames(X) <- .rows$gene
colnames(X) <- readLines(raw.data$col)
```

It's a sparse matrix...

${}$

```{r echo = T}
X[1:10, 1:5]
```

\begin{itemize}
\item<1> What are the rows and columns?
\item<2-> rows: `r num.int(nrow(X))` genes
\item<2-> columns: `r num.int(ncol(X))` cells
\end{itemize}

:::
::::::


## Computing row-wise (genes) statistics

```{r}
.stat <- rcpp_mmutil_compute_scores(raw.data$mtx, raw.data$row, raw.data$col)
row.scores <- setDT(.stat$row)
col.scores <- setDT(.stat$col)

row.scores[, c("gene","chr") := tstrsplit(`name`, split="[_]+", keep=1:2)]
row.scores[, `name` := NULL]
```

* Statistics across cells (columns) per gene (row)

${}$

```{r echo = T}
head(row.scores)[, .(`gene`, `nnz`, `mean`, `sd`, `cv`, `sum`, `sum.sq`)]
```

* `nnz`: number of non-zero elements

* `mean`: average; `sd`: standard deviation; `cv`: coefficient of variation

* `sum`: sum of all the counts; `sum.sq`: sum of all the squared counts


## Computing column-wise (cells) statistics

* Statistics across genes (rows) per cell (column)

${}$

```{r echo = T}
head(col.scores)
```

* `nnz`: number of non-zero elements

* `mean`: average; `sd`: standard deviation; `cv`: coefficient of variation

* `sum`: sum of all the counts; `sum.sq`: sum of all the squared counts

## QC #1. Column filtering to ensure the quality of cells

:::::: {.columns}
::: {.column width=.3}

::: {.block}
### Why cell Q/C?

* \onslide<1->{Too {\color{blue}few} genes expressed:} \onslide<2->{dying, {\color{blue}ambient cells}, too many technical drop out}

* \onslide<1->{Unusual {\color{magenta}high} expressions}: \onslide<3>{{\color{magenta}doublets}, high ribosomal genes (background activities), disequilibrium}

:::

:::
::: {.column width=.7}

```{r fig.width=3.5, fig.height=3.1, only.plot="2"}
hist(log10(col.scores$nnz), breaks=20, xlab="log10(NNZ)", main="")
```

```{r fig.width=3.5, fig.height=3.1, only.plot="3"}
hist(log10(col.scores$sum), breaks=20, xlab="log10(sum)", main="")
```

```{r fig.width=3.5, fig.height=3.1, only.plot="4"}
hist(log10(col.scores$mean), breaks=20, xlab="log10(mean)", main="")
```

```{r fig.width=3.5, fig.height=3.1, only.plot="5"}
hist(log10(col.scores$sd), breaks=20, xlab="log10(sd)", main="")
```

```{r fig.width=3.5, fig.height=3.1, only.plot="6"}
hist(log10(col.scores$cv), breaks=20, xlab="log10(CV)", main="")
```

:::
::::::

## Mean-variance relationship of cell statistics

```{r fig.width=5, fig.height=3, only.plot="1"}
ggplot(col.scores, aes(log10(mean), log10(sd))) +
    ggrastr::rasterise(geom_point(stroke = 0), dpi=300) +
    geom_smooth(method="lm", se=F, lty=2, col=2)
```

```{r fig.width=5, fig.height=3, only.plot="2"}
ggplot(col.scores, aes(log10(mean), log10(cv))) +
    ggrastr::rasterise(geom_point(stroke = 0), dpi=300)
```

## Robust Q/C by putting multiple scores together

```{r fig.width=5, fig.height=3, only.plot="1"}
ggplot(col.scores, aes(log10(1+nnz), log10(1+cv))) +
    ggrastr::rasterise(geom_point(stroke = 0), dpi=300) +
    scale_x_continuous("number of non-zeros", labels=function(x) 10^x) +
    scale_y_continuous("coefficient of variation", labels=function(x) round(10^x, 2))
```

```{r fig.width=5, fig.height=3, only.plot="2"}
ggplot(col.scores, aes(log10(1+nnz), log10(1+cv))) +
    stat_density_2d(geom = "raster",
                    aes(fill = after_stat(sqrt(density))),
                    show.legend=F,
                    contour=F) +
    geom_vline(xintercept = log10(1000), color="white", lty=2) +
    geom_hline(yintercept = log10(10), color="white", lty=2) +
    geom_hline(yintercept = log10(25), color="white", lty=2) +
    scale_x_continuous("number of non-zeros", labels=function(x) 10^x) +
    scale_y_continuous("coefficient of variation", labels=function(x) round(10^x, 2),
                       breaks = log10(c(1, 10, 25, 50))) +
    scale_fill_viridis_c()
```

## Can we systematically dissect "axes of variation?"

Unsupervised Learning Methods considered in single-cell analysis:

\large

\begin{enumerate}
\item<1-> \textbf{Principal} Component Analysis (PCA)
\item<2-> (more general) matrix factorization (lect 17)
\item<2-> (Variational) autoencoder (lect 18)
\item<3> GPT (Generative Pretrained Transformer; \textit{ask ChatGPT})
\end{enumerate}

##

\huge

What is Principal Component Analysis?

## PCA: Consider this multivariate linear model

\only<1>{
$$\left(\begin{array}{l}
X_{1}\\
X_{2}\\
\vdots \\
X_{p}\\
\end{array}
\right) =
\left(
\begin{array}{l l l}
U_{11} & \cdots & U_{1k}\\
U_{21} & \cdots & U_{2k}\\
\vdots & \vdots & \vdots \\
U_{p1} & \cdots & U_{pk}\\
\end{array}
\right)
\left(
\begin{array}{l}
V_{1}\\
\vdots\\
V_{k}
\end{array}
\right)
+
\left(
\begin{array}{l}
\epsilon_{1}\\
\epsilon_{2}\\
\vdots\\
\epsilon_{p}
\end{array}
\right)$$

or

$\mathbf{x} = U \mathbf{v} + \epsilon$}

\only<2>{For many columns, $X = U V + E$, or
$$\left(\begin{array}{l l l}
X_{11} & \cdots & X_{1n}\\
X_{21} & \cdots & X_{2n}\\
\vdots & & \vdots \\
X_{p1} & \cdots & X_{pn}\\
\end{array}
\right) =
\left(
\begin{array}{l l l}
U_{11} & \cdots & U_{1k}\\
U_{21} & \cdots & U_{2k}\\
\vdots & \vdots & \vdots \\
U_{p1} & \cdots & U_{pk}\\
\end{array}
\right)
\left(
\begin{array}{l l l}
V_{11} & \cdots & V_{1n}\\
\vdots & & \vdots \\
V_{k1} & \cdots & V_{kn} \\
\end{array}
\right)
+ \cdots$$}

* If we **knew** $U$, we would be able to solve weights $V$.

* And vice versa...

## PCA: What is a projection matrix?

Suppose we knew $V$. For each gene (row) $g$, we solve the following regression.

\begin{eqnarray*}
\mathbf{x}_{g}^{\top} &\sim& V \mathbf{u}_{g}^{\top} + \epsilon I\\
\onslide<2->{\left(\begin{array}{l}
X_{g1} \\
\vdots \\
X_{gn}
\end{array}\right)
&\sim&
\left(\begin{array}{l l l}
V_{11} & \cdots & V_{1k} \\
\vdots & \vdots & \vdots \\
V_{n1} & \cdots & V_{nk} \\
\end{array}\right)
\left(\begin{array}{l}
U_{g1} \\
\vdots \\
U_{gk}
\end{array}\right) +
\left(\begin{array}{l}
\epsilon \\
\vdots \\
\epsilon
\end{array}\right)}
\end{eqnarray*}

Least square solution:

\only<1-2>{$$\underset{\mathbf{u}_{g}}{\min} \, \|\mathbf{x}_{g}^{\top} - V \mathbf{u}_{g}^{\top}\|$$}

\only<3>{$$\hat{\mathbf{u}}_{g}^{\top} = (V^{\top}V)^{-1} V^{\top}\mathbf{x}_{g}^{\top}$$}

## PCA: What is a projection matrix?

Plugging

$$\hat{\mathbf{u}}_{g}^{\top} = (V^{\top}V)^{-1} V^{\top}\mathbf{x}_{g}^{\top}$$

into the following prediction model:

\begin{eqnarray*}
\hat{\mathbf{x}}_{g}^{\top} &=& V \hat{\mathbf{u}}_{g}^{\top} \\
\only<2>{
&=& V (V^{\top}V)^{-1} V^{\top}\mathbf{x}_{g}^{\top} \\ }
\only<3>{
\hat{\mathbf{x}}_{g} &=& \mathbf{x}_{g} \underbrace{V (V^{\top}V)^{-1} V^{\top}}_{\textsf{$n{\times}n$ projection matrix}} }
\end{eqnarray*}

## PCA: Projection can be done on the other side

A multivariate regression model for sample (column) $j$:

$$\mathbf{x}_{j} \sim U \mathbf{v}_{j} + \epsilon$$

The least-square solution for the $V$:

$$\hat{\mathbf{v}}_{j} = (U^{\top}U)^{-1} U^{\top} \mathbf{x}_{j}$$

Then we have

$$\hat{\mathbf{x}}_{j} = \underbrace{U (U^{\top}U)^{-1} U^{\top}}_{\textsf{\color{red} $p{\times}p$ projection matrix}} \mathbf{x}_{j}$$

## PCA = maximizing total variance of the projected data

::: {.block}

### Constrained optimization

Given the rank-1 factorization
$$\hat{X} = \mathbf{u} \mathbf{v}^{\top}$$
and
the least square solution $\hat{\mathbf{v}}$
$$\hat{\mathbf{v}}^{\top} = \frac{\mathbf{u}^{\top}}{\mathbf{u}^{\top}\mathbf{u}} X$$

We want to maximize the variance of this projected vector $\mathbf{v}$
$$\hat{\mathbf{v}}^{\top} \hat{\mathbf{v}}
= \frac{\mathbf{u}^{\top}}{\mathbf{u}^{\top}\mathbf{u}} X
X^{\top} \frac{\mathbf{u}}{\mathbf{u}^{\top}\mathbf{u}}
= \mathbf{u}^{\top}XX^{\top}\mathbf{u}$$
where we assume $\mathbf{u}$ is a unit vector.

:::

## PCA is an eigen value problem

:::::: {.columns}
::: {.column width=.45}

::: {.block}
### PCA

Letting the covariance matrix $\hat{\Sigma} = XX^{\top}/(p-1)$, we want to find a unit vector $\mathbf{v}$ by

$$\max \mathbf{v}^{\top} \hat{\Sigma}\mathbf{v}$$

subject to $\mathbf{v}^{\top}\mathbf{v} = 1$.

:::

:::
::: {.column width=.45}

::: {.block}
### Eigen value problem

Given the covariance matrix $\hat{\Sigma}$, we can resolve an eigen-value $\lambda$ and the corresponding eigen-vector $\mathbf{v}$ such that

$$\hat{\Sigma} \mathbf{v} = \lambda \mathbf{v}$$

:::

:::
::::::


## SVD: another equivalent method for PCA

:::::: {.columns}
::: {.column width=.45}

::: {.block}
### Singular Value Decomposition

SVD identifies three matrices of $X$:

$$X = U D V^{\top}$$

where both $U$ and $V$ vectors are orthonormal,
namely,

- $U^{\top}U = I$, $\mathbf{u}_{k}^{\top}\mathbf{u}_{k}=1$ for all $k$,

- $V^{\top}V = I$, $\mathbf{v}_{k}^{\top}\mathbf{v}_{k} = 1$ for all $k$.

:::

:::
::: {.column width=.45}

::: {.block}
### Covariance by SVD

Covariance across the columns (samples)

$$X^{\top}X/p = V D^{2} V^{\top}/p$$

Covariance across the rows (genes)

$$XX^{\top}/n = U D^{2} U^{\top}/n$$

:::

*Remark*: standardized matrix

:::
::::::

## SVD: another equivalent method for PCA

We can confirm the equivalent relations by multiplying singular vectors to the covariance matrix:

\small

\begin{eqnarray*}
\onslide<1->{
	\underbrace{\frac{\left( X^{\top}X \right)}{p-1}}_{\textsf{\color{blue} sample covariance}} \mathbf{v}_{1}
	&\propto&
	 \left(\mathbf{v}_{1}, \mathbf{v}_{2},\ldots, \mathbf{v}_{k}\right)
      \left(
      \begin{array}{l l l l}
        D_{1}^{2} & 0 & \ldots & \ldots \\
        0 & D_{2}^{2} & 0 & \ldots \\
        0 & \ldots & \ddots & 0 \\
        0 & \ldots & 0 & D_{k}^{2} \\
      \end{array} \right)
  \left(
  \begin{array}{l}
    \mathbf{v}_{1}^{\top}\\
    \mathbf{v}_{2}^{\top}\\
    \vdots\\
    \mathbf{v}_{k}^{\top}
  \end{array}
  \right)
  \mathbf{v}_{1} \\
	}
	\only<2>{
  &\propto&
       \left(\mathbf{v}_{1}, \mathbf{v}_{2},\ldots, \mathbf{v}_{k}\right)
      \left(
      \begin{array}{l l l l}
        D_{1}^{2} & 0 & \ldots & \ldots \\
        0 & D_{2}^{2} & 0 & \ldots \\
        0 & \ldots & \ddots & 0 \\
        0 & \ldots & 0 & D_{k}^{2} \\
      \end{array} \right)
  \left(
  \begin{array}{l}
    1 \\
    0 \\
    \vdots\\
    0
  \end{array}
  \right) \\
  }
\only<3>{
  &\propto&
       \left(\mathbf{v}_{1}, \mathbf{v}_{2},\ldots, \mathbf{v}_{k}\right)
      \left(
      \begin{array}{l}
        D_{1}^{2} \\
        0 \\
        0 \\
        0 \\
      \end{array} \right)}
	  \only<4>{
	  &=&
  \underbrace{\frac{D_{1}^{2}}{p-1}}_{\textsf{\color{red}eigenvalue}}
  \underbrace{\mathbf{v}_{1}}_{\textsf{\color{red}eigenvector}}
  }
\end{eqnarray*}

## Learning PCA by singular value decomposition

:::::: {.columns}
::: {.column width=.5}

```{r echo = T, size="large"}
library(rsvd)
.svd <- rsvd(X, k=3)
names(.svd)
dim(.svd$v)
```

* `rsvd` provides faster SVD

:::
::: {.column width=.5}

```{r fig.width=3, fig.height=2.5}
ggplot(as.data.frame(.svd$v), aes(V1,V2)) +
    ggrastr::rasterise(geom_point(stroke=0),dpi=300) +
    xlab("PC1") + ylab("PC2")
```

:::
::::::


## Learning PCA by singular value decomposition - 2

:::::: {.columns}
::: {.column width=.5}

In single-cell data analysis, it is useful to transform the raw count values to more Gaussian-like ones.

${}$

```{r echo = T}
X.n <- apply(X, 2, function(x)x/sum(x))
X.n <- X.n / 1e4
log.x <- apply(log1p(X.n),2,scale)
.svd <- rsvd(log.x, k=3)
names(.svd)
dim(.svd$v)
```

:::
::: {.column width=.5}

```{r fig.width=3, fig.height=2.5}
ggplot(as.data.frame(.svd$v), aes(V1,V2)) +
    ggrastr::rasterise(geom_point(stroke=0),dpi=300) +
    xlab("PC1") + ylab("PC2")
```

:::
::::::


## Learning PCA by singular value decomposition - 3

:::::: {.columns}
::: {.column width=.5}

We can also add pseudo-count to make the SVD results smoother..

${}$

```{r}
.svd <- rcpp_mmutil_svd(raw.data$mtx, RANK=3, NUM_THREADS=16)
```

```{r echo = T}
names(.svd)
dim(.svd$V)
```

:::
::: {.column width=.5}

```{r fig.width=3, fig.height=2.5}
ggplot(as.data.frame(.svd$V), aes(V1,V2)) +
    ggrastr::rasterise(geom_point(stroke=0),dpi=300) +
    xlab("PC1") + ylab("PC2")
```

:::
::::::

```{r}
.dt <- cbind(data.table(.svd$V), col.scores)
```

## PCA to confirm/remvoe low-quality cells

```{r fig.width=5, fig.height=3, only.plot="1"}
ggplot(.dt, aes(V1, V2, fill = `nnz`)) +
    ggrastr::rasterise(geom_point(pch=21, stroke=.1),dpi=300) +
    xlab("PC1") + ylab("PC2") +
    scale_fill_distiller(palette = "RdBu", trans="log10")
```

```{r fig.width=5, fig.height=3, only.plot="2"}
ggplot(.dt, aes(V1, V2, fill = `cv`)) +
    ggrastr::rasterise(geom_point(pch=21, stroke=.1),dpi=300) +
    xlab("PC1") + ylab("PC2") +
    scale_fill_distiller(palette = "RdBu", trans="log10")
```

```{r fig.width=5, fig.height=3, only.plot="3"}
ggplot(.dt, aes(V2, V3, fill = `nnz`)) +
    ggrastr::rasterise(geom_point(pch=21, stroke=.1),dpi=300) +
    xlab("PC2") + ylab("PC3") +
    scale_fill_distiller(palette = "RdBu", trans="log10")
```

```{r fig.width=5, fig.height=3, only.plot="4"}
ggplot(.dt, aes(V2, V3, fill = `cv`)) +
    ggrastr::rasterise(geom_point(pch=21, stroke=.1),dpi=300) +
    xlab("PC2") + ylab("PC3") +
    scale_fill_distiller(palette = "RdBu", trans="log10")
```

## After removing "dying" cells ...

```{r}
.dt.qc <- .dt[nnz > 1000]
```

```{r fig.width=5, fig.height=3, only.plot="1"}
ggplot(.dt.qc, aes(V1, V2, fill = `nnz`)) +
    ggrastr::rasterise(geom_point(pch=21, stroke=.1),dpi=300) +
    xlab("PC1") + ylab("PC2") +
    scale_fill_distiller(palette = "RdBu", trans="log10")
```

```{r fig.width=5, fig.height=3, only.plot="2"}
ggplot(.dt.qc, aes(V1, V2, fill = `cv`)) +
    ggrastr::rasterise(geom_point(pch=21, stroke=.1),dpi=300) +
    xlab("PC1") + ylab("PC2") +
    scale_fill_distiller(palette = "RdBu", trans="log10")
```

```{r include = FALSE}
.qc.hdr <- str_c(fig.dir, "/qc_data")
qc.data <- fileset.list(.qc.hdr)

if.needed(qc.data,
{
    qc.data <-
        rcpp_mmutil_copy_selected_columns(raw.data$mtx,
                                          raw.data$row,
                                          raw.data$col,
                                          .dt.qc$name,
                                          .qc.hdr)
})
```

##

\huge

Should we normalize single-cell gene expression vectors?

## Recall: Should we always normalize within each sample?

What could go wrong with that?

Consider the following example:

\begin{center}
\includegraphics[width=.7\textwidth]{img/SequenceSpace.pdf}
\end{center}

```{r include=F}
genes <- c("Red","Blue","Green")
n.x <- 10
x.prob <- c(3, 3, 4) %>% (function(p)p/sum(p))
n.y <- 22
y.prob <- c(3, 3, 16) %>% (function(p)p/sum(p))
```

```{r include = F}
build.rgb.tab <- function(xx, yy, .factors = c("Red","Green","Blue")){
    ret <- data.frame(x = c(xx, yy),
                      s = c(rep("sample.1", length(xx)),
                            rep("sample.2", length(yy))))
    ret$x <- factor(ret$x, .factors)
    return(ret)
}

show.rgb.tab <- function(xx, yy){
    ggplot(build.rgb.tab(xx, yy), aes(x, fill = x)) +
        xlab("genes") +
        facet_grid(s ~ .) +
        scale_fill_manual(values=c("red","green","blue"), guide="none") +
        geom_bar(stat="count", linewidth=.1, color="black") +
        coord_flip()
}

take.rgb.tab <- function(xx, yy){
    ret <- build.rgb.tab(xx, yy) %>%
        as.data.table() %>%
        dcast(x ~ s, fun.aggregate = length) %>%
        as.data.frame() %>%
        column_to_rownames("x")
    return(ret)
}
```

## What if we sequence "deep" enough?

```{r include=F}
x <- unlist(sapply(1:length(genes), function(g) rep(genes[g], n.x * x.prob[g])))

y <- unlist(sapply(1:length(genes), function(g) rep(genes[g], n.y * y.prob[g])))
```

:::::: {.columns}
::: {.column width=.5}

```{r fig.width=2, fig.height=2}
show.rgb.tab(x, y)
```

:::
::: {.column width=.5}

```{r size="large"}
take.rgb.tab(x, y)
```

:::
::::::

## What if we sequence *less* than the actual counts?

```{r include=F}
set.seed(1)
x <- sample(genes, 10, x.prob, replace=T)
y <- sample(genes, 10, y.prob, replace=T)
```

:::::: {.columns}
::: {.column width=.5}

```{r fig.width=2, fig.height=2}
show.rgb.tab(x, y)
```

:::
::: {.column width=.5}

```{r size="large"}
take.rgb.tab(x, y)
```

::: {.block}
### Question
- Is it okay to normalize to have both samples have the same sequence depth?

- Are we comparing the gene "red" with the gene "green" or vice versa?
:::

:::
::::::

## QC 2. Can we select informative rows/genes?

```{r fig.width=6, fig.height=2.5}
.dt <- melt(row.scores, id.vars = c("gene","chr"))

ggplot(.dt, aes(x = log1p(value), y = ..count.., fill = variable)) +
    facet_wrap(~variable, scales="free", nrow=2) +
    geom_density(show.legend=FALSE) +
    scale_fill_brewer(palette = "Set3")
```

\small

\only<1>{`nnz`: number of non-zero elements, `sd`: standard deviation, `cv`: coefficient of variation (`sd`/`mean`), `sum.sq`: sum of squares.}

\only<2>{We may select only highly variable genes for the subsequent analysis}

## Mean-variance relationship of genes

```{r fig.width=5, fig.height=3, only.plot="1"}
ggplot(row.scores, aes(log10(mean), log10(sd))) +
    ggrastr::rasterise(geom_point(stroke = 0), dpi=300) +
    geom_smooth(method="lm", se=F, lty=2, col=2)
```

```{r fig.width=5, fig.height=3, only.plot="2"}
ggplot(row.scores, aes(log10(mean), log10(cv))) +
    ggrastr::rasterise(geom_point(stroke = 0), dpi=300)
```

## Gene Q/C: Any obvious cutoff?

```{r fig.width=5, fig.height=3, only.plot="1"}
ggplot(row.scores, aes(log10(1+nnz), log10(1+cv))) +
    ggrastr::rasterise(geom_point(stroke = 0), dpi=300) +
    scale_x_continuous("number of non-zeros", labels=function(x) 10^x) +
    scale_y_continuous("coefficient of variation", labels=function(x) round(10^x, 2))
```

```{r fig.width=5, fig.height=3, only.plot="2"}
ggplot(row.scores, aes(log10(1+nnz), log10(1+cv))) +
    stat_density_2d(geom = "raster",
                    aes(fill = after_stat(sqrt(density))),
                    show.legend=F,
                    contour=F) +
    scale_x_continuous("number of non-zeros", labels=function(x) 10^x) +
    scale_y_continuous("coefficient of variation", labels=function(x) round(10^x, 2),
                       breaks = log10(c(1, 10, 25, 50))) +
    scale_fill_viridis_c()
```

## Can we use the count data as they are?

\large

* In bulk RNA-seq, we transformed the count data to adjust size factors that vary across samples.

* In single-cell data analysis, we generally care a lot about zero values, not knowing whether they come from actual "no expression" or technical drop-out.

* Should we take some sort of `voom` transformation?

# Additional Q/C tools

## It's useful to show high-dimensional data in 2D

\vfill

:::::: {.columns}
::: {.column width=.45}

\onslide<1->{
\includegraphics[width=.8\linewidth]{img/scRNA_data_cartoon.pdf}
}

:::
::: {.column width=.45}

\onslide<2->{
\includegraphics[width=.7\linewidth]{img/scRNA_data_cartoon_tSNE.pdf}
}

:::
::::::

\vfill

\onslide<2->{
\tiny
\texttt{tSNE}: t-distributed Stochastic Neighbourhood Embedding (Van der Maaten \& Hinton, 2008).
}

## Warning: Don't over interpret 2D embedding

\vfill
\centerline{\includegraphics[width=\textwidth]{img/scRNA_ETM/tSNE_over.pdf}}
\vfill

\tiny

Kobak and Berens, \emph{Nature Biotech} (2021)

## Don't over interpret 2D embedding results

Check out these papers:

* [Initialization is critical for preserving global data structure in both t-SNE and UMAP](https://www.nature.com/articles/s41587-020-00809-z)

* [The art of using t-SNE for single-cell transcriptomics](https://www.nature.com/articles/s41467-019-13056-x)

* [Dimensionality reduction for visualizing single-cell data using UMAP](https://www.nature.com/articles/nbt.4314)

# Doublet detection in single-cell data

## What if we capture more than one cell in a droplet?

\vfill
\centerline{\includegraphics[width=.8\textwidth]{img/dropseq_technology/dropseq9.pdf}}
\vfill

\flushright
\tiny
Macosko \textit{et al.}, \emph{Cell} (2015)

## What is a doublet in single-cell data?

**Biological/technical definition:**

- One or more cells captured (usually at most two cells by chance)

- Thus, multiple cells accidental share the same cell barcode sequence

- Not so clear in general... since we missed the chance to assign
  different tags to different cells encapsulated in the same droplet.

**Statistical definition**:

- If we could find marker genes of multiple cell types are
  simultaneously expressed...

- An unvetted approach: Find ambiguous/intermediate coordinates in
  PCA/tSNE/UMAP (after removing ambient cells).


## Can we create artificial doublets?

\large

A straightforward definition (used in [`DoubletFinder`](https://github.com/chris-mcginnis-ucsf/DoubletFinder)):

For each cell $i$:

* Take some other $j$ by random selection

* Create an artificial doublet

$$\tilde{\mathbf{x}} \gets \frac{1}{2}(\mathbf{x}_{i} + \mathbf{x}_{j})$$

Some thought questions:

* Doublets within the same cell type?

* Doublets between the different cell types?


```{r echo = FALSE}
doublet.hdr <- str_c(fig.dir, "/qc_doublet")
doublet.data <- fileset.list(doublet.hdr)

.writev <- function(v, ...) {
    as.data.table(v) %>%
        fwrite(..., col.names=FALSE, row.names=FALSE)
}
```

```{r}
if.needed(doublet.data,
{
    set.seed(1)
  .rows <- readLines(qc.data$row)
  .cols <- readLines(qc.data$col)
  .rnd <- sample(length(.cols))

  ## doublet mixology
  X <- read.sparse(qc.data$mtx)
  X.rnd <- read.sparse(qc.data$mtx, .rnd)
  X.dbl <- round((X + X.rnd)/2)

  ## give distinctive names
  .dbl <- str_c(.cols,.cols[.rnd],sep="+")
  .writev(c(.cols, .dbl),file=doublet.data$col)
  .writev(.rows, file=doublet.data$row)

  rcpp_mmutil_write_mtx(cbind(X, X.dbl),
              doublet.data$mtx)
})
```

```{r}
.svd <- rcpp_mmutil_svd(doublet.data$mtx, 10, NUM_THREADS=16)
.stat <- rcpp_mmutil_compute_scores(doublet.data$mtx, doublet.data$row, doublet.data$col)
row.scores <- setDT(.stat$row)
col.scores <- setDT(.stat$col)
```


## k-Nearest Neighbour classification for doublet detection

\vfill

\centerline{\includegraphics[width=.9\textwidth]{img/doublet/doublet_knn.pdf}}

\vfill
\tiny

## Can you tell the difference by a quick visual inspection?

```{r fig.width=5, fig.height=2.5, echo = FALSE, only.plot=1}
.dt <- cbind(data.table(.svd$V), col.scores)
.dt[, dbl := "singlet"]
.dt[str_detect(`name`, "[+]+"), dbl := "doublet"]

ggplot(.dt, aes(V1, V2, fill=dbl)) +
    ggrastr::rasterise(geom_point(stroke=.1, size=1, pch=21, alpha=.5),dpi=300) +
    scale_fill_brewer("", palette = "Set1") +
    xlab("PC1") + ylab("PC2")
```

```{r fig.width=5, fig.height=2.5, echo = FALSE, only.plot=2}
.dt <- cbind(data.table(.svd$V), col.scores)
.dt[, dbl := "singlet"]
.dt[str_detect(`name`, "[+]+"), dbl := "doublet"]

ggplot(.dt, aes(V2, V3, fill=dbl)) +
    ggrastr::rasterise(geom_point(stroke=.1, size=1, pch=21, alpha=.5),dpi=300) +
    scale_fill_brewer("", palette = "Set1") +
    xlab("PC1") + ylab("PC2")
```

```{r fig.width=5, fig.height=2.5, echo = FALSE, only.plot=3}
.dt <- cbind(data.table(.svd$V), col.scores)
.dt[, dbl := "singlet"]
.dt[str_detect(`name`, "[+]+"), dbl := "doublet"]

ggplot(.dt, aes(V3, V4, fill=dbl)) +
    ggrastr::rasterise(geom_point(stroke=.1, size=1, pch=21, alpha=.5),dpi=300) +
    scale_fill_brewer("", palette = "Set1") +
    xlab("PC1") + ylab("PC2")
```

##

\Large

> Can we design a classifier to distinguish singlets vs. doublets?

## k-Nearest Neighbour classification for doublet detection

\large

* Step 1. Create artificial doublets, $\tilde{\mathbf{x}}$

* Step 2. Mix them with the original cells and perform PCA

* Step 3. Find nearest neighbours of the original cells (using \#PC=10)

* Step 4. Count the number of doublets in the neighbourhood

```{r}
.knn.file <- str_c(fig.dir, "/doublet_knn.RDS")
if.needed(.knn.file,
{
  .out <-
    rcpp_mmutil_match_files(raw.data$mtx,
                            doublet.data$mtx,
                            knn = 50,
                            RANK = 10,
                            NUM_THREADS = 15)
  knn.dt <- setDT(.out)
  saveRDS(knn.dt, .knn.file)
})
```

## k-Nearest Neighbour classification of doublets

:::::: {.columns}
::: {.column width=.48}

* **Q: How many of my neighbours are indeed a doublet?**

```{r}
.src.cells <-
  fread(raw.data$col, col.names="cell", header=FALSE) %>%
  mutate(src.index = 1:n()) %>%
  as.data.table

.tgt.cells <-
  fread(doublet.data$col, col.names="cell", header=FALSE) %>%
  mutate(doublet = str_detect(cell, "[+]")) %>%
  mutate(tgt.index = 1:n()) %>% as.data.table

knn.dt <- readRDS(.knn.file) %>%
  merge(.src.cells) %>%
  merge(.tgt.cells, by="tgt.index", suffixes = c(".src", ".tgt")) %>%
  select(cell.src, cell.tgt, dist, doublet) %>%
  as.data.table()
```

$$\hat{P}_{i} = \frac{1}{|\mathcal{N}(i)|}\sum_{j \in \mathcal{N}(i)} I\{j\textsf{ is a doublet}\}$$

```{r}
.dbl.dt <-
  knn.dt[,
         .(`P`=mean(doublet)),
         by = .(cell.src)]
```

**Key assumption**:
There is a principal component that can set apart hidden doublets from the most of singlets.

:::
::: {.column width=.48}

```{r fig.width=2.5, fig.height=2}
.dt <- .dbl.dt %>%
  arrange(`P`) %>%
  mutate(i = 1:n())

ggplot(.dt, aes(i, `P`)) +
    ylab("Probabiliy of doublet") +
    xlab("cells sorted") +
    theme(axis.ticks.x = element_blank()) +
    theme(axis.text.x = element_blank()) +
    geom_point(stroke=0)
```
:::
::::::


## Doublets enriched in the outskirts of clusters

```{r fig.width=5, fig.height=2.5, only.plot=1}
.svd <- rcpp_mmutil_svd(qc.data$mtx, 10, NUM_THREADS=16)
tsne <- Rtsne::Rtsne(.svd$V, num_threads = 16)
colnames(tsne$Y) <- "tSNE" %&% 1:ncol(tsne$Y)

.dt <- data.table(.svd$V) %>%
    cbind(tsne$Y) %>% 
    mutate(name = readLines(qc.data$col)) %>%
    merge(.dbl.dt, by.x="name", by.y="cell.src") %>%
    arrange(P) %>% 
    na.omit()

ggplot(.dt, aes(V1, V2, fill=`P`, size=`P`)) +
    geom_point(stroke=.1, pch=21) +
    scale_size_continuous(range=c(.2, 2), guide="none") +
    scale_fill_distiller("P", palette="YlGnBu",direction=1)
```

```{r fig.width=5, fig.height=2.5, only.plot=2}
ggplot(.dt, aes(V3, V4, fill=`P`, size=`P`)) +
    geom_point(stroke=.1, pch=21) +
    scale_size_continuous(range=c(.2, 2), guide="none") +
    scale_fill_distiller("P", palette="YlGnBu",direction=1)
```

```{r fig.width=5, fig.height=2.5, only.plot=3}
ggplot(.dt, aes(V5, V6, fill=`P`, size=`P`)) +
    geom_point(stroke=.1, pch=21) +
    scale_size_continuous(range=c(.2, 2), guide="none") +
    scale_fill_distiller("P", palette="YlGnBu",direction=1)
```

```{r fig.width=5, fig.height=2.5, only.plot=4}
ggplot(.dt, aes(tSNE1, tSNE2, fill=`P`, size=`P`)) +
    geom_point(stroke=.1, pch=21) +
    scale_size_continuous(range=c(.2, 2), guide="none") +
    scale_fill_distiller("P", palette="YlGnBu",direction=1)
```

# Data normalization across many batches

## Batch normalization aims to minimize the difference between nearest cells across different batches

\vfill

:::::: {.columns}
::: {.column width=.45}
\only<1-2>{\centerline{\includegraphics[width=.85\linewidth]{img/scRNA_batch_normalization/mutual_neighbour1.pdf}}}

:::
::: {.column width=.45}

\only<2>{\centerline{\includegraphics[width=.85\linewidth]{img/scRNA_batch_normalization/mutual_neighbour2.pdf}}}

:::
::::::

\only<3>{\centerline{\includegraphics[width=.7\linewidth]{img/scRNA_batch_normalization/mutual_neighbour3.pdf}}}

:::::: {.columns}
::: {.column width=.45}

\only<4>{
\centerline{\includegraphics[width=.85\linewidth]{img/scRNA_batch_normalization/mutual_neighbour4.pdf}}
}

\only<5>{
\centerline{\includegraphics[width=.85\linewidth]{img/scRNA_batch_normalization/mutual_neighbour5.pdf}}
}

:::
::: {.column width=.45}

\only<4-5>{
What is the gap $\Delta$ between the batches?

\centerline{$\min_{\Delta} \sum_{a,b} W_{ab} \| \mathbf{x}_{a} - \mathbf{x}_{b} - \Delta\|_{2}$}
}

\only<4>{
A key assumption:
$$0 \approx W_{ac} < W_{ab}$$
}

\only<5>{
Fixed point (local) optimal solution:
$$\Delta \gets \frac{\sum_{a,b} W_{ab}(\mathbf{x}_{a} - \mathbf{x}_{a})}{\sum_{b} W_{ab}}$$
}

:::
::::::

\vfill
\flushright
\tiny
Haghverdi, .., and Marioni, \emph{Nature Biotechnology} (2018)

## A batch-balancing k-nearest neighbour graph

BBKNN method strikes balance between over- and under-normalization

\vfill

:::::: {.columns}
::: {.column width=.75}

\only<1>{\includegraphics[width=\linewidth]{img/scRNA_batch_normalization/bbknn1.pdf}}

:::
::: {.column width=.2}

\only<1>{
What kind of differences in due to inter-batch, technical discrepancy, not inter-cell-type divergence?
}

:::
::::::

\only<2>{\centerline{\includegraphics[width=.8\linewidth]{img/scRNA_batch_normalization/bbknn2.pdf}}}
\only<3>{\centerline{\includegraphics[width=.8\linewidth]{img/scRNA_batch_normalization/bbknn3.pdf}}}

\vfill
\tiny
Polanski, .., Teichmann \emph{Bioinformatics} (2019)



##

\vfill

:::::: {.columns}
::: {.column width=.75}

\centerline{
\includegraphics[width=\linewidth]{img/scRNA_batch_normalization/scanorama.pdf}
}

:::
::: {.column width=.2}

$${}$$

How do we integrate multiple samples/batches?

$${}$$

**Scanorama**: mutual nearest neighbourhood-based data integration

:::
::::::

\vfill
\tiny
Hie, Bryson, Berger \emph{Nature Biotechnology} (2019)

## Example: pancreatic cells across three batches

\vfill
Single-cell RNA-seq data from three donors (three batches)
\vfill

\centerline{\includegraphics[height=.35\textheight]{img/scRNA_batch_normalization/GSE131886.pdf}}

\vfill

*Goal*: Remove potential batch effects across different donors. (1) Construct BBKNN graphs between cells; (2) compute average discrepancy $\Delta$ between batches in the PC space; (3) adjust them.

\vfill
\tiny
Qadir, \textit{et al.}, \emph{PNAS} (2020)


```{r}
merged.data <- fileset.list("../data/GSE131886/GSE131886")

.batch.info <- fread(merged.data$col, header = FALSE, col.names = "tag")
.batch.info[, c("barcode", "batch") := tstrsplit(tag, split="_")]

bbknn.file <- fig.dir %&% "/GSE131886_bbknn.RDS"

if.needed(bbknn.file, {

    .svd <- rcpp_mmutil_svd(merged.data$mtx,
                            RANK = 50,
                            NUM_THREADS = 16)

    bbknn.out <- rcpp_mmutil_bbknn(
        r_svd_v = .svd$V,
        r_batches = .batch.info$batch,
        knn = 50,
        NUM_THREADS = 16)

    saveRDS(bbknn.out, bbknn.file)
})
bbknn.out <- readRDS(bbknn.file)
```

## BBKNN-guided normalization

```{r}
colnames(bbknn.out$V) <- "PC" %&% 1:ncol(bbknn.out$V)
colnames(bbknn.out$factors.adjusted) <- "PC.adj" %&% 1:ncol(bbknn.out$factors.adjusted)

tsne.0 <- Rtsne::Rtsne(bbknn.out$V, num_threads = 16)
colnames(tsne.0$Y) <- "tSNE" %&% 1:ncol(tsne.0$Y)

tsne.1 <- Rtsne::Rtsne(bbknn.out$factors.adjusted, num_threads = 16)
colnames(tsne.1$Y) <- "tSNE.adj" %&% 1:ncol(tsne.1$Y)

pca.dt <-
    data.table(bbknn.out$V,bbknn.out$factors.adjusted) %>%
    cbind(tsne.0$Y) %>% 
    cbind(tsne.1$Y) %>% 
    mutate(batch = .batch.info$batch)

theme_update(axis.text = element_blank())
```

```{r fig.width=5.8, fig.height=2.5, only.plot="1"}
p1 <-
    ggplot(pca.dt[sample(.N)], aes(PC1, PC2, color=batch)) +
    ggtitle("raw") +
    ggrastr::rasterise(geom_point(stroke=0, alpha=.7), dpi=300) +
    scale_color_brewer(palette = "Set1")

p2 <-
    ggplot(pca.dt[sample(.N)], aes(PC.adj1, PC.adj2, color=batch)) +
    ggtitle("BBKNN adjusted") +
    ggrastr::rasterise(geom_point(stroke=0, alpha=.7), dpi=300)+
    scale_color_brewer(palette = "Set1")

p1 | p2
```


```{r fig.width=5.8, fig.height=2.5, only.plot="2"}
p1 <-
    ggplot(pca.dt[sample(.N)], aes(PC3, PC4, color=batch)) +
    ggtitle("raw") +
    ggrastr::rasterise(geom_point(stroke=0, alpha=.7), dpi=300)+
    scale_color_brewer(palette = "Set1")

p2 <-
    ggplot(pca.dt[sample(.N)], aes(PC.adj3, PC.adj4, color=batch)) +
    ggtitle("BBKNN adjusted") +
    ggrastr::rasterise(geom_point(stroke=0, alpha=.7), dpi=300)+
    scale_color_brewer(palette = "Set1")

p1 | p2
```


```{r fig.width=5.8, fig.height=2.5, only.plot="3"}
p1 <-
    ggplot(pca.dt[sample(.N)], aes(PC5, PC6, color=batch)) +
    ggtitle("raw") +
    ggrastr::rasterise(geom_point(stroke=0, alpha=.7), dpi=300)+
    scale_color_brewer(palette = "Set1")

p2 <-
    ggplot(pca.dt[sample(.N)], aes(PC.adj5, PC.adj6, color=batch)) +
    ggtitle("BBKNN adjusted") +
    ggrastr::rasterise(geom_point(stroke=0, alpha=.7), dpi=300)+
    scale_color_brewer(palette = "Set1")

p1 | p2
```


```{r fig.width=5.8, fig.height=2.5, only.plot="4"}
p1 <-
    ggplot(pca.dt[sample(.N)], aes(PC7, PC8, color=batch)) +
    ggtitle("raw") +
    ggrastr::rasterise(geom_point(stroke=0, alpha=.7), dpi=300)+
    scale_color_brewer(palette = "Set1")

p2 <-
    ggplot(pca.dt[sample(.N)], aes(PC.adj7, PC.adj8, color=batch)) +
    ggtitle("BBKNN adjusted") +
    ggrastr::rasterise(geom_point(stroke=0, alpha=.7), dpi=300)+
    scale_color_brewer(palette = "Set1")

p1 | p2
```

```{r fig.width=5.8, fig.height=2.5, only.plot="5"}
p1 <-
    ggplot(pca.dt[sample(.N)], aes(tSNE1, tSNE2, color=batch)) +
    ggtitle("raw") +
    ggrastr::rasterise(geom_point(stroke=0, alpha=.7), dpi=300)+
    scale_color_brewer(palette = "Set1")

p2 <-
    ggplot(pca.dt[sample(.N)], aes(tSNE.adj1, tSNE.adj2, color=batch)) +
    ggtitle("BBKNN adjusted") +
    ggrastr::rasterise(geom_point(stroke=0, alpha=.7), dpi=300)+
    scale_color_brewer(palette = "Set1")

p1 | p2
```

## Discussions

\large

- What can we do with BBKNN graphs?

- Why do we need batch normalization?

- Is it possible to over-correct the differences?

- Is it also possible to under-correct the differences?


## Harmony: clustering-based data normalization

\vfill
\centerline{
\onslide<1->{\includegraphics[width=.23\linewidth]{img/scRNA_batch_normalization/harmony1.pdf}}
\onslide<2->{$\to$\includegraphics[width=.23\linewidth]{img/scRNA_batch_normalization/harmony2.pdf}}
\onslide<3->{$\to$\includegraphics[width=.23\linewidth]{img/scRNA_batch_normalization/harmony3.pdf}}
\onslide<4>{$\to$\includegraphics[width=.23\linewidth]{img/scRNA_batch_normalization/harmony4.pdf}}
}
\vfill
\flushright
\tiny
Korsunsky, .., Loh, Raychaudhuri \emph{Nature Methods} (2019)
