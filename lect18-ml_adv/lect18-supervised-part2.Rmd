---
title: "Supervised Learning II: Advanced Topics"
author: |
    | Yongjin Park
    | University of British Columbia
date: "`r format(Sys.time(), '%d %B, %Y')`"
classoption: "aspectratio=169"
output:
    powerpoint_presentation:
        reference_doc: "_template.pptx"
    html_document:
        self_contained: true
    beamer_presentation:
        colortheme: "orchid"
        keep_tex: true
        latex_engine: xelatex
        slide_level: 2
header-includes:
  - \AtBeginSection[]{\begin{frame}\frametitle{Today's lecture}\tableofcontents[currentsection]\end{frame}}
  - |
    \makeatletter
    \def\ps@titlepage{%
      \setbeamertemplate{footline}{}
    }
    \addtobeamertemplate{title page}{\thispagestyle{titlepage}}{}
    \makeatother
    \include{toc}
---

```{r setup, include=FALSE}
library(tidyverse)
library(data.table)
library(patchwork)
source("Util.R")
source("Setup.R")
fig.dir <- "Fig/"
setup.env(fig.dir)
dir.create("Data", showWarnings=FALSE)
theme_set(theme_classic())
```

```{r setup_cmdstan, include = FALSE}
library(cmdstanr)
library(posterior)
check_cmdstan_toolchain(fix = TRUE, quiet = TRUE)
register_knitr_engine()
```

# Non-parametric regression method

## Why do learn non-parametric methods?

\large

- We are already using it in many statistics applications!

    - E.g., mean-variance relationship modelling by `lowess` in `voom`
	
    - E.g., `geom_smooth` in the `ggplot` ecosystem

- Interpolation, extrapolation, and smoothing trends

\normalsize


```{r include=FALSE}
data(mcycle, package="MASS")

lm0 <- lm(accel ~ times, mcycle)
out0 <- loess(accel ~ times, mcycle, degree=0)
out1 <- loess(accel ~ times, mcycle, degree=1)
out2 <- loess(accel ~ times, mcycle, degree=2)

.dt <- data.table(mcycle,
                  lm = predict(lm0),
                  d0 = predict(out0),
                  d1 = predict(out1),
                  d2 = predict(out2))
```

## There are many things that a linear model cannot capture

```{r fig.width=5, fig.height=3, only.plot="1", echo=FALSE}
plt <- ggplot(.dt, aes(times, accel)) + geom_point(color="gray40")
print(plt)
```

```{r fig.width=5, fig.height=3, only.plot="2", echo=FALSE}
plt + geom_line(aes(y=lm), color="blue", size=1, lty=2)
```

## Let's think locally ... local polynomial regression `loess`

```{r fig.width=5, fig.height=3, only.plot="1", echo=FALSE}
plt + geom_line(aes(y=lm), color="blue", size=1, lty=2) +
    geom_line(aes(y=d0), color="magenta", size=1) +
    ggtitle("Local mean")
```


```{r fig.width=5, fig.height=3, only.plot="2", echo=FALSE}
plt + geom_line(aes(y=lm), color="blue", size=1, lty=2) +
    geom_line(aes(y=d1), color="magenta", size=1) +
    ggtitle("Local linear")
```


```{r fig.width=5, fig.height=3, only.plot="3", echo=FALSE}
plt + geom_line(aes(y=lm), color="blue", size=1, lty=2) +
    geom_line(aes(y=d2), color="magenta", size=1) +
    ggtitle("Local quadratic")
```



## Nadaraya-Watson (kernelized local) estimator

:::::: {.columns}
::: {.column width=.5}

```{r mcycle_train_test}
nn <- nrow(mcycle)
train.idx <- sample(nn, 50)
x.test <-
    mcycle[-train.idx, 1, drop = FALSE] %>%
    as.matrix
y.test <-
    mcycle[-train.idx, 2, drop = FALSE] %>%
    as.matrix
x.train <-
    mcycle[train.idx, 1, drop = FALSE] %>%
    as.matrix
y.train <-
    mcycle[train.idx, 2, drop = FALSE] %>%
    as.matrix
```

:::
::: {.column width=.5}

```{r echo=TRUE}
#' @param x.test testing data points
#' @param X training X
#' @param Y training Y
#' @param h bandwidth h
nw.rbf <- function(x.test, X, Y, h) {
    rbf <- kernlab::rbfdot(sigma=h)
    K <- kernlab::kernelMatrix(rbf, x.test, X)
    W <- K / rowSums(K)
    W %*% as.matrix(Y)
}
```

:::
::::::


## Basic idea: borrow neighbours' Y values in training data

```{r echo=FALSE}
library(kernlab)
lap <- laplacedot(sigma=.1)
rbf <- rbfdot(sigma=.1)
W.lap <- kernelMatrix(lap, x.test, x.train)
W.rbf <- kernelMatrix(rbf, x.test, x.train)
```

```{r echo=FALSE}
compare.kernel <- function(k){
    oo <- order(x.train - x.test[k])

    .dt <-
        rbind(data.table(x=x.train[oo], y=W.lap[k, oo], ker="Laplace"),
              data.table(x=x.train[oo], y=W.rbf[k, oo], ker="RBF"))

    ggplot(.dt, aes(x,y, color=ker)) +
        geom_line() +
        geom_point() +
        ylab("K(x,x*)") +
        theme(axis.text.x = element_blank()) +
        theme(axis.title.x = element_blank()) +
        scale_color_brewer("Kernel", palette = "Set2") +
        geom_vline(xintercept = x.test[k], color=2, lty=2)
}

focus.on.point <- function(k){
    .dt <- data.frame(cbind(x.train, y.train), w = W.rbf[k,])

    ww <- W.rbf[k, ]
    y.hat <- sum(ww/sum(ww) * unlist(y.train))

    .dt.lab <- data.frame(x=10, y=y.hat, label=round(y.hat))

    p1 <-
        ggplot(.dt, aes(times, accel)) +
        geom_point(aes(size=w), alpha=1, shape=21, fill=2, colour="gray20") +
        scale_size_continuous("RBF Kern", range=c(.5,3)) +
        geom_vline(xintercept = x.test[k], lty=2, color=2) +
        geom_hline(yintercept = y.hat, color=2) +
        ggrepel::geom_text_repel(aes(x,y,label=label),data=.dt.lab,
                                 size=4, alpha=.8)

    p0 <- compare.kernel(k) +
        ggtitle(paste0("dashed = test data: ", x.test[k]))

    p0/p1
}
```



```{r echo=F, fig.width=6, fig.height=3.2, only.plot="1"}
focus.on.point(1)
```

```{r echo=F, fig.width=6, fig.height=3.2, only.plot="2"}
focus.on.point(5)
```

```{r echo=F, fig.width=6, fig.height=3.2, only.plot="3"}
focus.on.point(35)
```

```{r echo=F, fig.width=6, fig.height=3.2, only.plot="4"}
focus.on.point(40)
```

```{r echo=F, fig.width=6, fig.height=3.2, only.plot="5"}
focus.on.point(50)
```

```{r echo=F, fig.width=6, fig.height=3.2, only.plot="6"}
focus.on.point(65)
```


## A full spectrum of Nadaraya-Watson with different bandwidth parameters

```{r echo = FALSE}
nw.coord <- function(h){
    xv <- as.matrix(seq(min(mcycle[,1]), max(mcycle[,1]), by = .1))
    yv <- nw.rbf(xv, x.train, y.train, h=h)
    .dt <- as.data.frame(cbind(xv, yv, h))
    colnames(.dt) <- c(colnames(mcycle)[1:2], "h")
    .dt
}

hv <- c(.01, .1, .5, 1, 3, 10, 50)
.dt <- do.call(rbind, lapply(hv, nw.coord)) %>%
    as.data.table

plt <- ggplot(mcycle, aes(times, accel)) +
    geom_point(color="gray40")

.show <- function(hh) {
    plt + geom_line(data=.dt[h==hh], color=2) +
        ggtitle(paste0("h = ", hh))
}
```

```{r echo=F, fig.width=5.5, fig.height=3.2, only.plot="1"}
.show(0.01)
```

```{r echo=F, fig.width=5.5, fig.height=3.2, only.plot="2"}
.show(0.1)
```

```{r echo=F, fig.width=5.5, fig.height=3.2, only.plot="3"}
.show(0.5)
```

```{r echo=F, fig.width=5.5, fig.height=3.2, only.plot="4"}
.show(1)
```

```{r echo=F, fig.width=5.5, fig.height=3.2, only.plot="5"}
.show(10)
```

```{r echo=F, fig.width=5.5, fig.height=3.2, only.plot="6"}
.show(50)
```

\vfill

Wasserman, _All of Non-parametric Statistics_ (2005)

`https://bookdown.org/egarpor/PM-UC3M/npreg-kre.html`


## Bias-variance tradeoff in generalization error

$$\mathbb{E}\!\left[\overset{\textsf{\color{red} true unknown model}}{f(X)} - \overset{\textsf{\color{blue} our attempt}}{\hat{f}(X)}\right]^2 =
\underbrace{\mathbb{E}\!\left[ f(X) - \mathbb{E}\!\left[\hat{f}\right] \right]^2}_{\textsf{\color{magenta} bias}^2} + 
\underbrace{\mathbb{E}\!\left[ \mathbb{E}\!\left[\hat{f}\right] - \hat{f}(X)) \right]^2}_{\textsf{\color{magenta} variance}}$$

\vfill

Wasserman, _All of Statistics_ (2005)

# Gaussian Process


## What is Gaussian process?

\large

- A non-parametric Bayesian approach to represent distribution over functions.

- A high-level generative model:

    1. Sample some function $f$ from GP

    2. Evaluate $f_{i} \gets f(\mathbf{x}_{i})$

- A function evaluated at each data point $i$, $f_{i}$, follows multivariate Gaussian

$$\mathbf{f} \sim \mathcal{N}\!\left(\underbrace{\mathbf{m}(\{\mathbf{x}_{i}\})}_{n \times 1 \textsf{\color{magenta} mean function}},
\underbrace{K(\{ \mathbf{x}_{i} \}, \{ \mathbf{x}_{i} \})}_{n \times n \textsf{\color{teal} covariance}}\right)$$

- We will only need to handle the sampled $\mathbf{f}$ vector, not an abstract $f \sim \mathcal{GP}$.

\normalsize

## Why is GP useful in practice?

\large

- When we don't have a clear idea of a type of function $f$

- High-dimensional design matrix $X$ where $d \gg n$

- Can avoid the curse of dimensionality!

- The goal is prediction!

$$\int df\, p(\underset{\textsf{\color{magenta}new data}}{\mathbf{y}^{\star}} | \underset{\textsf{\color{magenta}new data}}{X^{\star}}, \underset{\textsf{\color{blue} unknown function}}{f}) p(\underset{\textsf{\color{blue} unknown function}}{f}| \underset{\textsf{\color{teal} training data}}{X,\mathbf{y}}) \mathcal{GP}(f)$$

$$\to p(\mathbf{y}^{\star}|X^{\star}, \mathbf{y}, X)$$

\normalsize


## Covariance (Kernel) matrix when data points are scattered

:::::: {.columns}
::: {.column width=.5}

```{r}
n <- 100
d <- 1
x <- .rnorm(n, d)

rbf <- kernlab::rbfdot(sigma=1)
K <- kernlab::kernelMatrix(rbf, x)
```

:::
::: {.column width=.5}

```{r fig.width=2.8, fig.height=2.8, echo=FALSE}
.matshow(K, .lab=0)
```

:::
::::::

## Writing down Gaussian Process in `stan`

```{stan, output.var="gp"}
data {
  int<lower=1> N;                   // N data points
  int<lower=1> D;                   // Dimensionality
  array[N] vector[D] X;             // N x D data
}
transformed data {
  matrix[N, N] K;                   // (realized) Kernel matrix
  vector[N] mu = rep_vector(0, N);  // mean vector
  K = gp_exp_quad_cov(X, 1.0, 1.0); // RBF kernel
  for (i in 1:N)                    // regularizer
    K[i, i] = K[i, i] + 0.1;        //
  matrix[N, N] L;                   // Cholesky
  L = cholesky_decompose(K);        // for faster N(mu,K)
}
parameters {
  vector[N] y;
}
model {
  y ~ multi_normal_cholesky(mu, L);
}
```

See [Stan's GP tutorial](https://mc-stan.org/docs/2_22/stan-users-guide/simulating-from-a-gaussian-process.html)


## Or you can have a separate file for the `stan` code and import it to `R`

```{r eval=FALSE, size="large"}
gp <- cmdstan_model("example_gp1.stan")
```


## Let's generate "functions" with GP prior


:::::: {.columns}
::: {.column width=.3}

```{r message=FALSE}
.data <- list(N=n, D=d, X=x)

gp.sample <-
    gp$sample(data = .data,
              chains = 1,
              iter_warmup=10,
              iter_sampling=111)
```

```{r fig.width=1.6, fig.height=1.6, echo=F}
.matshow(K, .lab=0, .size=0)
```

```{r include=FALSE}
y.draw <- gp.sample$draws(format = "df")

y.melt <-
    y.draw %>%
    tidyr::gather(key = "i",
                  value = "y",
                  -.iteration) %>%
    dplyr::filter(str_starts(i, "y\\[")) %>%
    dplyr::mutate(ii = gsub("y|\\[|\\]", "", i)) %>%
    dplyr::mutate(ii = as.integer(ii)) %>%
    left_join(data.table(ii = 1:n, x = as.numeric(x)), by = "ii") %>%
    as.data.table

.show.gp <- function(iter, .dt){

    .dt.xy <- .dt[.iteration == iter] %>% 
        select(ii, x, y) %>%
        melt(id.vars = "ii")

    ggplot(.dt.xy, aes(ii, value, color=variable)) +
        facet_grid(variable ~ ., scale="free") +
        geom_line() +
        ggtitle(str_c("iteration = ", iter)) +
        theme(legend.position = "none")
}
```

:::
::: {.column width=.7}

```{r echo=F, only.plot="1", fig.width=4, fig.height=3}
.show.gp(1, y.melt)
```

```{r echo=F, only.plot="2", fig.width=4, fig.height=3}
.show.gp(10, y.melt)
```

```{r echo=F, only.plot="3", fig.width=4, fig.height=3}
.show.gp(100, y.melt)
```

:::
::::::


## When data points are "similar" to each other

:::::: {.columns}
::: {.column width=.5}

```{r}
xx <- rep(1:5, each=20) + .rnorm(n, 1) * .1
x <- matrix(xx, nrow = n, ncol = 1)

rbf <- kernlab::rbfdot(sigma=1)
K <- kernlab::kernelMatrix(rbf, x)
```

:::
::: {.column width=.5}

```{r fig.width=2.8, fig.height=2.8, echo=F}
.matshow(K, .lab=0)
```
:::
::::::


## Let's generate "functions" with the different kernel


:::::: {.columns}
::: {.column width=.3}

```{r message=FALSE}
.data <- list(N=n, D=d, X=x)

gp.sample <-
    gp$sample(data = .data,
              chains = 1,
              iter_warmup=10,
              iter_sampling=111)
```

```{r fig.width=1.6, fig.height=1.6, echo=F}
.matshow(K, .lab=0, .size=0)
```

```{r include = FALSE}
y.draw <- gp.sample$draws(format = "df")

y.melt <-
    y.draw %>%
    tidyr::gather(key = "i",
                  value = "y",
                  -.iteration) %>%
    dplyr::filter(str_starts(i, "y\\[")) %>%
    dplyr::mutate(ii = gsub("y|\\[|\\]", "", i)) %>%
    dplyr::mutate(ii = as.integer(ii)) %>%
    left_join(data.table(ii = 1:n, x = as.numeric(x)), by = "ii") %>%
    as.data.table
```


:::
::: {.column width=.7}

```{r echo=F, only.plot="1", fig.width=4, fig.height=3}
.show.gp(1, y.melt)
```

```{r echo=F, only.plot="2", fig.width=4, fig.height=3}
.show.gp(10, y.melt)
```

```{r echo=F, only.plot="3", fig.width=4, fig.height=3}
.show.gp(100, y.melt)
```

:::
::::::


## Gaussian Process regression


:::::: {.columns}
::: {.column width=.4}

```{r}
data(mcycle, package="MASS")

rbf <- kernlab::rbfdot(sigma=1)
xx <- mcycle[,1]
K <- kernlab::kernelMatrix(rbf, xx)
```

$$f \sim \mathcal{GP}(K)$$

$$\mathbf{y} \sim \mathcal{N}(\mathbf{f}, \sigma^{2} I)$$

:::
::: {.column width=.6}


```{r fig.width=2.8, fig.height=2.8, only.plot="1", echo=FALSE}
ggplot(mcycle,aes(times, accel)) +
    geom_point()
```

```{r fig.width=2.8, fig.height=2.8, only.plot="2", echo=F}
.matshow(K, .lab=0)
```

:::
::::::


## Writing down GP regression in `stan`

```{r eval=FALSE, size="large"}
gp.model <- cmdstan_model("example_gp2.stan")
```

```{r include=FALSE, results="none", message=FALSE}
gp.model <- cmdstan_model("example_gp2.stan")
```

## We can train Gaussian Process Regression models by MCMC

```{r message=FALSE}
data(mcycle, package="MASS")
x <- mcycle[, 1, drop = FALSE]
y <- mcycle[, 2]
n <- nrow(x)
d <- ncol(x)
```

```{r eval=FALSE}
.data <- list(N=n, D=d, x=x, y=y, Ntest=n, xtest=x)
gp.sample <- gp.model$sample(data = .data,
                             chains = 5,
                             parallel_chains = 5)
```

```{r include=FALSE, message=FALSE}

sample.gp <- function(xtrain, ytrain, xtest, ytest, file){
    
    if.needed(file, {

        d <- ncol(xtrain)
        .data <- list(N=nrow(xtrain),
                      D=ncol(xtrain),
                      x=xtrain,
                      y=ytrain,
                      Ntest=nrow(xtest),
                      xtest=xtest)

        gp.sample <- gp.model$sample(data = .data,
                                     seed = 1331,
                                     chains = 5,
                                     parallel_chains = 5)

        gp.pred <- posterior::as_draws_rvars(gp.sample$draws())

        pred.dt <- data.table(x = unlist(xtest), y=unlist(ytest),
                              mu = mean(gp.pred$pred),
                              sig = mean(gp.pred$sigma))

        obs.dt <- data.table(x = xtrain[,1], y = ytrain)

        saveRDS(list(pred = pred.dt, obs = obs.dt), file=file)
    })
    return(readRDS(file))
}

gp.full <- sample.gp(x,y,x,y,"example_gp2.rdata")

########################
## using first points ##
########################

gp.n10 <- sample.gp(x[1:10,,drop=FALSE],y[1:10],x,y,
                    "example_gp2_n10.rdata")

gp.n20 <- sample.gp(x[1:20,,drop=FALSE],y[1:20],x,y,
                    "example_gp2_n20.rdata")

gp.n50 <- sample.gp(x[1:50,,drop=FALSE],y[1:50],x,y,
                    "example_gp2_n50.rdata")

gp.n75 <- sample.gp(x[1:75,,drop=FALSE],y[1:75],x,y,
                    "example_gp2_n75.rdata")

##############################
## randomly selected points ##
##############################

set.seed(1331)
rr <- sample(n,10)
gp.r10 <- sample.gp(x[rr,,drop=F], y[rr], x, y,
                    "example_gp2_r10.rdata")

rr <- sample(n,20)
gp.r20 <- sample.gp(x[rr,,drop=F], y[rr], x, y,
                    "example_gp2_r20.rdata")

rr <- sample(n,50)
gp.r50 <- sample.gp(x[rr,,drop=F], y[rr], x, y,
                    "example_gp2_r50.rdata")

.show.gpr <- function(pred.dt, obs.dt = NULL){
    ret <- ggplot(pred.dt, aes(x,y)) +
        geom_point(colour="gray40") +
        geom_line(aes(x,mu - 3 * sig), lty=2, colour="blue") +
        geom_line(aes(x,mu), size=1, colour="navy") +
        geom_line(aes(x,mu + 3 * sig), lty=2, colour="blue")

    if(!is.null(obs.dt)){
        ret <- ret +
            geom_point(aes(x, y), data=obs.dt, colour="red", size=4, stroke=1, pch="+")
    }

    return(ret)
}
```


## What if we give only a partial data set?

```{r fig.width=5, fig.height=3, echo=F, only.plot="1"}
.show.gpr(gp.n10$pred, gp.n10$obs) + ggtitle("first 10 points")
```

```{r fig.width=5, fig.height=3, echo=F, only.plot="2"}
.show.gpr(gp.n20$pred, gp.n20$obs) + ggtitle("first 20 points")
```

```{r fig.width=5, fig.height=3, echo=F, only.plot="3"}
.show.gpr(gp.n50$pred, gp.n50$obs) + ggtitle("first 50 points")
```

```{r fig.width=5, fig.height=3, echo=F, only.plot="4"}
.show.gpr(gp.n75$pred, gp.n75$obs) + ggtitle("first 75 points")
```

```{r fig.width=5, fig.height=3, echo=F, only.plot="5"}
.show.gpr(gp.r10$pred, gp.r10$obs) + ggtitle("random 10 points")
```

```{r fig.width=5, fig.height=3, echo=F, only.plot="6"}
.show.gpr(gp.r20$pred, gp.r20$obs) + ggtitle("random 20 points")
```

```{r fig.width=5, fig.height=3, echo=F, only.plot="7"}
.show.gpr(gp.r50$pred, gp.r50$obs) + ggtitle("random 50 points")
```

## Full Gaussian Process regression training

```{r fig.width=5, fig.height=3, echo=F}
.show.gpr(gp.full$pred, NULL)
```

# Ensemble learning

```{r include=FALSE}
library(torch)
torch_set_num_threads(8)
```

## Motivation for Ensemble learning

\large

- Training a highly-expressive model can be "expensive" requiring many data points and increasing the risk of overfitting

- A simple local rule/model/classifier is often powerful enough to capture local patterns 

\normalsize


## Warm-up example: mixture of Gaussian distributions

```{r echo=FALSE}
#' @param nn sample size
#' @param D dimensionality
#' @param K number of clusters
simulate.data <- function(nn, D, K, sig = .2) {
  require(torch)
  torch_manual_seed(1447)
 .mu <- torch_randn(K, D) * 2
  kk <- torch_tensor(sample(K,nn,TRUE),
                     dtype=torch_long())
 .z <- nnf_one_hot(kk)$to(dtype=torch_float())
 .eps <- torch_randn(nn, D)
 list(x = torch_mm(.z, .mu) + .eps * sig,
      k = as.integer(kk), z = .z, mu = .mu)
}
``` 

```{r echo=FALSE, fig.width=4.5, fig.height=2.5, only.plot="1"}
sim.out <- simulate.data(30, 50, 3, sig = 1)
oo <- order(sim.out$k)
xx <- sim.out$x %r% oo
.matshow(xx, .size=0, .lab=0) + ggtitle("sample x gene")
```

```{r echo=FALSE, fig.width=4.5, fig.height=2.5, only.plot="2-"}
xx <- .sort.matrix(sim.out$x)
.matshow(xx, .size=0, .lab=0) + ggtitle("sample x gene")
```

```{r echo = FALSE}
sim.out <- simulate.data(300, 2, 3)
```


## What do we want to know from data?

```{r fig.width=3, fig.height=2.5, echo = FALSE}
X <- sim.out$x

.dt <- as.dt(X) %>%
    cbind(group = as.factor(sim.out$k))

.dt.mu <- as.dt(sim.out$mu)

plt.data <- 
    .gg.plot(.dt, aes(V1, V2, colour=group)) +
    geom_point(stroke=0, size=.7) +
    geom_point(data = .dt.mu, colour = "red", size=5, pch = "+") +
    scale_colour_brewer(palette="Paired") +
    theme(axis.title = element_blank())
print(plt.data)
```

**Two goals**: recover (1) group membership and (2) the centroids (red marks)

## A chicken-and-egg problem: guessing latent membership vs. parametric inference

* If we knew the membership of all the points, we can simply estimate the centre (e.g., taking sample mean within each cluster)

* If we knew the centre coordinates, we would be able to assign points to most probable groups easily based on distance from the centre points.

* Statistical answer: Solve the underlying inference problem.

* To a parameter estimator, the membership assignments are *hidden* (latent).

<!-- Make your dimensions right... -->
<!-- * $D$: each data point's dimension -->
<!-- * $K$: desired number of clusters (can be relaxed or tuned) -->

```{r echo = FALSE}
N <- nrow(X)
D <- ncol(X)
K <- 3
```

<!-- Random initialization of model parameters: -->
<!-- * $\mu_{k} \sim \mathcal{N}\!\left(0, 1/10 I\right)$ -->
<!-- * $\log\sigma_{k} \gets 0$ for all $k$ -->
<!-- Alternatively, we can start with random assignment of samples to the clusters. -->

```{r echo = FALSE}
set.seed(3)
.mu <- scale(matrix(rnorm(K*D), K, D)) * .1
mu <- torch_tensor(.mu, requires_grad = TRUE)
ln.sig <- torch_zeros(K, D, requires_grad=TRUE)
```

## Gaussian Mixture Model (k-means)

1. Initialize $\mu_{k}$ (the centre of each group) and $\sigma_{k}$ (the spread within each group)

2. Randomly assign group membership, $Z_{ik} = 1$ iff a point $i$ belongs to a group $k$.

3. Generate: $\mathbf{x}_{i} | Z_{ik} = 1, \boldsymbol{\mu}_{k} \sim \mathcal{N}\!\left(\boldsymbol{\mu}_{k}, \sigma^{2}I\right)$

How do we infer $Z$ and $\mu,\sigma$?

## Expectation Maximization for GMM MLE

\begin{eqnarray*}
\onslide<1->{
	J &\equiv& \log \prod_{i=1}^{n} p(\mathbf{x}_{i}|\mu,\sigma) \\
}
\onslide<2>{
	&=& \log \prod_{i=1}^{n} \sum_{Z} p(\mathbf{x}_{i}|Z,\mu,\sigma)p(Z)
}
\end{eqnarray*}

\onslide<2>{
* It might be difficult to enumerate all the $Z$'s... so let's introduce some other distributions that will help our "guessing" work, which we call it $q(Z)$
}

## EM algorithm: What is the best way to guess latent variables?

\begin{eqnarray*}
\onslide<1->{
  \sum_{i} \log p(\mathbf{x}_{i}|\mu,\sigma)
  &=& \sum_{i=1}^{n} {\color{blue} \log} \sum_{Z_{i}} {\color{red} \frac{q(Z_{i})}{q(Z_{i})} } p(\mathbf{x}_{i}|Z_{i},\mu,\sigma)p(Z_{i}) \\
  }
\onslide<2->{
  &\underset{\textsf{\color{blue} Jensen}}{\ge}&
   \sum_{i=1}^{n} \sum_{Z_{i}} q(Z_{i}) {\color{blue}\log} \frac{p(\mathbf{x}_{i}|Z_{i},\mu,\sigma)p(Z_{i})}{q(Z_{i})} \\ }
\end{eqnarray*}

\onslide<3->{ What is the best $q(Z)$? }

## EM algorithm: optimal E-step is to take the posterior probability

\onslide<1->{ If $q(Z)=p(Z|\mathbf{x}_{i},\mu,\sigma)$ (by Bayes rule),  }

\begin{eqnarray*}
\onslide<1->{
  &=&
      \sum_{i}^{n}\sum_{Z_{i}} p(Z_{i}|\mathbf{x}_{i},\mu,\sigma)
	\log \frac{p(\mathbf{x}_{i}|Z_{i},\mu,\sigma)p(Z_{i})}{p(Z_{i}|\mathbf{x}_{i}\mu,\sigma)}\\
}
\onslide<2->{
  &=&
\sum_{i}^{n}\sum_{Z_{i}} p(Z_{i}|\mathbf{x}_{i},\mu,\sigma)
\log \frac{p(\mathbf{x}_{i},Z_{i}|\mu,\sigma) p(\mathbf{x}_{i}|\mu,\sigma)}{p(\mathbf{x}_{i}, Z_{i}|\mu,\sigma)} \\
}
\onslide<3>{
&=&
    \sum_{i=1}^{n} \underbrace{\left[ \sum_{Z_{i}} p(Z_{i}|\mathbf{x}_{i},\mu,\sigma) \right]}_{\color{red} = 1} \log p(\mathbf{x}_{i}|\mu,\sigma) 
  \\
  }
  \onslide<3>{
  &=& \sum_{i} \log p(\mathbf{x}_{i}|\mu,\sigma)
      }
\end{eqnarray*}

\onslide<3>{The inequality becomes equality.}

## Expectation Maximization algorithm = expected MLE

**The goal**: 

\begin{eqnarray}
  \log p(X|\mu,\sigma) &=& \log \sum_{Z} p(X,Z|\mu,\sigma) \\
  &\underset{\textsf{\color{blue} Jensen}}{\ge}&
  \sum_{Z} \underbrace{p(Z|X,\mu,\sigma)}_{\color{red}\textsf{E-step}} \underbrace{\log p(X|Z,\mu,\sigma)}_{\color{red}\textsf{M-step}} \\  
 &=& \underbrace{\mathbb{E}_{p(Z|X,\mu,\sigma)}}_{\color{red}\textsf{E-step}} \left[ \underbrace{\log p(X|Z,\mu,\sigma)}_{\color{red}\textsf{M-step}} \right]
\end{eqnarray}

($Z$ is discrete, e.g., a membership indicator)

**Solution**: **Maximize** the lower bound by taking **the expectation** over the posterior probability.

## EM algorithm of GMM: E-step

Log-likelihood under some group ($\mu_{k}$ and $\sigma_{k}$):

$\log p(\mathbf{x}_{i}|\mu_{k}, \sigma_{k}) = \log \mathcal{N}\!\left(\mathbf{x}_{i}|\mu_{k},\sigma_{k}\right)$

```{r echo = FALSE}
#' @param X data (n x D)
#' @param .mu centre parameters (K x D)
#' @param .ln.sig log standard dev (K x D)
log.lik <- function(X, .mu, .ln.sig){
    .ln.sig <- torch_clamp(.ln.sig,
                           log(0.01),
                           log(100.))

    .rs <- torch_square(X - .mu)
    .sig <- torch_exp(.ln.sig)
    llik <- - .rs/.sig/2.0 - .ln.sig/2.0

    torch_sum(llik, dim=2) # sum over dim
}
```

How to estimate the posterior?

$$p(Z_{ik}| \mathbf{x}_{i}, \mu, \sigma)
= \frac{\exp\{\log p(\mathbf{x}_{i}|\mu_{k},\sigma_{k})\}}{\sum_{k'} \exp\{\log p(\mathbf{x}_{i}|\mu_{k'},\sigma_{k'})\}}$$

```{r echo = FALSE}
#' sample latent Z ~ the posterior
take.estep <- function() {
    llik.mat <- torch_zeros(nrow(X), K)
    for(k in 1:K){
        mu.k <- mu[k, , drop = FALSE]
        ln.sig.k <- ln.sig[k, , drop = FALSE]
        llik.mat[, k] <- log.lik(X, mu.k, ln.sig.k)
    }
    zz <- distr_categorical(logits = llik.mat)
    zz$sample()
}
```

*Remark*: We can stochastically sample $Z_{ik}=1$ with the posterior probability.

## EM algorithm of GMM: M-step

Maximization step to optimize model parameters

Let this expected lower-bound (ELBO)

$$\mathcal{L}(\mathbf{x}_{i}; \{\mu_{k}\}, \{\sigma_{k}\})
=
\sum_{i=1}^{n}\sum_{k=1}^{K} Z_{ik} \log p(\mathbf{x}_{i}| Z_{ik}, \mu_{k}, \sigma_{k})$$

Given $Z$, what are the unknown? We can take gradient steps (e.g., `torch`)

$$\mu_{k}^{(t)} \gets \mu_{k}^{(t-1)} + \rho \nabla_{\mu_{k}} \sum_{i} \mathcal{L}(\mathbf{x}_{i})$$

$$\sigma_{k}^{(t)} \gets \sigma_{k}^{(t-1)} + \rho \nabla_{\sigma_{k}} \sum_{i} \mathcal{L}(\mathbf{x}_{i})$$

```{r echo = FALSE}
## `torch` implementation
opt <- optim_adam(list(mu, ln.sig), lr=.25)
take.mstep <- function(Z){
  opt$zero_grad()
  llik.1 <- log.lik(X, mu[1, , drop= FALSE],
                    ln.sig[1, , drop=FALSE])
  llik <- llik.1 * Z[, 1]
  for(k in 2:ncol(Z)){
      llik.k <- log.lik(X, mu[k, , drop=FALSE],
                        ln.sig[k, , drop=FALSE])
      llik <- llik + llik.k * Z[, k]
  }
  loss <- torch_mean(- llik)
  loss$backward()
  . <- opt$step()
  return(torch_sum(llik)$item())
}
```

```{r echo = FALSE}
##################################
## actually taking the EM steps ##
##################################

llik.trace <- c()
Z.trace <- data.table()

.grad <- torch_zeros_like(mu)
mu.trace <- cbind(as.dt(mu),
                  grad=as.dt(.grad),
                  k = 1:nrow(mu),
                  t=0)

.grad <- torch_zeros_like(ln.sig)
ln.sig.trace <- cbind(as.dt(ln.sig),
                      grad=as.dt(.grad),
                      k = 1:nrow(mu),
                      t=0)

for(tt in 1:100){
    rand.idx <- take.estep()
    z <- nnf_one_hot(rand.idx)
    llik <- take.mstep(z)

    z.dt <- data.table(k=as.integer(rand.idx), i=1:nrow(X), t=tt)

    Z.trace <- rbind(Z.trace, z.dt)
    llik.trace <- c(llik.trace, llik)

    mu.dt <- cbind(as.dt(mu),
                   grad=as.dt(mu$grad),
                   k=1:nrow(mu),
                   t=tt)
    mu.trace <- rbind(mu.trace, mu.dt)

    ln.sig.dt <- cbind(as.dt(ln.sig),
                       grad=as.dt(ln.sig$grad),
                       k=1:nrow(mu),
                       t=tt)
    ln.sig.trace <- rbind(ln.sig.trace, ln.sig.dt)
}
```

## Alternate E- and M-step until convergence

:::::: {.columns}
::: {.column width=.5}

```{r echo=TRUE, eval=FALSE, size="large"}
for(tt in 1:100){
    rand.idx <- take.estep()
    z <- nnf_one_hot(rand.idx)
    llik <- take.mstep(z)
}
```

Find the details here:

```
https://github.com/STAT540-UBC/lectures
```

:::
::: {.column width=.5}

```{r echo = FALSE, fig.width=2.2, fig.height=1.5}
.dt <- as.dt(llik.trace, "llik") %>%
    mutate(iter = 1:n())

.gg.plot(.dt, aes(iter, llik)) +
    ylab("log-likelihood") +
    geom_smooth(se=FALSE, colour = "red", lty = 2, size=.5) +
    geom_point(pch=21, stroke = .5, size = .5, fill = "white")

```

:::
::::::


## Show the trace of the EM algorithm

```{r echo = FALSE}
show.mu.trace <- function(.mu, t.max, show.k=1:K){

    ret <- .gg.plot(as.dt(X), aes(V1, V2)) +
        geom_point(colour="gray", alpha=.8, size=.5) +
        theme(axis.title = element_blank())

    .mu <- .mu[t <= t.max & k %in% show.k]

    .dt <- .mu %>%
        mutate(k = factor(k, 1:K))

    .aes <- aes(V1, V2,
                fill=k,
                alpha = `t`)

    .dt.2 <- .dt[`t` == t.max] %>%
        mutate(k = factor(k, 1:K))

    .aes.2 <- aes(V1, V2,
                  colour=k,
                  xend = V1 - sign(grad.V1) * pmin(1, abs(10 * grad.V1)),
                  yend = V2 - sign(grad.V2) * pmin(1, abs(10 * grad.V2)))

    .arr <- arrow(length=unit(".2", "lines"))

    ret +        
        ggtitle(str_c("M-step Iter = ", t.max)) +
        geom_point(.aes, data=.dt, stroke = 0, pch=21, size=2) +
        geom_segment(.aes.2, size=.5, data=.dt.2, arrow=.arr) +
        scale_alpha_continuous(range=c(0, 1), guide="none") +
        scale_fill_brewer(palette = "Dark2", guide="none") +
        scale_colour_brewer(palette = "Dark2", guide="none")
}

show.z.trace <- function(.trace, t.max) {

    .dt <-
        .trace[`t` == t.max] %>%
        left_join(cbind(as.dt(X), i=1:nrow(X)), by="i") %>%
        mutate(`k`=factor(`k`, 1:K))

    .gg.plot(.dt, aes(V1, V2, fill=k)) +
        ggtitle(str_c("E-step Iter = ", t.max)) +
        geom_point(pch=21, alpha=.8, stroke=.2) +
        theme(axis.title = element_blank())+
        scale_fill_brewer(palette = "Dark2", guide = "none")
}
```

```{r fig.width = 5, fig.height = 2, echo = FALSE, only.plot = 1}
tt <- 1
show.z.trace(Z.trace, tt) | show.mu.trace(mu.trace, tt)
```

```{r fig.width = 5, fig.height = 2, echo = FALSE, only.plot = 2}
tt <- 2
show.z.trace(Z.trace, tt) | show.mu.trace(mu.trace, tt)
```

```{r fig.width = 5, fig.height = 2, echo = FALSE, only.plot = 3}
tt <- 3
show.z.trace(Z.trace, tt) | show.mu.trace(mu.trace, tt)
```

```{r fig.width = 5, fig.height = 2, echo = FALSE, only.plot = 4}
tt <- 4
show.z.trace(Z.trace, tt) | show.mu.trace(mu.trace, tt)
```

```{r fig.width = 5, fig.height = 2, echo = FALSE, only.plot = 5}
tt <- 5
show.z.trace(Z.trace, tt) | show.mu.trace(mu.trace, tt)
```

```{r fig.width = 5, fig.height = 2, echo = FALSE, only.plot = 6}
tt <- 10
show.z.trace(Z.trace, tt) | show.mu.trace(mu.trace, tt)
```

```{r fig.width = 5, fig.height = 2, echo = FALSE, only.plot = 7}
tt <- 20
show.z.trace(Z.trace, tt) | show.mu.trace(mu.trace, tt)
```

```{r fig.width = 5, fig.height = 2, echo = FALSE, only.plot = 8}
tt <- 25
show.z.trace(Z.trace, tt) | show.mu.trace(mu.trace, tt)
```

```{r fig.width = 5, fig.height = 2, echo = FALSE, only.plot = 9}
tt <- 30
show.z.trace(Z.trace, tt) | show.mu.trace(mu.trace, tt)
```

```{r fig.width = 5, fig.height = 2, echo = FALSE, only.plot = 10}
tt <- 40
show.z.trace(Z.trace, tt) | show.mu.trace(mu.trace, tt)
```

```{r fig.width = 5, fig.height = 2, echo = FALSE, only.plot = 11}
tt <- 100
show.z.trace(Z.trace, tt) | show.mu.trace(mu.trace, tt)
```

* Arrows: stochastic gradient $\nabla \mu$

* Colour: latent membership


## Mixture of linear regression models

```{r echo = FALSE}
#' @param nn sample size
#' @param K number of clusters
simulate.regression.data <- function(nn, K, sig = 1) {
    .beta <-
        list(torch_zeros(1,1), 2 * torch_randn(1, K-1)) %>% 
        torch_cat(dim=2)
    kk <- torch_randint(1,K+1,nn)$to(dtype=torch_long())
    .z <- nnf_one_hot(kk)$to(dtype=torch_float())
    .eps <- torch_randn(nn, 1)
    xx <- torch_randn(nn, 1)
    y.k <- (torch_mm(xx, .beta) + .eps * sig) * .z
    yy <- torch_sum(y.k, dim = 2, keepdim = TRUE)
    list(x = xx,
         y = yy,
         k = as.integer(kk),
         beta = .beta)
}
```

```{r echo = FALSE}
torch_manual_seed(47)
sim.out <- simulate.regression.data(1500, 3)
```


```{r fig.width=4, fig.height=3, echo=FALSE}
.dt <- data.frame(x = as.numeric(sim.out$x),
                  y = as.numeric(sim.out$y),
                  k = as.factor(sim.out$k))

.gg.plot(.dt, aes(x, y)) +
    geom_point(colour="gray40", stroke = 0)
```

## Modelling a mixture of regression models

```{r fig.width=4, fig.height=3, echo = FALSE}
.gg.plot(.dt, aes(x, y, colour=k, group=k)) +
    scale_colour_brewer(palette = "Set2") +
    geom_point(stroke = 0, alpha = .5) +
    geom_smooth(method = "lm", se=FALSE) +
    ggpubr::stat_regline_equation(colour="black")
```


## What is the data generation process?

1. Initialize $\beta_{k}$ (the slope of each model) and $\sigma_{k}$ (the spread within each model)

2. Randomly assign group membership, $Z_{ik} = 1$ iff a point $i$ belongs to a group $k$.

3. Generate: $Y_{i} | X_{i}, Z_{ik} = 1, \boldsymbol{\mu}_{k} \sim \mathcal{N}\!\left(X_{i}\boldsymbol{\beta}_{k}, \sigma_{k}^{2}\right)$

## What is the expected log-likelihood?

```{r echo = FALSE}
log.lik.reg <- function(X, Y, .beta, .ln.sig){
    Y.hat <- torch_mm(X, .beta)
    .rs <- torch_square(Y - Y.hat)
    .sig <- torch_exp(.ln.sig)
    llik <- - .rs/.sig/2.0 - .ln.sig/2.0
}
```

Expected log-likelihood (to maximize):

$$\mathcal{L} = \sum_{i=1}^{n} \sum_{k=1}^{K} p(Z_{ik}|Y_{i}, X_{i}, \beta_{k}, \sigma_{k}) \log p(Y_{i}|X_{i}, \beta_{k}, \sigma_{k})$$

where

$$\log p(Y_{i}|X_{i}, \beta_{k}, \sigma_{k}) 
= \log \mathcal{N}\!\left(Y_{i}|X_{i}\beta_{k},\sigma_{k}^{2}\right)
= 
-\frac{1}{2\sigma_{k}^{2}}(Y_{i} - X_{i} \beta_{k})^{2}
-\frac{1}{2}\log \sigma_{k}^{2}$$

So, it's equivalent to finding weighted least square estimates (if $\sigma_{k}=1$):

$$\min \sum_{k=1}^{K} \sum_{i=1}^{n} \underbrace{\mathbb{E}\!\left[Z_{ik}\right]}_{\textsf{\color{red} E-step}} \underbrace{(Y_{i} - X_{i} \beta_{k})^{2}}_{\textsf{\color{blue} M-step}}$$

## E-step: What is the posterior probability of assigning each data point?

```{r regression_initialization, echo = FALSE}
torch_manual_seed(17)
X <- sim.out$x
Y <- sim.out$y
K <- 3
reg.beta <- torch_zeros(1, K, requires_grad = TRUE)
reg.ln.var <- torch_ones(1, K, requires_grad = TRUE)
```

```{r echo = FALSE}
take.estep <- function() {
    llik.mat <- log.lik.reg(X, Y, reg.beta, reg.ln.var)
    zz <- distr_categorical(logits = llik.mat)
    zz$sample()
}
```

Given $\beta_{k}$ and $\sigma_{k}$:

$$p(Z_{ik}|X_{i}, Y_{i}, \beta_{k}, \sigma_{k}) = 
\frac{
\exp\left( - \frac{1}{2\sigma_{k}^{2}} \left(Y_{i} - 
\only<1>{ \overbrace{X_{i} \beta_{k}}^{\textsf{\color{red} prediction}} } 
\only<2>{ X_{i} \beta_{k} }
\right)^{2} - \log \sigma_{k}\right)
}{
\sum_{k'} \exp\left( - \frac{1}{2\sigma_{k'}^{2}} \left(Y_{i} - 
\only<1>{ \underbrace{X_{i} \beta_{k'}}_{\textsf{\color{red} prediction}} } 
\only<2>{ X_{i} \beta_{k'} }
\right)^{2} - \log \sigma_{k'}\right)
}$$

* Intuition: For the observed $(X_{i}, Y_{i})$, we ask, "how far is the predicted $X_{i}\beta_{k}$ from the observed $Y_{i}$?" 

* Sample inversely proportional to the distance from different $X_{i}\beta_{k}$

* More rigorously: We need to introduce a Lagrangian multiplier to enforce the "sum to 1" constraint, $\sum_{k} Z_{ik} = 1$.

## M-step: Maximize regression model parameters

```{r echo = FALSE}
opt <- optim_adam(list(reg.beta, reg.ln.var), lr=.1)
take.mstep <- function(z) {
    opt$zero_grad()
    Z <- nnf_one_hot(z, num_classes = K)
    llik <- log.lik.reg(X, Y, reg.beta, reg.ln.var) * Z
    loss <- torch_mean(torch_sum(-llik, dim=2))
    loss$backward()
    opt$step();
    return(torch_sum(llik)$item())
}
```

For each regression model $k$, we can optimize the parameters $\beta_{k}$ and $\sigma_{k}$. 

For example, 

$$\nabla_{\beta_{k}, \sigma_{k}} \mathcal{L} = 
\overbrace{\sum_{i=1}^{n}}^{\textsf{\color{blue} could be a minibatch}}
\underbrace{\mathbb{E}\!\left[Z_{ik}\right]}_{\textsf{from the E-step}} 
\underbrace{\nabla_{\beta_{k}, \sigma_{k}} \log p(Y_{i}|X_{i}, \beta_{k}, \sigma_{k})}_{\textsf{\color{red} local gradients}}$$

we can simply take stochastic gradient steps:

$$\beta_{k} \gets \beta_{k} + \rho \nabla_{\beta_{k}} \mathcal{L}$$

$$\sigma_{k} \gets \sigma_{k} + \rho \nabla_{\sigma_{k}} \mathcal{L}$$


```{r run_em_regresssion_mixture, echo = FALSE}
llik.trace <- c()
Z.trace <- data.table()

.grad <- torch_zeros_like(reg.beta$t())
beta.trace <- data.table(beta=as.numeric(reg.beta$t()),
                         grad=as.numeric(.grad),
                         k = 1:ncol(reg.beta),
                         t=0)

for(tt in 1:100){
    rand.idx <- take.estep()
    llik <- take.mstep(rand.idx)
    
    z.dt <- data.table(k=as.integer(rand.idx), i=1:nrow(X), t=tt)
    Z.trace <- rbind(Z.trace, z.dt)
    llik.trace <- c(llik.trace, llik)

    beta.dt <- data.table(beta=as.numeric(reg.beta$t()),
                          grad=as.numeric(reg.beta$grad$t()),
                          k=1:ncol(reg.beta),
                          t=tt)
    
    beta.trace <- rbind(beta.trace, beta.dt)
}
```

## EM algorithm for a mixture of regression models

```{r fig.width=4, fig.height=3, echo = FALSE}
.dt <- as.dt(llik.trace, "llik") %>%
    mutate(iter = 1:n())

.gg.plot(.dt, aes(iter, llik)) +
    ylab("log-likelihood") +
    geom_smooth(se=FALSE, colour = "red", lty = 2, size=.5) +
    geom_point(pch=21, stroke = .5, size = .5, fill = "white")
```

## EM algorithm for a mixture of regression models

```{r include = FALSE}
show.reg.trace <- function(t.max){

    .dt <- data.frame(x = as.numeric(X),
                      y = as.numeric(Y),
                      Z.trace[`t` == t.max])

    .beta <- beta.trace[`t` == t.max, ]

    .aes <- aes(slope=`beta`- sign(`grad`)*pmin(10*abs(`grad`),1),
                intercept=0)

    .gg.plot(.dt, aes(x, y, fill=as.factor(k))) +
        facet_grid(.~as.factor(k)) +
        ggtitle(str_c("EM Iter = ", t.max)) +
        geom_point(pch=21, alpha=.75, stroke=.2) +
        geom_abline(aes(slope=`beta`, intercept=0),
                    data=.beta, size=.5, colour="gray") +
        geom_abline(.aes, data=.beta, size=.5, lty=2, colour="red") +
        theme(axis.title = element_blank()) +
        scale_colour_brewer("", palette = "Paired", guide = "none") +
        scale_fill_brewer("", palette = "Paired", guide = "none")
}
```

```{r fig.width=5.5, fig.height=3, echo=F, only.plot=1}
show.reg.trace(1)
```

```{r fig.width=5.5, fig.height=3, echo=F, only.plot=2}
show.reg.trace(5)
```

```{r fig.width=5.5, fig.height=3, echo=F, only.plot=3}
show.reg.trace(10)
```

```{r fig.width=5.5, fig.height=3, echo=F, only.plot=4}
show.reg.trace(15)
```

```{r fig.width=5.5, fig.height=3, echo=F, only.plot=5}
show.reg.trace(20)
```

```{r fig.width=5.5, fig.height=3, echo=F, only.plot=6}
show.reg.trace(25)
```

```{r fig.width=5.5, fig.height=3, echo=F, only.plot=7}
show.reg.trace(30)
```

## A Mixture of Regression models

- Divide and conquer

- What will be a potential application of this approach?

- Can we apply the same inference algorithm for non-linear regression models?

- What is the benefit of modelling a mixture of regressions models, as opposed to modelling a mixture of densities?



# Variable selection

## Revisit a multivariate liner regression problem

$$
\left(\begin{array}{l}
y_{1}\\
y_{2}\\
\vdots\\
y_{n}
\end{array}\right)
=
\theta_{1} \left(\begin{array}{l}
X_{11}\\
X_{21}\\
\vdots\\
X_{n1}
\end{array}\right) +
\cdots
\theta_{p} \left(\begin{array}{l}
X_{1p}\\
X_{2p}\\
\vdots\\
X_{np}
\end{array}\right).
$$

- Different number of variables will define a class of potential models

- E.g., $\mathcal{F}_{1}$: a class of models with one variable

- $\mathcal{F}_{2}$: a class of models with two variables

- (...)

- $\mathcal{F}_{q}$: a class of models with $q$ variables


## Challenges in a $p \gg n$ regression problem in genomics

```{r include = FALSE}
dir.create("Data/genotype/", recursive=TRUE, showWarnings=FALSE)
genotype.file <- "Data/genotype.rds"

if.needed(genotype.file, {

    library(bigsnpr)
    .bed.file <- "Data/genotype/1000G_phase3_common_norel.bed"
    if(!file.exists(.bed.file)){
        download_1000G("Data/genotype/")
    }
    .bk.file <- "Data/genotype/1000G_phase3_common_norel.rds"
    if(!file.exists(.bk.file)){
        BED <- snp_readBed(.bed.file)
    }
    data <- snp_attach(.bk.file)

    ntot <- nrow(data$genotypes)
    .train.ind <- sample(ntot, 1000)
    .test.ind <- setdiff(1:ntot, .train.ind)
    X <- data$genotypes[.train.ind, 1:2000]

    saveRDS(X, file = genotype.file)
})

X <- readRDS(genotype.file)
```

```{r echo = FALSE}
#' @param X
#' @param h2
#' @param n.causal
#' @param n.traits
simulate.pgs <- function(X, h2, n.causal, n.traits = 1) {
    .rnorm <- function(d1, d2) matrix(rnorm(d1*d2), d1, d2)
    causal.snp <- sample(ncol(X), n.causal)
    xx.causal <- scale(X[, causal.snp, drop=FALSE])
    xx.causal[is.na(xx.causal)] <- 0

    n.ind <- nrow(X)
    y.true <- scale(xx.causal %*% .rnorm(n.causal, n.traits))
    y.err <- scale(.rnorm(n.ind, n.traits))
    y.obs <- y.true * sqrt(h2) + y.err * sqrt(1 - h2)

    list(y = y.obs, causal = causal.snp)
}
```

```{r echo = FALSE}
set.seed(1)
sim <- simulate.pgs(X, h2=0.4, n.causal=10)
X <- scale(X)
y <-  sim$y

## sort individuals for visualization
o <- order(y)
y <- y[o, , drop = FALSE]
X <- X[o, , drop = FALSE]
```


:::::: {.columns}
::: {.column width=.45}

```{r fig.width=2.5, fig.height=1.5, echo=F}
ggplot(data.table(y, i = 1:nrow(y)), aes(i, y)) +
    geom_point(size=.5) +
    xlab("samples")
```

```{r echo = TRUE, results = "asis"}
dim(y)
```

:::
::: {.column width=.45}


```{r fig.width=3, fig.height=1.5, echo=F}
p1 <-
    .matshow(X[1:20, 1:50], .size=0, .lab=0) +
    ggtitle(nrow(X) %&% " x " %&% ncol(X))

p0 <- ggplot() + theme_void() + geom_text(aes(x=0,y=0,label="..."), size=5)

wrap_plots(p1, p0, p0, p0, nrow = 2, widths=c(3,1), heights=c(4,1))
```

```{r echo = TRUE, results = "asis"}
dim(X)
```

:::
::::::

There are `r length(sim$causal)` true non-zero variables.

## True causal variables explain a large fraction of variation

```{r echo = TRUE}
.lm <- lm(y ~ X[, sim$causal, drop = FALSE] - 1)
```

```{r fig.width = 4, fig.height = 2.8, echo=F}
.dt <- data.table(y = as.numeric(y), y.hat = fitted(.lm))
.gg.plot(.dt, aes(y.hat, y)) +
    geom_point(color = "gray40", size = .3) +
    geom_abline(slope = 1, color = "red", lty = 2)
```

## Variant-by-variant correlations

```{r fig.width=5.5, fig.height=2, echo=F}
.df <- reshape2::melt(X[, sim$causal, drop = FALSE], value.name="x") %>%
    left_join(data.frame(Var1 = 1:nrow(y), y = as.numeric(y)))

.gg.plot(.df, aes(as.factor(x), y)) +
    xlab("x") +
    facet_wrap(~Var2, scales = "free", nrow = 2) +
    geom_boxplot(outlier.size=0, outlier.stroke=0) +
    theme(strip.text = element_blank()) +
    theme(axis.text.x = element_blank())+
    theme(strip.background = element_blank())
```

## How do we know "causal" variables from `r num.int(ncol(X))` variables?

* Let's try out one by one and rank them by univariate

\Large

`cor.test(x,y)`

\normalsize

```{r echo=FALSE}
univar.dt <-
    apply(X, 2, function(x) cor.test(x, sim$y)$p.value) %>%
    (function(p) data.table(pval = p)) %>%
    mutate(i = 1:n()) %>%
    mutate(l = as.integer(i %in% sim$causal)) %>%
    arrange(pval) %>%
    mutate(r = 1:n()) %>%
    as.data.table
```

```{r echo=FALSE}
plt.pvalue.upto <- function(pp){
    .aes <- aes(as.integer(r), -log10(pval), fill=as.factor(l))
    .dt <- univar.dt[1:pp,] %>% arrange(l)
    .gg.plot(.dt, .aes) +
        xlab("variables ranked by p-value") +
        theme(legend.position = c(1,1)) +
        theme(legend.justification = c(1,1)) +
        geom_point(pch=21) +
        scale_fill_manual("causal", values =c("gray", "red")) +
        scale_y_continuous("p-value", labels=function(x)num.sci(10^(-x)))
}
```


```{r fig.width=2, fig.height=2, only.plot="2", echo=F}
plt.pvalue.upto(10)
```

```{r fig.width=3, fig.height=2, only.plot="3", echo=F}
plt.pvalue.upto(50)
```

```{r fig.width=4, fig.height=2, only.plot="4", echo=F}
plt.pvalue.upto(150)
```

```{r fig.width=5, fig.height=2, only.plot="5", echo=F}
plt.pvalue.upto(250)
```

```{r fig.width=5.5, fig.height=2, only.plot="6", echo=F}
plt.pvalue.upto(500)
```

## Classical statistics does not really help 

\large

- Classical variable selection by univariate (one-by-one) tests will not work for a $p \gg n$ regression problem


- Especially if we have col-linearity in the design matrix $X$

\normalsize

## Variable selection in high-dimensional genotype matrix ($n \ll p$)

Regression analysis = projecting the observed $\mathbf{y}$ vector on to
column space of $\{\mathbf{x}_{j}: j \in[p]\}$,
$$
\left(\begin{array}{l}
y_{1}\\
y_{2}\\
\vdots\\
y_{n}
\end{array}\right)
=
\theta_{1} \left(\begin{array}{l}
X_{11}\\
X_{21}\\
\vdots\\
X_{n1}
\end{array}\right) +
\cdots
\theta_{p} \left(\begin{array}{l}
X_{1p}\\
X_{2p}\\
\vdots\\
X_{np}
\end{array}\right).
$$
Variable selection = column selection.

\vfill

\begin{itemize}
\item <1-> Intuitive idea : choose the best combination of variables. $\to 2^{p}$ choices (even harder).
\item <2-> Alternative idea : make as many $\theta_{j}$'s nearly zero values.
\item <3-> What prior does: penalize $|\theta_{j}| > 0$ so that only the strong
  enough variables take non-zero values.
\end{itemize}

## Reconciling two related concepts -- MLE and MSE

Equivalence of maximum-likelihood estimation and mean square error
minimization (isotropic Gaussian error distribution).

\vfill

__[MLE]__ Find ${\boldsymbol{\theta}}$ maximizing
$$
\ln p(\mathbf{y}|X,\boldsymbol{\theta})
= - \frac{1}{2\sigma^{2}} \sum_{i=1}^{n} (y_{i} - \mathbf{x}_{i} \boldsymbol{\theta})^{2} + \textrm{const.}
$$
_without prior contribution of parameter, and $\sigma$ is known._

\vfill

__[MSE]__ Find $\boldsymbol{\theta}$ minimizing
$$
\sum_{i=1}^{n} (y_{i} - \mathbf{x}_{i} \boldsymbol{\theta})^{2}.
$$

\vfill

## MLE, MSE, an optimization problem

Minimization of the convex loss function:
$$
L(\boldsymbol{\theta}) = (\mathbf{y} - X\boldsymbol{\theta})^{\top}(\mathbf{y} - X\boldsymbol{\theta})
$$

\onslide<2->{We can optimize setting the derivative with respect to $\boldsymbol{\theta}$ to zero:
$$
\nabla_{\boldsymbol{\theta}} L = X^{\top} (\mathbf{y} - X\boldsymbol{\theta}) = 0
$$
Rearranging the equation
$$
\mathbf{y}^{\top} X = X^{\top} X \boldsymbol{\theta} \implies
\hat{\boldsymbol{\theta}}_{MLE} = (X^{\top} X)^{-1} X^{\top} \boldsymbol{y}.
$$
}

\vfill

* \onslide<3->{Approximately, $p(\boldsymbol{\theta} | \mathbf{y}, X) \approx \mathcal{N}\!\left(\boldsymbol{\theta} \middle| \hat{\boldsymbol{\theta}}_{MLE}, \sigma^{2} (X^{\top} X)^{-1} \right)$.}

* \onslide<4->{How hard is $(X^{\top} X)^{-1}$ (i.e., inverse of ${p\times p}$ matrix)?}

* \onslide<5->{What if $n \ll p$?  What if we want to include $p(\boldsymbol{\theta})$?}

## Bayesian/regularization idea to add the missing probability component

\large

We've been discussing the conditional likelihood

$$p(\mathbf{y}|X,\boldsymbol{\theta})$$

without a prior probability of regression coefficients,

$${\color{blue} p(\boldsymbol{\theta})}$$

**What will be a suitable prior distribution of $\boldsymbol{\theta}$?**

## Recall: Reconciling two related concepts -- MLE and MSE

Equivalence of maximum-likelihood estimation and mean square error
minimization (isotropic Gaussian error distribution).

\vfill

__[MLE]__ Find ${\boldsymbol{\theta}}$ maximizing
$$
\ln p(\mathbf{y}|X,\boldsymbol{\theta})
= - \frac{1}{2\sigma^{2}} \sum_{i=1}^{n} (y_{i} - \mathbf{x}_{i} \boldsymbol{\theta})^{2} + \textrm{const.}
$$
_without prior contribution of parameter, and $\sigma$ is known._

\vfill

__[MSE]__ Find $\boldsymbol{\theta}$ minimizing
$$
\sum_{i=1}^{n} (y_{i} - \mathbf{x}_{i} \boldsymbol{\theta})^{2}.
$$

## Ridge regression, a linear regression with Gaussian prior (L2)

Prior distribution
$$
p(\boldsymbol{\theta}) = \mathcal{N}\!\left(\boldsymbol{\theta}|\mathbf{0}, \lambda^{-1} I\right) \propto \exp\left(-\frac{\lambda}{2}\|\boldsymbol{\theta}\|^{2}\right)
$$
where
\large
$$\|\boldsymbol{\theta}\|^{2} = \sum_{j=1}^{p} \theta_{j}^{2},\,\textsf{\color{blue}L2-norm}.$$

\normalsize
Maximize
$$
\ln p(\mathbf{y}|X,\boldsymbol{\theta}) + \ln p(\boldsymbol{\theta}|\lambda)
= - \frac{1}{2\sigma^{2}} \sum_{i=1}^{n} (y_{i} - \mathbf{x}_{i} \boldsymbol{\theta})^{2}
- \frac{\lambda}{2} \|\boldsymbol{\theta}\|^{2}
$$

Minimize $L_{2}$-regularized error
$$
\sum_{i=1}^{n} (y_{i} - \mathbf{x}_{i} \boldsymbol{\theta})^{2}
+ \frac{\lambda}{2} \|\boldsymbol{\theta}\|^{2}
$$

## Lasso regression, a linear regression with Laplace prior (L1)

Prior distribution
$$
p(\boldsymbol{\theta}) = \textsf{Laplace}(\boldsymbol{\theta}| \lambda) \propto \exp\left(-\lambda\|\boldsymbol{\theta}\|_{1}\right)
$$
where
\large
$$\|\boldsymbol{\theta}\|_{1} = \sum_{j=1}^{p} |\theta_{j}|,\,\textsf{\color{blue}L1-norm}.$$

\normalsize
Maximize
$$
\ln p(\mathbf{y}|X,\boldsymbol{\theta}) + \ln p(\boldsymbol{\theta}|\lambda)
= - \frac{1}{2\sigma^{2}} \sum_{i=1}^{n} (y_{i} - \mathbf{x}_{i} \boldsymbol{\theta})^{2}
- \lambda \|\boldsymbol{\theta}\|_{1}
$$

Minimize $L_{1}$-regularized error
$$
\sum_{i=1}^{n} (y_{i} - \mathbf{x}_{i} \boldsymbol{\theta})^{2}
+ \lambda \|\boldsymbol{\theta}\|_{1}
$$

\vfill

(Tibshirani, 1996)

## Posterior inference of the regularized regression models

Our goal is to estimate (1) posterior distribution
$$
p(\boldsymbol{\theta}|\mathbf{y}, X) = \frac{p(\mathbf{y}|X,\boldsymbol{\theta})p(\boldsymbol{\theta})}{p(\boldsymbol{y}|X)}.
$$

Then (2) using $p(\boldsymbol{\theta}|\mathbf{y},X)$, predict
$p(\mathbf{y}^{\star}|\mathbf{y},X)$ by averaging over all possible
$\boldsymbol{\theta}$ sampled from the estimated posterior
distribution.

\vfill

* Usually posterior prediction (2) is can be easily simulated with
  accurate estimation of posterior distribution (1).

* Posterior inference can be done analytically or not, depending on
  the choice of $p(\boldsymbol{\theta})$\footnote{We term prior $p(\boldsymbol{\theta})$ a {\it conjugate prior} if its posterior $p(\boldsymbol{\theta}|\mathbf{y},X)$ is of the same type of distribution.}.

## We can find an analytical solution in L2-regularized regression

$$
\ln p(\mathbf{\boldsymbol{\theta}}|\mathbf{y},X) =
-\frac{1}{2\sigma^{2}} (\mathbf{y} - X\boldsymbol{\theta})^{\top}(\mathbf{y} - X\boldsymbol{\theta}) - \frac{\lambda}{2} \boldsymbol{\theta}^{\top}\boldsymbol{\theta} + \textrm{const.}
$$

\onslide<2->{
By taking derivative with respect to $\boldsymbol{\theta}$ and setting it the zero vector:
$$
\nabla_{\boldsymbol{\theta}} = -\frac{1}{\sigma^{2}} X^{\top}(\mathbf{y} - X\boldsymbol{\theta})
-\lambda \boldsymbol{\theta} = 0
$$
}

\vfill

\onslide<3->{
Rearranging the equation:
$$
X^{\top}\mathbf{y} = (X^{\top}X + \lambda\sigma^{2} I) \boldsymbol{\theta}\,
\implies\,
\hat{\boldsymbol{\theta}} = (X^{\top}X + \lambda\sigma^{2} I)^{-1} X^{\top}\mathbf{y}
$$
}

\onslide<4>{
\textit{Remark}: For $n \ll p$, the inverse $(X^{\top}X)^{-1}$ may not exists, but $(X^{\top}X + \lambda\sigma^{2}I)^{-1}$ can exist with a proper $\lambda$.
}


## For L1/L2 constraints, the greedy algorithm of `glmnet` works so well

Goal:

$$
\min_{\boldsymbol{\theta}} \quad
\overbrace{(\mathbf{y} - X\boldsymbol{\theta})^{\top}(\mathbf{y} - X\boldsymbol{\theta})}^{\textsf{\color{blue} RSS}} + \underbrace{\lambda \alpha \|\boldsymbol{\theta}\|_{1}}_{\textsf{\color{red} variable selection}} + \underbrace{\lambda (1 - \alpha) \|\boldsymbol{\theta}\|_{2}}_{\textsf{\color{magenta} shrinkage}}
$$

\onslide<2->{
The variable-by-variable update equation makes sense:

For each $\theta_{j}$,
}

\only<2>{$$
\hat{\theta}_{j}^{\textsf{glmnet}} \gets
\frac{S\left(
\sum_{i=1}^{n} X_{ij} (y_{i} - \hat{y}_{i}^{(-j)}),
\lambda\alpha
\right)}
{ \sum_{i=1}^{n} X_{ij}^{2} +\lambda (1- \alpha) }
\quad\textsf{vs.}\quad
\theta_{j}^{\textsf{MLE}} \gets
\frac{
\sum_{i=1}^{n} X_{ij} \left(
y_{i} - \sum_{k\neq j} X_{ik}\hat{\theta}_{k}
\right)}
{\sum_{i=1}^{n} X_{ij}^{2}}
$$
\tiny
Friedman {\it et al.}, Regularization Paths for Generalized Linear Models via Coordinate Descent (2010)
}

\only<3>{$$
\hat{\theta}_{j} \gets
\frac{\overset{\textsf{\color{red} threshold}}{S}
\left(
\sum_{i=1}^{n} X_{ij} \overbrace{(y_{i} - y_{i}^{(-j)})}^{\textsf{\color{red} residual w/o the variable } \theta_{j} },
\lambda\alpha
\right)}
{ \sum_{i=1}^{n} X_{ij}^{2} + \underbrace{\lambda (1- \alpha)}_{\textsf{\color{magenta} shrinkage}}}
$$
where $S(z, \tau)$ will set it to zero if $|z| < \tau$.
}

## Cross-validation: How do we tune hyper-parameters (e.g., $\lambda$)?

1. Divide the total training data $\mathcal{D}^{\textsf{train}} = \{(X,y)\}$ into two parts:

    * (1) cross-validation training $\{(X,y)\}$ and
	* (2) CV testing data $\{(X^\star,y^\star)\}$

2. For each different $(\lambda, \alpha)$ combination,

    - Train coefficients $\theta$ using CV training $\{(X,y)\} \subset \mathcal{D}^{\textsf{train}}$

	- Test how well $\sum_{j} X^{\star}_{ij} \hat{\theta}_{j}$ predicts $y^\star$?

3. Choose the optimal $(\lambda^\star, \alpha^\star)$


## How do we tune hyper-parameters (e.g., $\lambda$)?

Well, in `R`, we simply run

:::::: {.columns}
::: {.column width=.35}

```{r echo=TRUE, size="large"}
glm.cv.out <-
 glmnet::cv.glmnet(X,
          y,
          nfolds=5,
          alpha=1)
```

:::
::: {.column width=.55}

```{r fig.width=3, fig.height=2.5, only.plot="2", echo=F}
plot(glm.cv.out)
```

:::
::::::


## Revisit our working example with L1-regularization (`glmnet`)

```{r fig.width=5, fig.height=3.5, echo=FALSE}
.out <- glmnet::glmnet(X,y,alpha=1)
plot(.out)
```

## At the optimal $\lambda$ value found by `cv.glmnet`...

```{r echo = FALSE}
lambda.cv <- glm.cv.out$lambda.min
.out <- glmnet::glmnet(x=X, y=y, lambda = lambda.cv)

lasso.dt <-
    data.table(beta = as.numeric(.out$beta)) %>%
    mutate(i = 1:n()) %>%
    mutate(l = as.integer(i %in% sim$causal)) %>%
    left_join(univar.dt) %>%
    arrange(desc(abs(beta))) %>%
    mutate(lasso = 1:n()) %>%
    as.data.table
```

```{r fig.width=5, fig.height=3, echo = FALSE}
.gg.plot(lasso.dt, aes(lasso, beta, fill=as.factor(l))) +
    theme(legend.position = c(1,1)) +
    theme(legend.justification = c(1,1)) +
    xlab("variables (ranked by glmnet)") + ylab("coefficients") +
    geom_point(data=head(lasso.dt, 100), pch=21) +
    geom_hline(yintercept = 0, color="black") +
    scale_fill_manual("causal", values=c("gray", "red"))
```

## Bias-variance tradeoff explains why a regularized regression works in practice

\large

$$
\min_{\boldsymbol{\theta}} \quad
\overbrace{(\mathbf{y} - X\boldsymbol{\theta})^{\top}(\mathbf{y} - X\boldsymbol{\theta})}^{\approx \textsf{\color{blue} bias}} + 
\underbrace{\lambda \alpha \|\boldsymbol{\theta}\|_{1}}_{\textsf{\color{red} variable selection}} + 
\underbrace{\lambda (1 - \alpha) \|\boldsymbol{\theta}\|_{2}}_{\textsf{\color{magenta} shrinkage}}
$$

* The second and the third terms control the model variance


## Sum of Single Effect (SuSiE) regression

```{r run_susie, size="large"}
library(susieR)
susie.out <- susie(X, y)
```

\vfill

\large
$$\mathbf{y} = \sum_{l=1}^{L} \underbrace{ \sum_{j} \mathbf{x}_{j} 
    \overset{\textsf{\color{magenta}probabilistic selection}}{\alpha_{j}} 
	\overset{\textsf{\color{teal} single variant effect}}{\beta_{j}} }_{\textsf{layer-by-layer}}  + \boldsymbol{\epsilon}$$

\normalsize
where $\sum_{j=1}^{p} \alpha_{j} = 1$.

\vfill

Wang .. Stephens, _Journal of the Royal Statistical Society_ (2020)

## SuSiE identifies top causal variants

```{r echo = FALSE}
susie.dt <- 
    data.table(pip = susie_get_pip(susie.out)) %>%
    mutate(i = 1:n()) %>% 
    left_join(univar.dt) %>%
    mutate(l = as.integer(i %in% sim$causal)) %>%
    arrange(desc(pip)) %>%
    mutate(susie = 1:n())
```

```{r fig.width=5, fig.height=3, echo = FALSE}
.gg.plot(susie.dt, aes(susie, pip, fill=as.factor(l))) +
    theme(legend.position = c(1,1)) +
    theme(legend.justification = c(1,1)) +
    xlab("variables (ranked by SuSiE)") +
    ylab("posterior probability") +
    geom_point(data=head(susie.dt, 100), pch=21) +
    geom_hline(yintercept = 0, color="black") +
    scale_fill_manual("causal", values=c("gray", "red"))
```

## SuSiE can avoid the col-linearity problem

```{r fig.width=5, fig.height=2, echo = FALSE, only.plot="1"}
logit <- function(x) log(x) - log(1-x)
sigmoid <- function(x) 1/(1+exp(-x))

.aes <- aes(i, logit(pmin(pmax(pip,1e-2),1-1e-2)), fill=as.factor(l))

p1 <-
    .gg.plot(susie.dt[order(`pip`)], .aes) +
    theme(legend.position = c(1,1)) +
    theme(legend.justification = c(1,1)) +
    xlab("variables (along genomic axis)") +
    scale_y_continuous("posterior probability", labels=function(x) round(sigmoid(x),2)) +
    geom_vline(data=susie.dt[pip > .5], aes(xintercept=i), lty=2, size=.5, color=3) +
    geom_point(pch=21, stroke=0) +
    geom_hline(yintercept = 0, color="black", lty = 2, size = .2) +
    scale_fill_manual("causal", values=c("gray", "red"))
p1
```

```{r fig.width=5, fig.height=3, echo = FALSE, only.plot="2"}
p2 <-
    .gg.plot(susie.dt[order(`l`)], aes(i, -log10(pval), fill = as.factor(l))) +
    geom_vline(data=susie.dt[pip > .5], aes(xintercept=i), lty=2, size=.5, color=3) +
    geom_point(pch=21) +
    scale_fill_manual("causal", values=c("gray", "red"), guide=FALSE) +
    xlab("variables (along genomic axis)")
p1/p2
```

```{r fig.width=6, fig.height=3.5, echo = FALSE, only.plot="3"}
susie_plot(susie.out, y = "PIP", add_bar = TRUE, add_legend = TRUE)
```

```{r fig.width=6, fig.height=3.5, echo = FALSE, only.plot="4"}
susie_plot(susie.out, y = "log10PIP", add_legend = TRUE)
```

