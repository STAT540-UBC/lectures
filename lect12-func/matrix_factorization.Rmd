---
title: "Matrix Factorization"
author: "Yongjin Park"
classoption: "aspectratio=169"
---

```{r setup, include=FALSE, message=FALSE}
set.seed(1447)
library(tidyverse)
library(data.table)
library(patchwork)
source("Util.R")
source("Setup.R")
fig.dir <- "Fig/"
setup.env(fig.dir, .echo=FALSE)
dir.create("Data", showWarnings=FALSE)
```

## A toy example: Can we uncover hidden structures in a gene expression matrix?

:::::: {.columns}
::: {.column width=.5}


```{r echo=FALSE}
U <- .rnorm(100, 2); V <- .rnorm(100, 2)
X <- U %*% t(V) + .rnorm(100, 100)
```

Suppose the data matrix (gene expression data) were generated by the following:

$$\mathbf{x}_{i} = \mathbf{u}_{i} V^{\top} + \epsilon_{i}$$

for all $i$ with $\epsilon_{i} \sim \mathcal{N}\!\left(\mathbf{0},I\right)$

```{r echo=FALSE}
X <- .sort.matrix(X)
```

```{r echo=FALSE, fig.width=2.5, fig.height=2, onslide.plot="1-"}
.matshow(X, .lab=0, .size=0)
```

:::
::: {.column width=.5}

```{r echo=FALSE}
.svd <- svd(X, nu=10, nv=10)
u.hat <- sweep(.svd$u, 2, .svd$d[1:2], `*`)
```

Can we reverse engineer the $U$ and $V$ matrices from the data $X$?

$$X \to U V^{\top}$$

```{r echo=FALSE, fig.width=2.7, fig.height=2.3, only.plot="2"}
p1 <- .matshow(u.hat, .lab=0, .size=0) +
    ggtitle("features")
p.times <- ggplot() + theme_void() +
    geom_text(aes(0,0,label="X"), size=5)
p2 <- wrap_plots(.matshow(t(.svd$v), .lab=0, .size=0),
                 ggplot() + theme_void(),
                 ncol=1, heights=c(1,9)) +
    ggtitle("feature loading")
wrap_plots(list(p1, p.times, p2), nrow=1, widths=c(1,2,8))
```

:::
::::::


# Principal Component Analysis

## Working example: GSE107011

```{r include=FALSE}
## if (!require("BiocManager", quietly = TRUE)){
##     install.packages("BiocManager")
## }
## if(!require("GEOquery", quietly = TRUE)){
##     BiocManager::install("GEOquery")
## }
raw.data.file <- "GSE107011/GSE107011_Processed_data_TPM.txt.gz"
if(!file.exists(raw.data.file)){
    GEOquery::getGEOSuppFiles("GSE107011")
}
raw.data <- fread(raw.data.file)
X <- as.matrix(raw.data[, -1])
rownames(X) <- unlist(raw.data[,1])
.sd <- apply(X, 1, sd, na.rm=TRUE)
msv <- order(.sd, decreasing = TRUE)[1:200]
```

Top 200 most variable genes in the data matrix:

:::::: {.columns}
::: {.column width=.25}

```{r fig.width=1, fig.height=2.5, only.plot="1"}
x.sub <- X[msv, , drop = FALSE]
.matshow(x.sub, .scale=TRUE, .lab=0, .size=0)
```

```{r fig.width=1, fig.height=2.5, only.plot="2"}
x.sub <- t(scale(t(log(1 + X[msv, , drop = FALSE]))))
.matshow(x.sub, .scale=TRUE, .lab=0, .size=0)
```

:::
::: {.column width=.65}

* We will call such a high-dimensional matrix $X$ ($m \times n$)

* X has $m$=`r num.int(nrow(X))` rows (transcripts/genes/features)

* X has $n$=`r num.int(ncol(X))` columns (samples/#data points)

* The rows were log-transformed and scaled by `scale()` for visualization

* Each sample is a `r num.int(nrow(X))`-dimensional vector!

* Each gene is a `r num.int(ncol(X))`-dimensional vector...

:::
::::::


## 1-dimensional representations/summary of data matrix

:::::: {.columns}
::: {.column width=.25}

Top 200 genes:

```{r fig.width=1, fig.height=2.5}
x.sub <- X[msv, , drop = FALSE]
.matshow(x.sub, .scale=FALSE, .lab=0, .size=0)
```

:::
::: {.column width=.65}

```{r fig.width=3.5, fig.height=1.25, onslide.plot="1-"}
.gg.plot(as.data.table(x.sub[1,]),
         aes(x=colnames(x.sub), y=V1)) +
    ggtitle("Each sample (column):") +
    ylab("expression") +
    xlab("samples") +
    theme(axis.text.x = element_blank()) +
    geom_point(pch=21)
```

```{r fig.width=4, fig.height=1.25, onslide.plot="2"}
.gg.plot(as.data.table(X[,1]),
         aes(x=1:nrow(X), y=V1)) +
    ggtitle("Each gene (row):") +
    ylab("expression") +
    xlab("genes") +
    geom_point(pch=21, size=.2, alpha=.5)
```

:::
::::::


## Can you see some common patterns?

:::::: {.columns}
::: {.column width=.45}

```{r fig.width=2, fig.height=2.5, onslide.plot="1-"}
x.sub <- t(scale(t(log(1 + X[msv, , drop = FALSE]))))
.matshow(x.sub, .scale=TRUE, .lab=0, .size=0) +
    ggtitle("Top 200 genes")
```

:::
::: {.column width=.45}

```{r fig.width=2, fig.height=2.5, onslide.plot="2"}
x.sorted <- .sort.matrix(x.sub)
.matshow(x.sorted, .scale=TRUE, .lab=0, .size=0) +
    ggtitle("Rearrange them...")
```

:::
::::::

## PCA: How do we recover "common" patterns from data?

Principal Component Analysis

:::::: {.columns}
::: {.column width=.45}

::: {.block}

### (Pearson 1901)

* Projection [of the original data] that minimizes the projection cost between the original and projected

* The cost = mean squared distance

:::

:::
::: {.column width=.45}

::: {.block}

### (Hotelling 1933)

* Orthogonal projection of data into a lower-dimensional **[principal]** sub-space,

* such that the total **variation of the projected** is maximized

:::

:::
::::::

## What do we mean by projection to a low-dimensional space?

```{r}
x.sub <- t(scale(t(log(1 + X[msv, , drop = FALSE]))))
x.sub[is.na(x.sub)] <- 0
x.sub <- .sort.matrix(x.sub)
```

**Recap:** We learned that a multivariate linear regression projects a data vector $\mathbf{x}$ onto the column space of the design matrix $U$:

$$\left(\begin{array}{l}
X_{1}\\
X_{2}\\
\vdots \\
X_{m}\\
\end{array}
\right) = 
\left(
\begin{array}{l l l}
U_{11} & \ldots & U_{1k}\\
U_{21} & \ldots & U_{2k}\\
\vdots & \vdots & \vdots \\
U_{m1} & \ldots & U_{mk}\\
\end{array} 
\right)
\left(
\begin{array}{l}
W_{1}\\
\vdots\\
W_{k}
\end{array}
\right)
+ 
\left(
\begin{array}{l}
\epsilon_{1}\\
\epsilon_{2}\\
\vdots\\
\epsilon_{m}
\end{array}
\right)$$

or

$\mathbf{x} = U \mathbf{w} + \epsilon$, for many columns, $X = U W + E$.

* If we **knew** $U$, we would be able to solve weights $W$. If $k = 2$, it would result in a projection to the 2D space.

* Unlike regression, we need ask: How do we know this unknown $U$ matrix?

## What is a projection matrix?

The purpose of a multivariate regression-based prediction:

$$\mathbf{x} \to \hat{\mathbf{x}} = U \mathbf{w}$$

The least-square solution for the $W$:

$$\hat{\mathbf{w}} = (U^{\top}U)^{-1} U^{\top}\mathbf{x}$$

Then we have

$$\hat{\mathbf{x}} = \underbrace{U (U^{\top}U)^{-1} U^{\top}}_{\textsf{\color{red} projection matrix}} \mathbf{x}$$

You can think of "projection" as **"prediction" in the linear space** defined by the columns of a design matrix.

## What will be "good" features (the $U$ matrix) to regress on?

:::::: {.columns}
::: {.column width=.35}

```{r fig.width=2, fig.height=2.5, only.plot="1"}
.matshow(x.sub, .scale=TRUE, .lab=0, .size=0) +
    ggtitle("Top 200 genes")
```

```{r}
U <- matrix(1, nrow(x.sub), 1)
.lm <- lm(x.sub ~ U -1)
```

```{r fig.width=2, fig.height=2.5, only.plot="2"}
.matshow(.lm$fitted.values, .lab=0, .size=0) +
    ggtitle("one proj.")
```

```{r fig.width=2, fig.height=2.5, only.plot="3"}
.matshow(x.sub, .scale=TRUE, .lab=0, .size=0) +
    ggtitle("Top 200 genes")
```

:::
::: {.column width=.05}

:::
::: {.column width=.65}

```{r fig.width=3.2, fig.height=2.7, only.plot="2"}
p0 <- ggplot() + theme_void()
p.eq <- ggplot() + theme_void() + geom_text(aes(0,0,label="="), size=10)
p.prod <- ggplot() + theme_void() + geom_text(aes(0,0,label="x"), size=10)
p1 <- .matshow(U, .lab=0, .size=0, .scale=FALSE) +
    ggtitle("1's")
p2 <- .matshow(matrix(.lm$coefficients, nrow=1),
               .lab=0, .size=0, .scale=TRUE) +
    ggtitle("how much contribution?")
p2 <- wrap_plots(p2, p0, ncol=1, heights = c(1,13))
wrap_plots(p.eq, p0, p1, p0, p.prod, p2, nrow=1, widths=c(1, 1, 1, 1, 1, 11))
```

```{r fig.width=2, fig.height=2.5, only.plot="3"}
var.exp <- mean(apply(.lm$fitted, 2, var) / apply(x.sub, 2, var))
.matshow(.lm$fitted.values, .lab=0, .size=0) +
    ggtitle(ceiling(var.exp*100) %&% "% var. explained")
```

:::
::::::


## How about an average vector?

:::::: {.columns}
::: {.column width=.35}

```{r fig.width=2, fig.height=2.5, only.plot="1"}
.matshow(x.sub, .scale=TRUE, .lab=0, .size=0) +
    ggtitle("Top 200 genes")
```

```{r}
set.seed(11)
U <- matrix(apply(x.sub, 1, mean), ncol=1)
.lm <- lm(x.sub ~ U -1)
```

```{r fig.width=2, fig.height=2.5, only.plot="2"}
.matshow(.lm$fitted.values, .lab=0, .size=0) +
    ggtitle("avg. proj.")
```

```{r fig.width=2, fig.height=2.5, only.plot="3"}
.matshow(x.sub, .scale=TRUE, .lab=0, .size=0) +
    ggtitle("Top 200 genes")
```

:::
::: {.column width=.65}

```{r fig.width=3.2, fig.height=2.7, only.plot="2"}
p0 <- ggplot() + theme_void()
p.eq <- ggplot() + theme_void() + geom_text(aes(0,0,label="="), size=10)
p.prod <- ggplot() + theme_void() + geom_text(aes(0,0,label="x"), size=10)
p1 <- .matshow(U, .lab=0, .size=0, .scale=TRUE) +
    ggtitle("avg")
p2 <- .matshow(matrix(.lm$coefficients, nrow=1),
               .lab=0, .size=0, .scale=TRUE) +
    ggtitle("how much contribute?")
p2 <- wrap_plots(p2, p0, ncol=1, heights = c(1,13))
wrap_plots(p.eq, p0, p1, p0, p.prod, p2, nrow=1, widths=c(1, 1, 1, 1, 1, 11))
```

```{r fig.width=2, fig.height=2.5, only.plot="3"}
var.exp <- mean(apply(.lm$fitted, 2, var) / apply(x.sub, 2, var))
.matshow(.lm$fitted.values, .lab=0, .size=0) +
    ggtitle(ceiling(var.exp*100) %&% "%")
```

:::
::::::

## Let's try out random projection

:::::: {.columns}
::: {.column width=.35}

```{r fig.width=2, fig.height=2.5, only.plot="1"}
.matshow(x.sub, .scale=TRUE, .lab=0, .size=0) +
    ggtitle("Top 200 genes")
```

```{r}
set.seed(11)
U <- matrix(rnorm(nrow(x.sub)*2), nrow(x.sub), 2)
.lm <- lm(x.sub ~ U - 1)
```

```{r fig.width=2, fig.height=2.5, only.plot="2"}
.matshow(.lm$fitted.values, .lab=0, .size=0) +
    ggtitle("random proj.")
```

```{r fig.width=2, fig.height=2.5, only.plot="3"}
.matshow(x.sub, .scale=TRUE, .lab=0, .size=0) +
    ggtitle("Top 200 genes")
```

:::
::: {.column width=.65}

```{r fig.width=3.2, fig.height=2.7, only.plot="2"}
p0 <- ggplot() + theme_void()
p.eq <- ggplot() + theme_void() + geom_text(aes(0,0,label="="), size=10)
p.prod <- ggplot() + theme_void() + geom_text(aes(0,0,label="x"), size=10)
p1 <- .matshow(U, .lab=0, .size=0, .scale=TRUE) +
    ggtitle("rand")
p2 <- .matshow(as.matrix(.lm$coefficients),
               .lab=0, .size=0, .scale=TRUE) +
    ggtitle("how much contribute?")
p2 <- wrap_plots(p2, p0, ncol=1, heights = c(1,13))
wrap_plots(p.eq, p0, p1, p0, p.prod, p2, nrow=1, widths=c(1, 1, 1, 1, 1, 11))
```

```{r fig.width=2, fig.height=2.5, only.plot="3"}
var.exp <- mean(apply(.lm$fitted, 2, var) / apply(x.sub, 2, var))
.matshow(.lm$fitted.values, .lab=0, .size=0) +
    ggtitle(ceiling(var.exp*100) %&% "%")
```

:::
::::::


## Can we find a set of "good" vectors to maximize the explained variability?

::: {.block}
### Recap: Sample covariance matrix

* Sample mean: $\bar{X}_{i} = \sum_{j=1}^{m} X_{ji} / m$

* Sample variance: $\sum_{j=1}^{m} (X_{ji} - \bar{X}_{i})^{2} / (m-1)$

* Sample covariance between $i$ and $k$: 
$$\frac{1}{m-1}\sum_{j=1}^{m} (X_{ji} - \bar{X}_{i}) (X_{jk} - \bar{X}_{k})$$

:::

If all the column vectors $\mathbf{x}_{i}$ are standardized, the column-by-column covariance $X^{\top}X / (m-1)$.

If all the row vectors $\mathbf{x}_{d}$ are standardized, the row-by-row covariance $X X^{\top} / (n-1)$.

## _Theory:_ Total variance of the projected data

### Total variance

Given the projected, $\hat{X} = \mathbf{u}_{1} \cdot (W_{11},\ldots,W_{1n})$, 
our goal is

$$\max \mathbb{V}\!\left[\hat{X}\right]$$


* We have two unknown variables $U$ and $W$

* There are an infinite number of solutions.


## _Theory:_ Total variance of the projected data

### Constrained total variance

Given the projected, $\hat{X} = \mathbf{u}_{1} \mathbf{w}$, 
and a unit vector, namely $\|\mathbf{u}_{1}\|=1$, 
our goal is equivalent to

$$\max \mathbf{u}^{\top} X X^{\top} \mathbf{u}$$

Because each $\hat{W}_{i}$ is the solution to the least-square problem:
$$\hat{W}_{i} = \arg\min \|\mathbf{x}_{i} - \mathbf{u} W_{i}\|$$
by solving the least square:
$$\hat{W}_{i} = \mathbf{x}_{i}^{\top}\mathbf{u} / \mathbf{u}^{\top}\mathbf{u},\,\forall i$$


## _Theory_: Why is Principal Component Analysis an eigen value problem?

### PCA

Letting the feature-by-feature sample covariance matrix $\hat{\Sigma} = XX^{\top}/(n-1)$, we want to find a unit vector $\mathbf{u}$ by 

$$\max \mathbf{u}^{\top} \hat{\Sigma}\mathbf{u}$$

subject to $\mathbf{u}^{\top}\mathbf{u} = 1$.

## _Theory_: Why is Principal Component Analysis an eigen value problem?

### Eigen value problem

Given the covariance matrix $\hat{\Sigma}$, we can resolve an eigen-value $\lambda$ and the corresponding eigen-vector $\mathbf{u}$ such that

$$\hat{\Sigma}\mathbf{u} = \lambda \mathbf{u}$$

("eigen" mean "own" in German).

## _Theory_: Why is Principal Component Analysis an eigen value problem?

*Why are they equivalent?* Solving the PCA problem is 

$$\iff \max \underbrace{\mathbf{u}^{\top}\hat{\Sigma}\mathbf{u}}_{\textsf{total variation}} + \underbrace{\lambda\left(1 - \mathbf{u}^{\top}\mathbf{u}\right)}_{\textsf{constraint}},\,\lambda >0\,\quad\textsf{a.k.a. Lagrangian}$$

Taking the derivative with respect to $\mathbf{u}$ and setting it to zero, we get the eigen-value problem.


## Let's see how much the first eigenvector can explain

:::::: {.columns}
::: {.column width=.35}

```{r fig.width=2, fig.height=2.5, only.plot="1"}
.matshow(x.sub, .scale=TRUE, .lab=0, .size=0) +
    ggtitle("Top 200 genes")
```

```{r}
.eigen <- eigen(x.sub %*% t(x.sub))
U <- .eigen$vector[,1,drop=FALSE]
.lm <- lm(x.sub ~ U -1)
```

```{r fig.width=2, fig.height=2.5, only.plot="2"}
.matshow(.lm$fitted.values, .lab=0, .size=0) +
    ggtitle("eigen proj.")
```

```{r fig.width=2, fig.height=2.5, only.plot="3"}
.matshow(x.sub, .scale=TRUE, .lab=0, .size=0) +
    ggtitle("Top 200 genes")
```

:::
::: {.column width=.65}

```{r fig.width=3.2, fig.height=2.7, only.plot="2"}
p0 <- ggplot() + theme_void()
p.eq <- ggplot() + theme_void() + geom_text(aes(0,0,label="="), size=10)
p.prod <- ggplot() + theme_void() + geom_text(aes(0,0,label="x"), size=10)
p1 <- .matshow(U, .lab=0, .size=0, .scale=TRUE) +
    ggtitle("e-vec")
p2 <- .matshow(matrix(.lm$coefficients, nrow=1),
               .lab=0, .size=0, .scale=TRUE) +
    ggtitle("loading")
p2 <- wrap_plots(p2, p0, ncol=1, heights = c(1,13))
wrap_plots(p.eq, p0, p1, p0, p.prod, p2, nrow=1, widths=c(1, 1, 1, 1, 1, 11))
```

```{r fig.width=2, fig.height=2.5, only.plot="3"}
var.exp <- mean(apply(.lm$fitted, 2, var) / apply(x.sub, 2, var))
.matshow(.lm$fitted.values, .lab=0, .size=0) +
    ggtitle(round(var.exp*100) %&% "%")
```

:::
::::::


## Top two eigen-vectors

:::::: {.columns}
::: {.column width=.35}

```{r fig.width=2, fig.height=2.5, only.plot="1"}
.matshow(x.sub, .scale=TRUE, .lab=0, .size=0) +
    ggtitle("Top 200 genes")
```

```{r}
U <- .eigen$vector[,1:2,drop=FALSE]
.lm <- lm(x.sub ~ U -1)
```

```{r fig.width=2, fig.height=2.5, only.plot="2"}
.matshow(.lm$fitted.values, .lab=0, .size=0) +
    ggtitle("eigen proj.")
```

```{r fig.width=2, fig.height=2.5, only.plot="3"}
.matshow(x.sub, .scale=TRUE, .lab=0, .size=0) +
    ggtitle("Top 200 genes")
```

:::
::: {.column width=.65}

```{r fig.width=3.2, fig.height=2.7, only.plot="2"}
p0 <- ggplot() + theme_void()
p.eq <- ggplot() + theme_void() + geom_text(aes(0,0,label="="), size=10)
p.prod <- ggplot() + theme_void() + geom_text(aes(0,0,label="x"), size=10)
p1 <- .matshow(U, .lab=0, .size=0, .scale=TRUE) +
    ggtitle("e-vec")
p2 <- .matshow(as.matrix(.lm$coefficients),
               .lab=0, .size=0, .scale=TRUE) +
    ggtitle("loading")
p2 <- wrap_plots(p2, p0, ncol=1, heights = c(1,13))
wrap_plots(p.eq, p0, p1, p0, p.prod, p2, nrow=1, widths=c(1, 1, 1, 1, 1, 11))
```

```{r fig.width=2, fig.height=2.5, only.plot="3"}
var.exp <- mean(apply(.lm$fitted, 2, var) / apply(x.sub, 2, var))
.matshow(.lm$fitted.values, .lab=0, .size=0) +
    ggtitle(round(var.exp*100) %&% "%")
```

:::
::::::


## Top three eigen-vectors

:::::: {.columns}
::: {.column width=.35}

```{r fig.width=2, fig.height=2.5, only.plot="1"}
.matshow(x.sub, .scale=TRUE, .lab=0, .size=0) +
    ggtitle("Top 200 genes")
```

```{r}
U <- .eigen$vector[,1:3,drop=FALSE]
.lm <- lm(x.sub ~ U -1)
```

```{r fig.width=2, fig.height=2.5, only.plot="2"}
.matshow(.lm$fitted.values, .lab=0, .size=0) +
    ggtitle("eigen proj.")
```

```{r fig.width=2, fig.height=2.5, only.plot="3"}
.matshow(x.sub, .scale=TRUE, .lab=0, .size=0) +
    ggtitle("Top 200 genes")
```

:::
::: {.column width=.65}

```{r fig.width=3.2, fig.height=2.7, only.plot="2"}
p0 <- ggplot() + theme_void()
p.eq <- ggplot() + theme_void() + geom_text(aes(0,0,label="="), size=10)
p.prod <- ggplot() + theme_void() + geom_text(aes(0,0,label="x"), size=10)
p1 <- .matshow(U, .lab=0, .size=0, .scale=TRUE) +
    ggtitle("e-vec")
p2 <- .matshow(as.matrix(.lm$coefficients),
               .lab=0, .size=0, .scale=TRUE) +
    ggtitle("loading")
p2 <- wrap_plots(p2, p0, ncol=1, heights = c(1,13))
wrap_plots(p.eq, p0, p1, p0, p.prod, p2, nrow=1, widths=c(1, 1, 1, 1, 1, 11))
```

```{r fig.width=2, fig.height=2.5, only.plot="3"}
var.exp <- mean(apply(.lm$fitted, 2, var) / apply(x.sub, 2, var))
.matshow(.lm$fitted.values, .lab=0, .size=0) +
    ggtitle(round(var.exp*100) %&% "%")
```

:::
::::::



## Singular Value Decomposition can find a PCA solution

::: {.block}
### Singular Value Decomposition

SVD identifies three matrices of $X$:

$$X = U D V^{\top}$$

where both $U$ and $V$ vectors are orthonormal,
namely, 

- $U^{\top}U = I$, $\mathbf{u}_{k}^{\top}\mathbf{u}_{k}=1$ for all $k$,

- $V^{\top}V = I$, $\mathbf{v}_{k}^{\top}\mathbf{v}_{k} = 1$ for all $k$.
:::

## SVD: another equivalent method for PCA

:::::: {.columns}
::: {.column width=.45}

::: {.block}
### Singular Value Decomposition

SVD identifies three matrices of $X$:

$$X = U D V^{\top}$$

where both $U$ and $V$ vectors are orthonormal,
namely, 

- $U^{\top}U = I$, $\mathbf{u}_{k}^{\top}\mathbf{u}_{k}=1$ for all $k$,

- $V^{\top}V = I$, $\mathbf{v}_{k}^{\top}\mathbf{v}_{k} = 1$ for all $k$.

:::

:::
::: {.column width=.45}

::: {.block}

### Covariance by SVD

Covariance across the columns (samples)

$$X^{\top}X/(m-1) = V D^{2} V^{\top}/(m-1)$$

Covariance across the rows (genes)

$$XX^{\top}/(n-1) = U D^{2} U^{\top}/(n-1)$$

:::

*Remark*: standardized matrix

:::
::::::

## How can SVD find an equivalent solution for PCA?

\begin{eqnarray*}
\onslide<1->{
	\underbrace{\left( \frac{1}{m-1} X^{\top}X \right)}_{\textsf{\color{blue} sample covariance}} \mathbf{v}_{1}
	&=& 
	\frac{1}{m-1} \left(\mathbf{v}_{1}, \mathbf{v}_{2},\ldots, \mathbf{v}_{k}\right)
      \left(
      \begin{array}{l l l l}
        D_{1}^{2} & 0 & \ldots & \ldots \\
        0 & D_{2}^{2} & 0 & \ldots \\
        0 & \ldots & \ddots & 0 \\
        0 & \ldots & 0 & D_{k}^{2} \\
      \end{array} \right)
  \left(
  \begin{array}{l}
    \mathbf{v}_{1}^{\top}\\
    \mathbf{v}_{2}^{\top}\\
    \vdots\\
    \mathbf{v}_{k}^{\top}
  \end{array}
  \right)
  \mathbf{v}_{1} \\
	}
	\onslide<2->{
  &=&
      \frac{1}{m-1} \left(\mathbf{v}_{1}, \mathbf{v}_{2},\ldots, \mathbf{v}_{k}\right)
      \left(
      \begin{array}{l l l l}
        D_{1}^{2} & 0 & \ldots & \ldots \\
        0 & D_{2}^{2} & 0 & \ldots \\
        0 & \ldots & \ddots & 0 \\
        0 & \ldots & 0 & D_{k}^{2} \\
      \end{array} \right)
  \left(
  \begin{array}{l}
    1 \\
    0 \\
    \vdots\\
    0
  \end{array}
  \right) \\
  }
\onslide<4>{
\boldsymbol{\hat{\Sigma}} \, \mathbf{v}_{1}
}
\onslide<3->{
  &=&
      \frac{1}{m-1} \left(\mathbf{v}_{1}, \mathbf{v}_{2},\ldots, \mathbf{v}_{k}\right)
      \left(
      \begin{array}{l}
        D_{1}^{2} \\
        0 \\
        0 \\
        0 \\
      \end{array} \right)}
	  \onslide<4>{
	  =
  \underbrace{\frac{D_{1}^{2}}{m-1}}_{\textsf{\color{red}eigenvalue}}
  \underbrace{\mathbf{v}_{1}}_{\textsf{\color{red}eigenvector}}
  }
\end{eqnarray*}


## Run SVD to find principal components

```{r echo=TRUE}
svd.out <- svd(x.sub, nu = 10, nv = 10)
U <- svd.out$u; D <- diag(svd.out$d[1:5]); V <- svd.out$v
```

:::::: {.columns}
::: {.column width=.25}

```{r fig.width=2, fig.height=2.3}
.matshow(x.sub, .scale=TRUE, .lab=0, .size=0) +
    ggtitle("data matrix")
```

:::
::: {.column width=.65}

```{r fig.width=3.2, fig.height=2.5}
p0 <- ggplot() + theme_void()

p1 <- .matshow(U, .lab=0, .size=.05, .scale=TRUE) +
    ggtitle("U")

p.D <- .matshow(D, .lab=0, .size=.2, .scale=TRUE) +
    ggtitle("D")

p.V <- .matshow(t(V), .lab=0, .size=.05, .scale=TRUE) +
    ggtitle("t(V)")

p.eq <- ggplot() + theme_void() + geom_text(aes(0,0,label="="), size=6)
p.prod <- ggplot() + theme_void() + geom_text(aes(0,0,label="x"), size=5)
p.prod <- wrap_plots(p.prod, p0, ncol=1, heights=c(1,8))

p2 <- wrap_plots(p.D, p0, ncol=1, heights=c(1,8))
p3 <- wrap_plots(p.V, p0, ncol=1, heights=c(1,8))

wrap_plots(p.eq, p0, p1, p.prod, p2, p.prod, p3,
     nrow=1,
     widths=c(1, 1, 1, 1, 1, 1, 8))
```

:::
::::::


## Sample covariance matrix

$$\hat{\Sigma} = X^{\top}X/(m-1)$$

```{r fig.width=2, fig.height=2}
.matshow(cov(x.sub), .lab = 0) + ggtitle("sample x sample covariance")
```

## Principal components decompose the covariance matrix

```{r}
p.eq <- ggplot() + theme_void() + geom_text(aes(0,0,label="="), size=10)

eigen.values <- svd.out$d^2 / (nrow(x.sub) - 1)
.show.cov.pc <- function(j) {
    p0 <- ggplot() + theme_void() + geom_text(aes(0,0,label="+"),size=5)
    .title <- round(eigen.values[j], 2) %&% "%"
    p <- .matshow(V[,j,drop=FALSE] %*% t(V[,j,drop=FALSE]), .lab = 0) +
        ggtitle(.title) + theme(title = element_text(size=8))
    wrap_plots(p, p0, nrow=1, widths=c(8,2))
}

v.prod <- lapply(1:ncol(V), .show.cov.pc)
```

$$\hat{\Sigma} = \frac{X^{\top}X}{m-1} 
\onslide<2->{
= V \frac{D^{2}}{m-1} V^{\top} 
}
\onslide<3->{
= \sum_{k=1} \lambda_{k} \mathbf{v}_{k} \mathbf{v}_{k}^{\top},\quad \lambda_{k}
}
\onslide<4>{
=\frac{D_{k}^{2}}{m-1}
}$$

:::::: {.columns}
::: {.column width=.25}

```{r fig.width=1.25, fig.height=1.65}
.matshow(cov(x.sub), .lab = 0) + ggtitle("n x n")
```

:::

::: {.column width=.02}
$${\large\boldsymbol{=}}$$
:::

::: {.column width=.75}

```{r fig.width=4.3, fig.height=1.65}
wrap_plots(v.prod[1:3], nrow = 1)
```

* How much variance is explained by each component?

:::
::::::

## How much variance is explained by each principal component?

```{r fig.width=5, fig.height=3, only.plot="1"}
.dt <- data.table(eigen.values) %>% mutate(k=1:n()) %>% as.data.table
vtot <- sum(.dt$eigen.values)
.dt[, pr := eigen.values / vtot]
.dt[, cpr := cumsum(pr)]
.gg.plot(.dt, aes(k, eigen.values, fill=eigen.values)) +
    geom_line() +
    ylab("eigenvalues (lambda)") +
    xlab("principal components") +
    geom_point(pch=21) +
    scale_fill_distiller(palette = "RdPu", direction=1, guide="none")
```

```{r fig.width=5, fig.height=3, only.plot="2"}
.gg.plot(.dt, aes(x=k, fill=eigen.values)) +
    geom_line(aes(y=pr)) +
    geom_text_repel(aes(y=pr, label=round(100*pr) %&% "%"),
                    data = .dt[1:10, ],
                    size = 3, nudge_x = 10, nudge_y = .1,
                    segment.size = .2,
                    segment.color = "red",
                    vjust=0, hjust=0, colour="red") +
    geom_text_repel(aes(y=cpr, label=round(100*cpr) %&% "%"),
                    data = .dt[2:5,],
                    size = 3, nudge_x = 10, nudge_y = 0,
                    segment.size = .2,
                    segment.color = "gray20",
                    vjust=0, hjust=0, colour="gray40") +
    geom_line(aes(y=cpr), lty=2) +
    geom_point(aes(y=pr), pch=21) +
    ylab("% var. explained\n(lambda/sum(lambda))") +
    xlab("principal components") +
    scale_fill_distiller(palette = "RdPu", direction=1, guide="none")
```

# Non-negative matrix factorization

## Why another factorization method?

Show Lee and Seung

## NMF-based methods can help data integration

Show LIGER

## NMF to understand hidden patterns

Show ICGC work

## Why another factorization method for genomics data?
  
Most genomics data were measured by counting the number of short reads. It is generally hard to make it look "normal," and the normality assumption may introduce unwanted bias.

```{r include=FALSE}
.stat <- data.table(mu = apply(t(X), 2, mean, na.rm=TRUE),
                    sig = apply(t(X), 2, sd, na.rm=TRUE))
```

:::::: {.columns}
::: {.column width=.5}

```{r fig.width=2.5, fig.height=2}
par(cex=.5, cex.main=1.5, cex.axis=1)
qqnorm(scale(.stat$mu), main="QQ gene-level average vs. N(0,1)")
abline(a=0, b=1, col=2)
```

:::
::: {.column width=.5}

```{r fig.width=2.5, fig.height=2}
par(cex=.5, cex.main=1.5, cex.axis=1)
qqnorm(scale(log(1 + .stat$mu)), main="QQ gene-level log-average vs. N(0,1)")
abline(a=0, b=1, col=2)
```

:::
::::::


## What is NMF? How can we find NMF solutions?

For each **non-negative** element $X_{ij} $of a matrix, we want to find non-negative $U$ and $V$ (factors and factor-loading values):

$$X_{ij} \approx \sum_{k=1}^{K} U_{ik} V_{kj}$$

with 

$$U_{ik} \ge 0,\, V_{ik} \ge 0.$$


```{r include=FALSE}
.sd <- apply(X, 1, sd, na.rm=TRUE)
msv <- order(.sd, decreasing=TRUE)[1:200]
x.sub <- apply(X[msv, , drop = FALSE], 2,
               function(x) length(msv) * x/sum(x))
x.sub <- .sort.matrix(x.sub)

rescale.nmf <- function(.beta, .theta) {
    uu <- apply(.beta, 2, sum)
    beta <- sweep(.beta, 2, uu, `/`)
    prop <- sweep(.theta, 2, uu, `*`)
    zz <- apply(t(prop), 2, sum)
    prop <- sweep(prop, 1, zz, `/`)
    list(beta = beta, prop = prop, depth = zz)
}
```

We can use some packages e.g., `NMF` in `R`.

```{r echo=TRUE}
library(NMF)
out <- nmf(x.sub, rank=10)
U <- basis(out)
V <- t(coef(out))
```

## NMF naturally induces (1) gene sets (2) sample loading

```{r include = FALSE}
nmf.out <- rescale.nmf(basis(out), t(coef(out)))
```

:::::: {.columns}
::: {.column width=.4}

```{r fig.width=2, fig.height=2.5}
.matshow(x.sub, .lab=0, .size=.01, .scale=FALSE)
```

:::
::: {.column width=.05}
$${\Huge\boldsymbol{\sim}}$$
:::
::: {.column width=.1}

```{r fig.width=.5, fig.height=2.5}
.matshow(nmf.out$beta, .lab=0, .size=.01, .scale=FALSE)
```

:::
::: {.column width=.05}
$${\Huge\boldsymbol{\times}}$$
:::
::: {.column width=.4}

```{r fig.width=2, fig.height=.3}
.matshow(t(nmf.out$prop), .lab=0, .size=.01, .scale=FALSE)
```

:::
::::::




```{r include = FALSE}
.col.annot <- setDT(tstrsplit(colnames(x.sub), "_"))
colnames(.col.annot) <- c("iid", "celltype", "subtype")
.col.annot[, Var1 := rownames(V)]
.col.annot[is.na(`subtype`), ct := `celltype`]
.col.annot[!is.na(`subtype`), ct := paste0(`celltype`, "-", `subtype`)]
```

## Some NMF factors are highly correlated with cell types

```{r fig.width=5, fig.height=3.2}

.dt.lab <- .col.annot %>% 
    select(Var1, ct) %>%
    unique() %>% 
    mutate(row = Var1, col = ct, weight=1) %>% 
    col.order(rownames(V), TRUE)

.dt <- reshape2::melt(V) %>%
    mutate(row = Var1, col = Var2, weight = log(value)) %>% 
    col.order(rownames(V), TRUE)

p1 <-
    .gg.plot(.dt, aes(row, col, fill = value)) +
    geom_tile() +
    xlab("samples") + ylab("NMF factors") +
    theme(axis.text = element_blank()) +
    theme(axis.ticks.x = element_blank()) +
    scale_fill_distiller(direction=1, trans="sqrt")

p2 <-
    .gg.plot(.dt.lab, aes(row, col)) +
    xlab("samples") + ylab("cell types") +
    theme(axis.text.y = element_text(size=6)) +
    theme(axis.text.x = element_blank()) +
    theme(axis.ticks.x = element_blank()) +
    geom_tile()

wrap_plots(p1,p2,ncol=1, heights=c(1,3))

```

## Some NMF factors are highly correlated with cell types

```{r fig.width=5, fig.height=3}

Y <- .col.annot %>%
    mutate(i = 1:n(), weight=1) %>%
    dcast(i ~ ct, value.var = "weight", fill=0)

R <- cor(V, Y[,-1], method="spearman")

.matshow(.sort.matrix(R), .lab=1.5) +
    theme(axis.text = element_text(size=8)) +
    theme(axis.text.x = element_text(size=8, angle=90, vjust=1, hjust=1))

```

## Sample-specific factor loading matrix can be an input to other methods

```{r echo=TRUE}
.tsne <- Rtsne::Rtsne(apply(.svd$v, 2, scale))
```

```{r fig.width=4.5, fig.height=2.5}
.dt <- data.table(tSNE = .tsne$Y) %>% cbind(.col.annot)

.med <- .dt %>%
    group_by(ct, celltype) %>%
    summarize(tSNE.V1 = median(tSNE.V1), tSNE.V2 = median(tSNE.V2)) %>% 
    ungroup()

p1 <-
    .gg.plot(.dt, aes(tSNE.V1, tSNE.V2, fill = celltype)) +
    geom_point(pch=21, size=3, show.legend=F) +
    geom_text_repel(aes(label=as.character(ct)), data=.med, size=2)
print(p1)
```

## Sample-specific factor loading matrix can be an input to other methods

```{r echo=TRUE}
.tsne <- Rtsne::Rtsne(apply(log(V), 2, scale))
```

```{r fig.width=4.5, fig.height=2.5}
.dt <- data.table(tSNE = .tsne$Y) %>% cbind(.col.annot)

.med <- .dt %>%
    group_by(ct, celltype) %>%
    summarize(tSNE.V1 = median(tSNE.V1), tSNE.V2 = median(tSNE.V2)) %>% 
    ungroup()

p2 <- .gg.plot(.dt, aes(tSNE.V1, tSNE.V2, fill = celltype)) +
    geom_point(pch=21, size=3, show.legend=F) +
    geom_text_repel(aes(label=as.character(ct)), data=.med, size=2)
print(p2)
```

## PCA vs. NMF: Which one looks better?

```{r fig.width=6, fig.height=2.5}
(p1 + ggtitle("PCA"))|(p2 + ggtitle("NMF"))
```
